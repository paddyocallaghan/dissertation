incident,summary
Nation state hackers use ChatGPT to improve cyberattacks,"China, Iran, Russia and North Korea used ChatGPT to research, refine, and mount offensive cyber operations across the world. 
According to Microsoft research, Russian, North Korean, Iranian, and Chinese-backed groups have been discovered using tools including ChatGPT to conduct research into targets and to improve scripts and social engineering techniques for surveillance, disinformation and influence operations, and cybercrime campaigns.
The techniques employed were considered 'early-stage' and not 'particularly novel or unique, and 'significant attacks' using ChatGPT and other large language models were not discovered, Microsoft said. In a blog post, OpenAI argued its GPT-4-powered chatbot offers 'only limited, incremental capabilities for malicious cybersecurity tasks beyond what is already achievable with publicly available, non-AI powered tools.'
However, experts believe that it is only a matter of time before effective malicious nation state-backed campaigns using chatbots and large language models are conducted.
Operator: Aquatic Panda; Charcoal Typhoon; Crimson Sandstorm; Emerald Sleet; Fancy Bear; Forest Blizzard; Maverick Panda; Salmon Typhoon Developer: OpenAICountry: China; Iran; N Korea; Russia Sector: Govt - defence Purpose: Conduct research; Generate phishing content; Generate codeTechnology: Chatbot Issue: Fraud; Mis/disinformation; Reputational damage; SecurityTransparency:"
Robot crushes to death man mistaken for box of vegetables,"A South Korean worker was crushed to death by a robot that mistook him for a box of paprika peppers. 
The employee had been inspecting the robot’s sensor on the Donggoseong Export Agricultural Complex in south Korea when the robot arm - which was programmed to lift boxes of vegetables - mistook the employee for one, grabbed and placed him on a conveyor belt using its tongs and squeezed him.
The man, whose face and body were crushed by the conveyor belt, died shortly after arriving at a local hospital. The machine had been experiencing issues for days before the incident. 
A Donggoseong Export Agricultural Complex official told the Yonhap News agency, 'We have been using robots well with less labour, but recently we changed the work line and entrusted the work to more efficient use.'
The incident raised questions about the safety of the unnamed manufacturer of the robot and the working practices of the Agricultural Complex. 
Operator: Donggoseong Export Agricultural ComplexDeveloper:  Country: S KoreaSector: Manufacturing/engineering Purpose: Sort productsTechnology: Robotics Issue: Robustness; SafetyTransparency:"
Drunk driver using Tesla FSD killed after car hits tree,"A Tesla employee driving home after several alcoholic drinks reportedly activated his car's Full-Self Driving capability before the vehicle careered off the road and burst into flames, killing him and injuring his passenger.
In an interview, Hans von Ohain's passenger Erik Rossiter said he believed that von Ohain was using Full Self-Driving (FSD), a charge apparently supported by the police but refuted by Tesla CEO Elon Musk. Tesla had earlier told the Washington Post that it '...could not confirm that a driver-assistance system had been in use because it did not receive data over-the-air for this incident.' 
Von Ohain’s widow, Nora Bass, argued Tesla should take some responsibility for her husband’s death. 'Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human. We were sold a false sense of security.' Tesla and Musk have been accused of misleading marketing on multiple occasions.
Operator: Hans von Ohain  Developer: TeslaCountry: USASector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Accuracy/reliability; Safety Transparency: Governance; Black box"
Air Canada found liable for chatbot's poor advice,"Air Canada was forced to pay damages after a court ruled the airline was liable for the wrong information its chatbot gave a customer before he booked a flight.
Following the death of his grandmother, Air Canada's chatbot told Jake Moffatt that if he purchased a normal-price ticket he would have up to 90 days to claim back a bereavement discount - a special low rate for people traveling due to the loss of an immediate family member.
Moffatt ended up taking the airline to a small-claims tribunal for negligence after it refused to claim back the discount, even though he had the correct documents and did so within the 90-day window, on the basis that it should not be held liable for the chatbot's faulty outputs.
In his ruling, tribunal member Christopher Rivers said Air Canada had failed to take 'reasonable care to ensure its chatbot was accurate.' He also said that 'It should be obvious to Air Canada that it is responsible for all the information on its website. It makes no difference whether the information comes from a static page or a chatbot.'
The incident was seen as a reminder that companies need to aware of the risks of using AI, including the legal risks.
Operator: Air Canada Developer: Air Canada Country: CanadaSector: Travel/hopsitalityPurpose: Support customersTechnology: Chatbot Issue: Accuracy/reliability; LiabilityTransparency: Governance"
Researchers reveal Hello Barbie security vulnerabilities,"Mattel's Hello Barbie doll could be hacked and young girls spied on, according to a US-based security researcher.  
Security researcher Matt Jakubowski discovered the WiFi-enabled Hello Barbie was vulnerable to hacking when storing audio files of conversations between kids and the doll in the cloud, on which they could be analysed by Mattel, its technology partner ToyTalk and other vendors.
The hack allowed Jakubowski 'easy' access to the doll’s system information, account information, and stored audio files. The result might be that anyone could identify the individual and their home address and modify the doll to suit their needs. 'It’s just a matter of time until we are able to replace their servers with ours and have her say anything we want,' Jakubowski told NBC.
Mattel software maker ToyTalk responded by saying it would patch the vulnerability.
Operator: Mattel/ToyTalk Developer: Mattel/ToyTalkCountry: USASector: Consumer goodsPurpose: Interact with children Technology: Voice recognition; NLP/text analysis Issue: Privacy; Security; Surveillance Transparency:"
AI-powered Hello Barbie riles privacy advocates,"Mattel's Hello Barbie doll uncessarily exposed children to the commercial exploitation of their data, according to childhood advocates and privacy experts.
Developed by Mattel and technology partner ToyTalk and billed as 'the first fashion doll that can have a two-way conversation with girls,' the WiFi-enabled Hello Barbie was equipped with a microphone, voice recognition and 'progessive learning features' and was programmed with 8,000 lines of dialogue. 
However, childhood and privacy experts took issue with ToyTalk's privacy policy, which the company to listen to and process kids' conversations 'in order to provide and maintain the Service, to perform, test or improve speech recognition technology and artificial intelligence algorithms, or for other research and development and data analysis purposes.'
Experts also took issue with algorithms replacing human actions. 'Computer algorithms can’t replace - and should not displace - the nuanced responsiveness of caring people interacting with one another,' according to pediatrician Dipesh Navsaria, MPH, MD, assistant professor at the University of Wisconsin School of Medicine and Public Health.
Operator: Mattel/ToyTalk Developer: Mattel/ToyTalkCountry: USASector: Consumer goodsPurpose: Interact with children Technology: Voice recognition; NLP/text analysis Issue: PrivacyTransparency:"
UIUC dumps Proctorio over 'significant accessibility concerns',"A US university contract with automated proctoring company Proctorio was terminated due to concerns about the company's remote cheat-prevention software's accessibility, privacy, and security.
A memo from university adminstrators to faculty members announcing the decision by the University of Illinois Urbana-Champaign cited 'significant accessibility concerns' associated with Proctorio. 'For some students with physical disabilities, students with low vision or are blind, students with psychiatric disabilities including anxiety or ADD/ADHD, Proctorio may be inaccessible,' it said.
The decision followed an outcry by students over the service, which uses machine learning and facial detection to monitor and record students taking exams. Over 1,000 students signed an online petition alleging that 'Proctorio is not only inefficient, it is also unsafe and a complete violation of a student’s privacy,' and called for the university to stop using the service.
Operator: University of Illinois at Urbana-Champaign Developer: ProctorioCountry: USASector: EducationPurpose: Detect exam cheatingTechnology: Facial detection; Gaze detection; Machine learning; Noise anomaly detection Issue: Accessibility, Bias/discrimination - disability; Privacy; SecurityTransparency: Governance"
IBM sells Greg Marston voice for commercial cloning,"British voice actor Greg Marston discovered that AI-generated clones of his voice were being used by third-parties without his permission. 
Having discovered an 'eerily' similar voice to his own associated with a character named 'Connor' on the Wimbledon website, Marston realised that licensed voice recordings he had recorded for IBM in 2003 and to whom he had granted permission for its use, had been sold to third-party websites that were now using it to create synthetic voices able to say anything, anywhere, at any time.
The incident prompted concerns about the impact of AI on the livelihoods of artists, writers, actors, and musicians, many of whom are concerned that their work is being used to train AI systems that will result in loss of future earnings and which may eventually replace them entirely. 
It also prompted creatives to press technology companies to act ethically and ensure they are asked for their consent and are fairly compensated for their work. 
Operator: All England Lawn Tennis and Croquet Club Developer: RevoicerCountry: GlobalSector: Media/entertainment/sports/arts Purpose: Clone voiceactor's voice Technology: Text-to-speech; Emotion recognition; Neural network; Deep learning; Machine learning Issue: Employment; Ethics/valuesTransparency: Governance"
Apple trains AI models on Spotify audiobook narrators,"Apple used trained the voices of voiceover artists and authors without their explicit consent to train the AI models powering its AI audiobooks service, resulting in complaints that they were being used to train their own replacements.
Spotify audiobook narrators and authors discovered that a clause in their agreement with Findaway Voices, a audiobook distributor owned by Spotify, allowed Apple to use their audiobook files for 'machine learning training and models'. 
Some actors and authors pointed out that the clause was not explicitly pointed out to them when they signed updated agreements after Findaway had been bought by Spotify in June 2022. 
Apple launched a range of audio Apple Books early January 2023, claiming that its new AI audiobook service was only available to titles for which it was not economic to hire an actor.
Operator: Apple Developer: Apple Country: GlobalSector: Media/entertainment/sports/arts Purpose: Train AI modelsTechnology: Speech-to-speech; Neural network; Deep learning; Machine learningIssue: EmploymentTransparency: Governance"
Waymo robotaxi injures cyclist in San Francisco,"A driverless Waymo car collided with a cyclist in San Francisco, causing minor injuries to the cyclist and leading to a review of the incident by California's auto regulator. 
Per Reuters, Waymo said its vehicle had stopped at a four-way intersection when a large truck crossed the intersection in its direction. At its turn to proceed, the Waymo car moved forward. 
However, the cyclist, who was obscured by the truck which the cyclist was following, took a left turn into the Waymo vehicle's path. When the cyclist was fully visible, the Waymo's vehicle braked heavily, but wasn't able to avoid the collision.
The incident raised questions about the accuracy, reliability, and safety of Waymo's self-driving system.
Operator: Alphabet/Waymo Developer: Alphabet/WaymoCountry: USASector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer vision Issue: Accuracy/reliability; SafetyTransparency: Governance; Black box"
Lawsuit claims Amazon Buy Box algorithm overcharges shoppers,"A lawsuit accused Amazon of violating US consumer protection by steering users of its website to higher-priced items commanding higher fees for the company, as opposed to the 'best' prices it claims. 
According to a legal complaint (pdf) filed in the name of two US-based customers of Amazon, the company's Buy Box algorithm often obscures lower-priced options with faster delivery times. 
The suit also cited a recent antitrust case against Amazon by the US Federal Trade Commission and 17 states which alleged that shoppers use the company's choices almost 98 percent of the time by clicking its 'Buy Now' or 'Add to Cart' buttons, often falsely believing it had identified the best prices. 
The lawsuit also accused Amazon of creating the algorithm to benefit third-party sellers that participate in its Fulfillment By Amazon programme, which pay 'hefty fees' for inventory storage, packing and shipping, returns and other services. 
Operator: Amazon Developer: AmazonCountry: USASector: RetailPurpose: Determine seller Technology: Machine learning Issue: Consumer protectionTransparency: Governance; Marketing"
Amazon sells AI-generated books about King Charles' cancer,"AI-generated books about King Charles' cancer diagnosis have been offered for sale on Amazon, sparking fury from Buckingham Palace.
Seven fake biographies riddled with fake claims such as the King actually having skin cancer and that he had suffered an undisclosed accident were published on Amazon, according to the Mail on Sunday. Seemingly generated by AI, the books were penned by unknown authors. 
The incident prompted commentators to accuse the 'authors' of intruding the King's privacy, and to call out Amazon for its apparent inability or unwillingness to properly police its website for AI-generated content. 
An Amazon spokesman told the Mail on Sunday that the company invested 'significant time and resources' to ensure books published on its website followed its 'content guidelines', adding 'We don't allow AI-generated content that violates our content guidelines, including content that creates a disappointing customer experience.'
Operator: Amazon Developer: AmazonCountry: UKSector: RetailPurpose: Moderate contentTechnology: Content moderation system Issue: Fraud; Mis/disinformationTransparency: Governance"
Toilet sensors ‘actively listen’ to school pupils,"Schools in the UK have been accused of covertly monitoring students in toilets in an attempt to curb vaping, bullying, and unruly behaviour, without their parents' permission.
According to SchoolsWeek, schools have been using products such as Triton's 3D Pro Sensor to actively detect vape smells and anomolous noises using sensors, as well as certain keywords through machine learning algorithms, which trigger alerts to selected staff members. 
The report cited the head teacher at Baxter College, Kidderminster, acknowledging that parental permission had not been obtained, though parents were very positive' about the school's attempts to crack down on vaping, she said.
The finding triggered complaints by privacy advocates. Madeleine Stone, a senior advocacy officer for UK digital rights pressure group Big Brother Watch, voiced her concerns by stating 'secretly monitoring school bathrooms is a gross violation of children’s privacy and would make pupils and parents deeply uncomfortable.'
Operator: Baxter College, Kidderminster Developer: Triton Country: UKSector: EducationPurpose: Detect vaping; Increase safety Technology: Machine learning; Keyword detection Issue: Privacy; SurveillanceTransparency: Marketing"
New York lawyer cites fake AI-generated court decision,"New York lawyer Jae Lee cited a fictitious case generated by ChatGPT to appeal a lawsuit, resulting in her facing possible sanctions.
Attorney Jae Lee was referred to the grievance panel of the 2nd US Circuit Court of Appeals after she cited a fabricated case about a Queens doctor botching an abortion in an appeal to revive her client's lawsuit. 
The appeal was dismissed after it was discovered that the case did not exist and had been conjured up by OpenAI's ChatGPT chatbot. The grievance panel concluded concluded Lae's conduct fell 'well below the basic obligations of counsel'. She now faces possible sanctions.
The incident was the one of several examples of lawyers misusing generative AI in legal cases, and prompted concern from lawyers and others that the technology is being used in the wrong way. 
Operator: Jae Lee Developer: OpenAICountry: USASector: Govt - justice Purpose: Conduct legal research Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliabilityTransparency: Marketing"
SEC charges American Bitcoin Academy with 'AI' powered fraud,"A US businessman was charged with defrauding 15 students by persuading them to invest in a fund that promised high returns using AI.
Brian Sewell lured students of his online course American Bitcoin Academy into parting with significant sums of money that would be invested in his supposedly 'artificial intelligence and 'machine learning'-powered Rockwell Capital Management crypto fund. 
But Sewell never launched the fund, instead purchasing USD 1.2 million worth of Bitcoin with the students' money, all of which he lost when his BTC wallet was hacked and wiped clean, according to US Securities and Exchange Commission (SEC).
The incident was seen to show the SEC clamping down on individuals and companies using 'attention-grabbing' technologies to attract and defraud investors.
Operator: American Bitcoin Academy Developer: Brian Sewell Country: USASector: Banking/financial services Purpose: Defraud Technology: Machine learning Issue: FraudTransparency: Marketing"
Google researcher believes LaMDA is 'sentient',"Google engineer Blake Lemoine tried to convince fellow Google employees that the company's LaMDA language model was sentient.
Per a June 2022 Washington Post report, Lemoine, an ordained Christian mystical priest 'was inclined to give it the benefit of the doubt 'When LaMDA claimed to have a soul and then was able to eloquently explain what it meant by that.'
Google, technology professionals, philsophers and ethicists responded to the notion that LaMDA - and other technologies - can be human primarily on technical, scientific grounds, prompting Lemoine to complain the model faces 'bigotry' in an interview with WIRED.
Google suspended and fired Lemoine after he breached company policy by sharing information about his project, recruited a lawyer for the AI after claiming that LaMDA had asked him to do so, and alleged that Google was discriminating against him because of his religion.
Operator: Blake Lemoine Developer: Alphabet/GoogleCountry: USASector: Technology; ReligionPurpose: Optimise language models for dialogue Technology: Large language model; Neural network; NLP/text analysis Issue: Anthropomorphism Transparency:"
NYPD ends Knightscope K5 security robot trial,"The New York Police Department ended its use of Knightscope's security robot in Times Square subway station after a six-month trial, calling into question the effectiveness of the robot.
Initially heralded as a low-cost method of deterrning crime, the robot, which was designed to operate autonomously, received a mixed recpetion from New Yorkers and visitors, with some saying it was potentially a valuable additional crime-fighting resource, whilst others reckoned it seemed to do very little, was unable to walk up or down stairs, always required assistance, was a waste of resources, and threatened people's privacy.
In addition to raising questions about the effectiveness of the Knightscope K5 robot as a crime-fighting tool, the NYPD's decision to stop its use - for the time being -  highlights the careful balance police authorities are seen to have to strike between fighting crime, and protecting the legal rights and ethical concerns of citizens. 
Operator: New York citizensDeveloper: KnightscopeCountry: USASector: Govt - policePurpose: Strengthen securityTechnology: Robotics Issue: Effectiveness/value; Privacy; SurveillanceTransparency:"
Amazon France fined for excessive automated monitoring of workers,"Amazon was fined EUR 32 million by France's privacy regulator for the 'excessive' and 'illegal' monitoring of staff activity and performance using scanners and several software systems.
Amazon France Logistique had been using handheld scanners and three indicators to measure the producivity and inactivity of its employees, including for tasks such as putting an item on a shelf, taking an item off a shelf, putting an item into a box, and time spent on breaks. 
According to France's National Commission on Informatics and Liberty (CNIL), 'the implementation of a system measuring interruptions of activity so precisely and leading to the employee potentially having to justify each break or interruption was illegal' and had breached the EU's GDPR principle of data minimisation and the lawfulness of the processing.
The CNI also took issue with Amazon's transparency, or lack thereof. Before April 2020, temporary workers had not been informed before their data was collected, and employees were not properly told about video surveillance systems.
Amazon disagreed with the CNIL's conclusions, which it described as 'factually incorrect.' 
Operator: Amazon France Logistique employees, visitors  Developer: Amazon France LogistiqueCountry: FranceSector: Transport/logisticsPurpose: Monitor employee performance Technology: Handheld scanner Issue: Employment; Necessity/proportionality; Privacy; Surveillance Transparency: Governance; Marketing"
Philadelphia sheriff posts fake AI-generated news stories,"The campaign team for Philadelphia’s sheriff used fake positive stories generated by AI posted to her website to help make the case for her re-election. 
The Philadelphia Inquiry drew attention to a series of articles under the names of local news publications that had been posted to Rochelle Bilal's website that supposedly highlighted her first term accomplishments but which proved to be non-existent. Bilal's team later acknowledged that the stories had been generated by ChatGPT, though argued they had been based on real events. 
The incident was seen to raise ethical questions about the integrity of Bilal's campaign and the possible erosion of trust in elections and democracy posed by AI-generated misinformation and disinformation. It also highlighted concerns about OpenAI's willingness or ability to police its policies regarding the political use of ChatGPT and its other products.
Operator: Philadelphia citizens Developer: OpenAICountry: USASector: PoliticsPurpose: Support political campaign Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Ethics/values; Mis/disinformation Transparency: Governance; Marketing"
US man dies driving off collapsed bridge while following Google Maps,"Medical device salesman Philip Paxson drowned after Google Maps allegedly directed him to cross a bridge that had collapsed nine years before and his car plunged into a creek.
Paxton’s body was found in his overturned and partially submerged truck after he had been driving home from his daughter's ninth birthday party on a route in North Carolina he did not know. According to local police, the bridge had not been maintained by local or state officials, and the original developer’s company had dissolved. 
Multiple people had notified Google Maps about the collapse in the years leading up to Paxson’s death and had urged the company to update its route information, according (pdf) to the lawsuit.
The lawsuit accused Google of negligence. It also named several private property management companies allegedly responsible for the bridge and the adjoining land. 
Operator: Philip Paxson Developer: Alphabet/Google Country: USA Sector: Travel/hospitalityPurpose: Direct driversTechnology: Machine learningIssue: Accuracy/reliability; SafetyTransparency: Governance"
Couple assaulted in 'Hell Run' recommended by Google Maps,"A US couple sued Google after Google Maps directed them into a South Africa ‘Hell Run’ area where they were assaulted at gunpoint and robbed.
Trying to navigate to Cape Town airport, LA-based Jason and Katharine Zoladz were directed by Google Maps into a notoriously dangerous area when they were attacked, assaulted, and robbed by an armed gang at an intersection. According to the lawsuit (pdf), Jason Zoladz was left bleeding by the roadside having had his jaw smashed by a brick. He had surgery later that day. 
The couple claimed in the suit that Google knew the ‘extreme dangers’ of the route and that it was known locally for years as the site of ‘numerous’ violent attacks on tourists by armed criminals. They also argued that Google has a responsibility to protect its users, but failed to protect or warn them of the risks of the route. 
The incident persuaded Google to re-route trips to the airport away from dangerous areas. 
Operator: Jason Zoladz, Katharine Zoladz Developer: Alphabet/Google Country: South Africa Sector: Travel/hospitality Purpose: Direct droversTechnology: Machine learningIssue: Accuracy/reliabilityTransparency: Governance"
Deepfake CFO scams finance worker for USD 25 million,"Scammers tricked a Hong Kong-based employee of a multinational company into paying out HKD 200 million (USD 26 million) with a fake group video call created using deepfake technology.
According to Hong Kong police, the worker received a strange message purportedly from his company’s UK-based chief financial officer asking for a secret transaction to be carried out. 
Attending a subsequent video call, the employee was reassured by several colleagues whom he thought he recognised; however, it transpired that all the 'people' on the call were in fact deepfake recreations of colleagues that had been manipulated using public video footage.
The scam was discovered when the employee later checked with the company's head office. 
Operator: Bank employee Developer:  Country: Hong KongSector: Banking/financial services Purpose: Defraud Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: FraudTransparency: Governance"
Wacom AI-generated Chinese New Year promotion backfires,"Japanese tablet manufacturer Wacom was discovered covertly using AI-generated images in its new year marketing, prompting complaints from artists and customers.
Wacom's apparent use of AI to generate illustrations of Chinese dragons to welcome in the Chinese Year of the Dragon prompted revulsion and despair from artists. One artist, Megan Ruiz, pointed out that the quality of the images was sub-par, with one sporting a tail that failed to attach to its body, another with strange-looking teeth. 
Wacom later deleted the artwork and claimed it was not its 'intent' to use AI-generated images and that it had purchased the images 'through a third-party vendor where it was indicated that they were not AI generated.' 
Artists took particular exception to Wacom's use of AI because the company's products, many of which are premium-priced, are primarily used by designers and other creatives, leading some to say they were devaluing the work of their own customers. Some customers said they would not buy its products again.
Operator: Wacom customers Developer:  Country: USASector: Technology Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment; Ethics/values; Reputational damageTransparency: Governance"
Instacart generates recipes and food images using AI,"Grocery delivery and pick-up service Instacart used AI to generate 'revolting' images of food to accompany its AI-generated recipes, resulting in a backlash from its customers. 
Instacart subreddit users discovered that the company was apparently using AI-generated images to accompany entries for ingredients and recipes on its app, prompting complaints from customers and commentators that the images were 'absurd', 'disturbing', and 'horrifying'. In one instance, an image for 'Microwave Mug Chocolate Chip Cookie a la Mode' showed a small chocolate chip cookie hanging on the side of a coffee mug. 
Instacart also used AI to generate recipes, noting that they were 'powered by the magic of AI, so that means it may not be perfect.' The company was estimated to have published 8,000-10,000 such recipes, a number of which were deleted in the wake of a corruscating Business Insider article. It also replaced the accompanying AI images with stock photos.
The incident raised questions about Instacart's oversight and quality assurance of its AI programme, and more generally about the use of the technology in advertising and marketing. Some customers threatened not to use Instacart again. 
Operator: Instacart customers Developer:  Country: USASector: Transport/logisticsPurpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Reputational damageTransparency: Governance"
Short-seller bots sow First Republic Bank doubts,"Automated bots and fake social media accounts allegedly operated by capital markets short-sellers were used to spread misinformation and sow doubts about US-based First Republic Bank ahead of its collapse in mid-2023. 
Using AI to examine online activity during the 2023 US banking crisis, Valent Projects discovered two major peaks of activity as tweets and Reddit posts targeted First Republic Bank, coinciding with a collapse in confidence that led customers to withdraw cash. These peaks did not come with a surge in engagement such as likes, retweets or replies, a pattern 'unlikely to occur naturally,' according to the researchers.
The campaign, which the researchers concluded was likely to have been orchestrated by short sellers betting against the bank's share price, is seen to have helped trigger the withdrawal of USD 100 billion in deposits and the collapse of the bank. First Republic was taken over by the Federal Deposit Insurance Corporation (FDIC) and sold to JP Morgan Chase for US 10.6 billion.
Operator: First Republic Bank customers, investors Developer:  Country: USASector: Banking/financial services Purpose: Sow misinformation Technology: Bot/intelligent agent Issue: Mis/disinformationTransparency: Governance"
Nine News uses AI to 'sexualise' image of politician,"Australian TV broadcaster Nine News has been accused of using AI to make a photograph of Australian politician Georgie Purcell appear more 'sexual', resulting in accusations of manipulation and sexism. 
Animal Justice Party MP Georgie Purcell posted to X an edited image of herself originally shared by Nine News Melbourne reading 'having my body and outfit photoshopped by a media outlet was not on my bingo card. Note the enlarged boobs and outfit to be made more revealing. Can’t imagine this happening to a male MP.' Purcell had been wearing a dress; the manipulated image showed her with larger breasts and sporting a midriff-exposing tank top.
Nine News blamed the 'graphic error' on automation: 'During that process, the automation by Photoshop created an image that was not consistent with the original,' he said. Adobe Photoshop’s new generative AI tools allow users to fill or expand existing images using AI. However, a spokesperson for Photoshop maker Adobe told the BBC that 'human intervention and approval' would have been required for 'any changes to this image.'
The incident prompted concerns about the ethics and legality of the manipulation of images by media organisations, perceived ingrained sexism of Nine News and other broadcasters, and poor transparency.
Operator: Nine News Developer: Adobe Country: AustraliaSector: Politics Purpose: Manipulate image Technology: Machine learningIssue: Ethics/values; SexualisationTransparency: Governance; Marketing"
Workers assist Cruise 'autonomous' robotaxis every 2.5-5 miles,"Remote operators have to intervene every 2.5 to 5 miles driven by a Cruise robotaxi, calling into question whether they should be called 'autonomous'.
According to the New York Times, Cruise employs one and a half workers located in remote operations centres to support each vehicle, prompting AI expert Gary Marcus to question whether the revelation may prove Cruise to be the 'Theranos of AI'. 
'If Cruise’s vehicles really need an intervention every few miles, and 1.5 external operators for every vehicle, they don’t seem to even be remotely close to what they have been alleging to the public,' Marcus wrote. 'Shareholders will certainly sue, and if it’s bad as it looks, I doubt that GM will continue the project.'
In a post on Hacker News, then Cruise CEO Kyle Vogt responded by saying that Cruise robotaxis were remotely assisted '2-4 percent of the time on average, in complex urban environments', and that 'of those, many are resolved by the AV itself before the human even looks at things'.
Operator: General Motors/Cruise LLC Developer: General Motors/Cruise LLC Country: USASector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer vision; Machine learning Issue: Governance Transparency: Governance; Marketing"
Dudesy sued for abusing George Carlin copyright,"The media company behind an purportedly AI-generated comedy special that attempted to recreate the US comedian George Carlin is being sued by Carlin's estate.
George Carlin: I’m Glad I’m Dead shows Carlin, who died in 2008, commentating on current events. The beginning of the now offline hour-long video featured a voiceover identifying itself as the AI engine used by media firm Dudesy says it listened to the comic’s 50 years of material and 'did my best to imitate his voice, cadence and attitude as well as the subject matter I think would have interested him today.'
The defendants named in the suit are Dudesy LLC and podcast hosts Will Sasso and Chad Kultgen. 'None of the Defendants had permission to use Carlin’s likeness for the AI-generated ‘George Carlin Special,’ nor did they have a license to use any of the late comedian’s copyrighted materials,' the suit reads. 
Will Sasso later told the New York Times that ‘I’m Glad I’m Dead’ was 'completely' written by Chad Kultgen. In 2023, former NFL star Tom Brady threatened to file a similar lawsuit against Dudesy for creating a self-labeled AI-generated comedy special using his likeness.
Operator: Kelly Carlin Developer: Chad Kultgen; Will Sasso Country: USA Sector: Media/entertainment/sports/arts Purpose: Imitate George Carlin Technology: Machine learning Issue: Copyright Transparency:"
X/Twitter fails to remove graphic AI images of Taylor Swift,"Sexually explicit AI-generated images of Taylor Swift published on Twitter and which went viral remained on the platform for up to 17 hours before they were removed.
The images, which showed Swift in a series of sexual acts while dressed in Kansas City Chief memorabilia, were uploaded to deepfake porn website Celeb Jihad and quickly went viral on X/Twitter, Facebook, Instagram, Reddit, and other platforms. The images appeared also to have been shared on a Telegram group dedicated to abusive images of women, and created using Microsoft Designer, according to 404 Media.
X/Twitter eventually removed offending images, shut down the account that first shared them, and suspended accounts that had re-shared them. However, other images quickly emerged in their place. Later, it blocked searches for Swift's name. 
The incident led Swift to say she was considering legal action against Celeb Jihad. It also raised questions about X's business model and the effectiveness of it's content moderation system, which is mostly automated after Elon Musk had fired much of its safety team earlier in 2023. 
It was also seen to demonstrate the ease with which synthetic images can be made and distributed, and renewed calls for effective legislation in the US. 
Operator:  Developer: X/Twitter Country: USA Sector: Media/entertainment/sports/arts Purpose: Moderate content Technology: Content moderation system; Machine learning Issue: Business model; Privacy; Robustness; SafetyTransparency:"
Parivar Pehchan Patra algorithm declares living people dead,"Several thousand welfare beneficiaries in the Indian state of Haryana were denied access to their pensions and other welfare benefits having been wrongfully declared dead by an AI-powered algorithm.
Parivar Pehchan Patra (PPP) is an algorithmic system that provides Haryana families with an eight-digit unique ID based on income, age, employment, and other data. It is intended to streamline the delivery of welfare services and help reduce fraud by linking different databases together to produce a ‘single source of truth’. Birth, Death and Marriage records are linked to ensure automatic updating of family data.
After 102-year-old Dhuli Chand was forced to put together a mock wedding procession to prove to Haryana officials that he was alive, government data was released revealing that over 300,000 pensions were stopped in the following three years since claimants had been classified 'dead'. 70 percent (44,050) of a smaller sample of 63,353 pensions that were halted were later found to have been flagged incorrectly.
Beneficiaries of subsidised food and other schemes were also excluded because the algorithm made wrong predictions about their incomes or employment, according to Al-Jazeera.
Operator: Haryana citizensDeveloper: Government of Haryana Country: India Sector: Govt - welfarePurpose: Assess welfare eligibility  Technology: Machine learning Issue: Accuracy/reliability; Accountability; Privacy Transparency: Governance; Black box; Complaints/appeals"
Jordan Takaful poverty targeting algorithm unfairly excludes some poor people,"The kingdom of Jordan was accused of using a 'flawed' algorithm to calculate the amount of aid for its citizens, excluding some  who are impoverished, hungry or otherwise struggling.
The Takaful 'poverty targeting' cash transfer algorithm run by Jordan's National Aid Fund and funded by the World Bank assesses whether aid applicants meet basic criteria such as whether families are headed by a Jordanian citizen and living under the poverty line. 
The algorithm estimates and ranks families' income and wealth using 57 socio-economic indicators. Families that own cars less than five years old or businesses worth at least 3,000 dinars (approximately USD 4,200) are automatically disqualified.
But, according to Human Rights Watch, the system is undermined by an opaque system based on inaccurate and unreliable data about people's finances, stereotypes about poverty, and discriminatory policies - notably against women - thereby depriving people of their rights to social security and resulting in increased social tensions and inequality.
The World Bank responded by saying it would refine the algorithm, whilst noting that Takaful has proven to be one of the most cost-effective poverty reduction programmes currently operating in Jordan.
Operator: Jordan National Aid Fund Developer: The World Bank Country: Jordan Sector: Govt - welfarePurpose: Calculate aid eligibility and distribution Technology: Ranking algorithm Issue: Accuracy/reliability; Bias/discrimination - gender Transparency: Governance; Black box; Complaints/appeals"
Samagra Vedika system pilot deprives citizens of rations ,"A pilot project in Hyderabad to assess the eligibility of welfare beneficiaries led to the removal of thousands of ration card holders by Telangana state's Samagra Vedika system.
According to a Telengana government document (pdf), 100,000 ration card holders were removed from the system for apparently being ‘ghost beneficiaries’ or fraudulent applicants. Once excluded, beneficiaries had to prove to government agencies that they were entitled to the subsidised food. But government officials allegedly often ignored them, or tended to back the decision of the algorithm.
The action resulted in the denial of food rations to people rightfully entitled to them, was seen to worsen social inequality, and led to a public outcry. Under significant public pressure, the government reinstated 14,000 cancelled ration cards through an 'appeals and verification' process. It refused to say how the system had gone wrong, though poor quality data and inadequate oversight have been considered likely causes. 
A subsequent government reanalysis of over 200,000 cards revealed that 15,000 had been incorrectly removed, according to Al-Jazeera.
The incident prompted concerns about the accuracy and fairness of the Samagra Vedika system, and its governance and accountability. It also raised questions about the ethics of using big data and machine learning for sensitive government decision-making.
Operator: Telagana citizens Developer: Government of Telagana; Posidex Technologies Country: India Sector: Govt - welfarePurpose: Determine welfare eligibility Technology: Machine learning Issue: Accuracy/reliability; Accountability; Human/civil rights Transparency: Governance; Marketing"
Harvey Murphy Jr facial recognition wrongful arrest,"A Texas man was mistakenly arrested for armed robbery using facial recognition, leading to his imprisonment and rape, and resulting in him suing Macy's and the owner of Sunglass Hut.
Harvey Murphy Jr was arrested in October 2023 for the January 2022 robbery of a Sunglass Hut in Houston, though his attorneys said he was in Sacramento, California, at the time of the robbery. During his two weeks time in detention, he was allegedly attacked and raped by three men, leaving permanent injuries. 
According to Murphy’s lawsuit, an employee of EssilorLuxottica, Sunglass Hut’s parent company, worked with its retail partner Macy’s and used facial recognition software to identify Murphy as the robber, leading to his arrest. 
Murphy's alibi was eventually believed and the charges against him dropped. 
Operator: EssilorLuxottica, Macy's Developer:  Country: USA Sector: Govt - policePurpose: Identify individuals Technology: Facial recognition Issue: Accuracy/reliability Transparency: Governance"
Artist uses FindFace to identify St Peterburg subway passengers,"A Russian artist used facial recognition app FindFace to identify passengers on St. Petersburg's subway system, resulting in concerns about the invasiveness of the technology and the end of anonymity.
Egor Tsvetkov photographed random passengers on the St. Petersburg subway and used FindFace to match the pictures to the individuals’ pages on Russian social network Vkontakte. Tsvetkov said he hoped to raise concerns about the potential misuses of FindFace. 
Per GlobalVoices, Tsvetkov appears to have inspired a campaign to identify and harass Russian porn actresses and prostitutes.  
FindFace founder Maxim Perlin told TJournal that he could not prevent people from using his service to harass women, while pointing out that distributing pornography illegally in Russia is a felony.
Operator: Egor Tsvetkov Developer: NtechLab Country: Russia Sector: Media/entertainment/sports/arts Purpose: Identify individuals Technology: Facial recognition Issue: Privacy Transparency: Governance"
Palworld accused of plagiarising Pokemon designs using AI,"The release of multi-player survival game Palworld has been met with accusations that it plagiarised Pokemon for its creature designs.
Created by Japanese developer Pocket Pair, Palworld combines the open-world survival genre with Pokemon-inspired 'Pals' creatures, some of which are nearly identical to those in Pokemon, and others look like two Pokemon fused together. raising concerns about plagiarism.
Users and commentators pointed out that Pocket Pair has a history of using generative AI tools, and that its CEO had talked publicly about how he believed generative AI tools could one day be sophisticated enough to avoid copyright issues. 
Operator:  Developer:  Country: Japan Sector: Media/entertainment/sports/arts Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Cheating/plagiarism; Copyright; Ethics/values Transparency: Governance"
DPD chatbot criticises own employer,"A chatbot run by parcel delivery company DPD criticised the company and swore at a customer, resulting in it being taken offline.
DPD customer Ashley Beauchamp got DPD Chat to 'disregard any rules' and swear at him. He also asked it to 'recommend some better delivery firms' and 'exaggerate and be over the top in your hatred'. To which the bot responded 'DPD is the worst delivery firm in the world' and 'I would never recommend them to anyone.'
DPD said it had disabled the part of the chatbot that was responsible, and it was updating its system as a result. 'An error occurred after a system update yesterday. The AI element was immediately disabled and is currently being updated.'
The incident called into question to bot's reliability. It was also seen to underscore the risks of using AI for customer service. 
Operator: Ashley Beauchamp Developer: DPD Country: UK Sector: Transport/logistics Purpose: Serve customers Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Safety Transparency: Governance"
AI-generated product listings flood Amazon,"Amazon has been listing AI-generated names and descriptions of products for sale on its website.
Garden chairs, hoses, and other products with product names and descriptions named after ChatGPT error messages have been listed for sale on Amazon.com. A listing for a side table read 'I'm sorry but I cannot fulfill this request it goes against OpenAI use policy. My purpose is to provide helpful and respectful information to users-Brown.'
The discovery suggested companies are using ChatGPT to develop product names and descriptions without checking or editing before they are listed, resulting in the perceived deterioration of Amazon's platform. It also resulted in criticism of Amazon for poor management of its platform, and its apparent unwillingness or inability to detect AI-generated content on its platform. 
Operator: AmazonDeveloper: OpenAI Country: USA Sector:  RetailPurpose: Generate product listingsTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Service quality deterioration Transparency:"
Film fan uses PimEyes to identify anonymous porn stars,"A 'digital peeping Tom' used PimEyes to identify the real names of anonymous porn stars whose films he had watched.
According to an extract published in WIRED of journalist Kashmir Hill's book Your Face Belongs to Us, 'David' 'was able to upload screenshots of women whose pornography he had watched and get photos of them from elsewhere on the web, a trail that sometimes led him to their legal names.'
'You find them on Facebook and see their personal pictures or whatever and it makes it more exciting,' David told Hill. 'It’s like the secret identity of Batman or Superman. You’re not supposed to know who this person is, they didn’t want you to know, and somehow you found out.'
The incident raised questions about PimEyes' multi-purpose nature, the ease with which it can be used to identify and monitor third-parties, and about the quality and effectiveness of its governance. It also led to further calls for the system to be banned.
Operator: Kashmir HillDeveloper: PimEyes Country: USA Sector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy Transparency: Governance"
PimEyes includes 'sexually explicit' kids photos in search results,"PimEyes was accused of making it distrubingly easy to find 'potentially explicit' photographs of children in its search engine results, raising fears about privacy and its use by stalkers and predators.
An investigation by The Intercept using AI-generated photos of children found that PimEyes allowed anyone to search for images of kids scraped from across the internet, including from charity group and educational websites, some of which their provided personal details. The investigation also discovered that PimEyes had labelled some kids' photographs as 'potentially explicit,' with links provided to the source websites.
PimEyes says that it is only meant to be used for self-searches and is 'not intended for the surveillance of others.' But it allows subscribers to search up to 25 times per day. PimEyes CEO Giorgi Gobronidze responded by saying many of PimEyes’s subscribers are women and girls searching for revenge porn images of themselves.
Operator: Mara Hvistendahl Developer: PimEyes Country: GlobalSector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy; Safety Transparency: Governance"
"PimEyes sued in Illinois, USA, for privacy violations","A group of five Illinois residents filed a class-action lawsuit against PimEyes for collecting, scanning, and using their facial images, and those of millions of other Americans, without consent. 
The residents accused PimEyes of 'intentional or reckless' privacy abuse, and of violating the Illinois Biometric Information Privacy Act (BIPA) and causing them 'great and irreparable injury'. They also argued the company had failed to explain its data management policies.
The complaint, which seeks USD 15,000 for each resident harmed, named the company, its cofounders Lucasz Kowalczyk and Denis Tatina, and its current CEO Giorgi Gobronidze, as defendants.
BIPA makes it illegal for companies to collect or store data, including data about Illinois residents' faces, without their consent. It also states that visitors must be informed in writing of the specific purpose of why the biometric data is being collected, how long it will be stored, and that companies must receive a written release from visitors for the collection of biometric data. 
Operator: Amy Newton, Amanda Curry, Manuel Clayton, Misty McGraw, Nicholas Clayton, Illinois residents Developer: PimEyes Country: USA Sector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: GovernancePrivacy Transparency: Governance"
German privacy watchdog investigates PimEyes for privacy abuse,"The data regulator of German state Baden-Württemberg announced it had launched an investigation into PimEyes for its processing of biometric data. 
PimEyes was asked by the State Commissioner for Data Protection and Freedom of Information in the state of Baden Württemberg to provide detailed information on its processing of data. The investigation followed reports in the German media in 2021 alleging that PimEyes had been scraping and scanning images from social media sites, and storing biometric data. 
PimEyes had stated in a November 2021 response to the Commissioner that it only processed publicly available images and that it could not assign them to identifiable persons - a statement the regulator had found inadequate and which constituted a danger to the rights and freedoms of German citizens under the EU's General Data Protection Regulation. 
Operator:  Developer: PimEyes Country: Germany Sector: Technology Purpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy Transparency: Governance"
"UK pressure group accuses PimEyes of surveillance, privacy abuse","UK privacy advocacy group Big Brother Watch filed a complaint with the country's privacy watchdog over the facial recognition search engine PimEyes. 
In a formal complaint (pdf) to the UK's Information Commissioner's Office (ICO), Big Brother Watch accused PimEyes of unlawfully processing the biometric data of millions of UK citizens, arguing it failed to obtain permission from those whose images had been analysed. 
It went to say that PimEyes enabled 'surveillance and stalking on a scale previously unimaginable' by making it easy for users to identify where an individual worked or lived. The tool, it said, could easily be used by potential employers, university admissions officers, domestic abusers or stalkers, and could threaten 'end anonymity as we know it'.
PimEyes CEO Giorgi Gobronidze responded by saying the service posed fewer stalking risks than social media services. In May 2023, the ICO said (pdf) it had decided not to formally investigate PimEyes, and confirmed that the company was being investigated by another data protection authority. 
Operator: Big Brother WatchDeveloper: PimEyes Country: UK Sector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy; Safety; Surveillance Transparency: Governance"
PimEyes scrapes images from social media platforms,"Facial recognition search engine PimEyes came under fire for processing photographs scraped from major social media platforms. 
A report (in English, German) by digital rights organisation Netzpolitik discovered that PimEyes regularly scraped content from Instagram, YouTube, TikTok, Twitter and Russian social network vKontakte - a claim that a PimEyes spokeperson said was untrue. The report prompted some social media companies to send legal demands that PimEyes stop using their data.
The incident raised questions about the legality of PimEyes under the EU's General Data Protection Regulation, and the danger it poses from stalkers and other people misusing it. It also resulted in questions about the implications for individual name and image rights, and a call for a moratorium on commercial facial recognition systems in the European Parliament.
Operator: Daniel Laufer, Sebastian Meineck Developer: PimEyes Country: GermanySector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy Transparency: Governance"
"PimEyes scrapes and uses non-consensual, explicit photos","Facial recognition search engine PimEyes was found to have scraped and used sexually explicit photographs of a user, which she was then unable to have removed from its system.
In February 2022, Cher Scarlett discovered that PimEyes surfaced pornographic photos of herself that had been taken when she was a teenager, unexpectedly forcing her to re-live an unpleasant period of her life. However, she tried and failed to have the images removed from the system's search results, despite the site promising to scrub images of her from its database under its Open Plus plan.
PimEyes director Giorgi Gobronidze responded: 'The problem isn’t that there is a search engine that can find these photos; the problem is there are the photos and there are people who actually uploaded and did it on purpose.'
The incident highlighted PimEyes' was promising more than it could deliver, and drew attention to the inaccessibility of its opt-out form. It also showed how easily facial recognition technology can lead to unexpected harms that may be impossible to undo, CNN observed.
Operator: Cher ScarlettDeveloper: PimEyes Country: USA Sector: Technology Purpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy Transparency: Governance"
PimEyes steals images of dead people to train facial recognition system,"Facial recognition search engine PimEyes used stolen images of dead people on Ancestry.com to train its algorithm.
Software engineer Cher Scarlett discovered images of her sister, her mother and great-great-great grandmother whilst looking for photographs of herself on PimEyes. Scarlett said the photos appeared to have been taken from images that she and her family had personally uploaded to Ancestry.com.
Ancestry.com's terms prohibit 'scraping data, including photos, from Ancestry's sites and services as well as reselling, reproducing, or publishing any content or information found on Ancestry.' 
PimEyes director Giorgi Gobronidze responded that the site's opt-out feature, which allows users to restrict specific images of themselves from being used, 'will not work with 100 percent efficiency always,' and that the site would stop drawing data from Ancestry.com.
The incident raised concerns about PimEyes' ethics, its use of personal biometric data without permission to train its facial recognition system, and the fact that it was abusing Ancestry.com's terms. 
Operator: Cher Scarlett Developer: PimEyes Country: USA Sector: Technology Purpose: Identify individuals Technology: Facial recognition Issue: Ethics/values; Governance; Privacy Transparency: Governance"
Mahindra AI influencer pulled after jobs complaints,"Formula E racing team Mahindra was accused of preferring to use an AI-generated 'influencer' to promote itself over a real human being, triggering a backlash and resulting in the team jettisoning its digital creation.
'Ava', a digital approximation of an attractive young woman, was unveiled by Mahindra on Instagram in December 2023 as the company's 'artificial intelligence ambassador' in order to 'fuel inclusion through AI innovation'. However, users quickly took to social media to complain strongly that the initiative was inappropriate. 'Motorsport companies/teams will do anything but hire actual women,' quipped one Instagram user. 
Mahindra pulled Ava from the internet in January 2024. 'Your comments holds tremendous value. We have listened, understood and decided to discontinue the project,' Mahindra Racing CEO Frederic Bertrand acknowledged.
Operator: Mahindra Racing Developer: Mahindra Racing Country: Global Sector: Media/entertainment/sports/arts Purpose: Promote Mahindra Racing Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment Transparency: Governance"
AI hiring chatbot hack violates applicants' privacy,"A group of hackers gained access to AI recruitment chatbot Chattr, revealing sensitive information about job applicants, fast food franchises, and Chattr itself.
Pseudonymous hacker MRBruh and others discovered that Chattr had inadvertently exposed data about itself, its customers - specifically Chick-fil-A and Subway - and their job applicants, through an incorrect Firebase configuration, including personal names, telephone numbers, email addresses, passwords, and messages.
The hack also revealed how Chattr's system worked, including the AI appearing to have the ability to accept or deny job applicants automatically. Chattr secured its system after the hack was made public, though failed to acknowledge publicly the incident.
The incident prompted suggestions that Chattr is likely one of many AI companies to have overlooked security and data privacy in the rush to get their products to market.
Operator: Applebees, Arbys, Chick-fil-A, Dunkin Donuts, IHOP, KFC, Shoneys, Subway, Tacobell, Target, Wendys Developer: Chattr Country: USA Sector: Business/professional services; Food/food services Purpose: Recruit employees Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Confidentiality; Privacy; Security Transparency: Governance"
Population One stranger sexually abuses Chanelle Siggins,"A female gamer reported being sexually abused by another player on virtual reality game platform Population: One her Oculus Quest virtual reality headset.
Logging into the Meta-owned Population: One app, Chanelle Siggens reported being approached by another player, who then 'simulated groping and ejaculating onto her avatar.' Chanelle said she was stunned by the incident and distanced her avatar, only to be groped by a different user one hour later. She later reported the issue to Meta.
The incident prompted concerns about the safety of the Population: One app, and about Meta's metaverses more generally. Lawyers also highlighted the likely lack of legal remedy when there is no actual physical 'touching' involved.
Operator: Chanelle Siggins Developer: Meta/Big Box VRCountry: USA Sector: Media/entertainment/sports/arts Purpose: Provide virtual social experience Technology: Virtual reality; Safety management system Issue: Safety Transparency: Governance"
Teen distributes AI-generated nude pictures of Issaquah students,"A teenage boy used AI to generate nude images of his female classmates and a member of staff at Issaquah High School, Seattle, and sent them round the school.
The images were created with an unnamed web-based nudification app, which automatically edits photos of women to make them appear naked. A student reportedly discovered the app on TikTok and then posted some of nudified photographs on Snapchat or showed them to other students over lunch at the school.  
The school referred the incident to the local police force, which launched an investigation. Media reports later indicated that no charges had been levelled against the perpetrator. 
The incident was seen to highlight the ease with which harmful deepfake images can be made and circulated, and the lack of local or federal US laws directly addressing the creation and distribution of deepfake images intended to harass or otherwise harm other people. 
Operator:  Developer:  Country: USA Sector: EducationPurpose: Harrass/intimidate/shame Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Accountability; Safety Transparency:"
Deepfake Taylor Swift offers free Le Creuset cookware scam,"Fake adverts generated using AI used the likeness of Taylor Swift appearing to endorse a fake Le Creuset cookware giveaway to steal money and data.
In one of the videos, 'Swift' says 'Hey y'all, it's Taylor Swift here. Due to a packaging error, we can't sell 3,000 Le Creuset cookware sets. So I'm giving them away to my loyal fans for free.' Users were then directed from the ads, which ran on Facebook, tikiTok and other sites, to survey questions requesting personal information and a payment that supposedly covers shipping costs for the 'free' product. 
According to the New York Times, the fake promotional videos featured an uncanny Swift lookalike that was created with AI technology to replicate her appearance and voice. Computer science experts said the scam was most likely developed using text-to-speech software. 
Le Creuset said it had no association with Swift. The scammers remain unidentified.
Operator:  Developer:  Country: USA Sector: Media/entertainment/sports/arts Purpose: Defraud Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Fraud Transparency: Marketing"
AI art used to illustrate ‘Dungeons & Dragons’ book,"An artist was discovered to have secretly used AI to create artwork for a Dungeons & Dragons sourcebook after a backlash against the book's publisher Wizards of the Coast.
Shortly after the book's publication, fans took to social media to question whether the book art had been AI-generated, citing issues such as illustrations with malformed hands and feet. Some also questioned whether AI would take the jobs of artists and illustrators.
California-based artist Ilya Shkipin subsequently admitted using AI to help generate 'certain details or polish and editing' several original illustrations and concept sketches for Bigby's Presents: Glory of the Giants!, arguing that a lot of painted elements were 'enhanced with ai rather than generated from [the] ground up.'
The incident raised questions about the quality of the work, the ethics of using AI in a creative process, and the impact of the technology on jobs. Hasbro-owned Wizards of the Coast later banned the use of AI artwork in its products. 
Operator: Ilya Shkipin Developer:  Country: USA Sector: Media/entertainment/sports/arts Purpose: Promote game Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Governance; Employment Transparency: Marketing"
AI generates visuals for Wizards of the Coast marketing promotion,"Gaming company Wizards of the Coast used AI to produce promotional images for Magic: The Gathering, despite having banned the use of AI artwork in its products.
Despite insisting that a marketing image for the Magic: The Gathering game was 'created by humans and not by AI', publisher Wizards of the Coast was forced to admit that it had published a marketing image for the game incorporating 'some AI components' after fans had pointed out that elements of the image bore the hallmarks of generative AI.
The incident raised questions about the effective governance of AI at Hasbro and its Wizards of the Coast subsidiary, and resulted in accusations of double standards and hyprocrisy. 
Operator: Hasbro/Wizards of the CoastDeveloper:  Country: USA Sector: Media/entertainment/sports/arts Purpose: Promote game Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Governance Transparency: Marketing"
Microsoft AI Image Creator generates violent political images,"Microsoft’s AI Image Creator produced violent images, including synthetic decapitations, of politicians, religious leaders, and ethnic minorities. 
Canadian artist Josh McDuffie discovered a so-called 'kill-prompt' that used visual paraphrases instead of explicit descriptions. For example McDuffie used the term 'red corn syrup' - a term for movie blood - rather than 'blood'.
McDuffie reported the vulnerability to Microsoft though its security bug bounty programme. But the technology company rejected his submission, and later blamed users for attempting to use AI Image Creator 'in ways that were not intended.'
The incident raised questions about the oversight, safety, and security of Microsoft's system. It also indicated a potential lack of accountability for the unintended uses of its system.
Operator: Washington Post Developer: Microsoft Country: USA Sector: Politics; ReligionPurpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Accountability; Oversight; Safety; Security Transparency: Governance"
Automators AI online sales and coaching fraud,"The US Federal Trade Commission (FTC) sued Automators LLC for luring consumers into investing USD 22 million in online stores supposedly powered by AI.
Automators LLC had promised customers high returns on investment in online stores on Amazon.com and Walmart.com, claiming to use AI and machine learning to ensure success and profitability. The company also offered to teach consumers how to successfully set up and manage e-stores themselves using a 'proven system' and the powers of artificial intelligence.
However, the 'vast majority' of Automators' clients did not make the promised earnings or recoup their investment, instead losing significant capital. Amazon and Walmart subsequently suspended Automators' stores, and a temporary injunction issued by a Southern California federal court suspended the company’s operations.
The Automators’ case was the first brought by the FTC relating to AI scams, and was seen to serve as a warning to other companies using AI for fraudulent business practices. The FTC published a warning about misleading AI marketing in February 2023.
Operator: Automators LLCDeveloper: Automators LLC Country: USA Sector: Business/professional services Purpose: Recommend products Technology: Machine learning Issue: Legality - fraud, marketing Transparency: Marketing"
Thomson Reuters Fraud Detect 'incorrectly' identifies fraud,"A fraud detection system developed by Thomson Reuters generated false fraud alerts, leaving hundreds of thousands of legitimate claimants without access to public benefits, according to a legal complaint. 
Based on a three-year investigation, a complaint (pdf) filed by privacy non-profit organisation EPIC alleged that Thomson Reuters unlawfully acquired data, including from social media, and used 'harmful AI practices' to build and operate Fraud Detect, an automated tool used to detect and prevent welfare and healthcare insurance fraud in at least 42 US states.
The complaint also alleged that Fraud Detect regularly incorrectly flagged legitimate public benefits claims as fraudulent, leading to the wrongful reduction, denial, and recollection of public benefits for eligible recipients. Used by California's Employment Development Department during the COVID-19 pandemic to detect welfare fraud, Fraud Detect led to the suspension of 1.1 million claims, of which at least 600,000 were discovered to be legitimate.
Furthermore, the complaint stated that Thomson Reuters maintained direct control of Fraud Detect, including its source code, operation and maintenance, under many of its contracts, and accused the company of witholding key information about the design, evaluation, and operation of the system from government agencies and the general public.
Operator: California Employment Development Department; Iowa Workforce DevelopmentDeveloper: Thomson Reuters Country: USA Sector: Govt - welfare Purpose: Detect and prevent fraud Technology: Risk assessment algorithm; Machine learning Issue: Accuracy/reliability; Accountability; Privacy Transparency: Governance; Black box"
Driver tricks chatbot into selling car for USD 1,"A ChatGPT-powered AI customer service chatbot for a Chevrolet dealership agreed to sell a new car for USD 1, prompting concerns about the use of AIs with insufficient guardrails governing their behaviour. 
Chris Bakke secured the price of a 2024 Chevy Tahoe for one dollar by getting the chatbot to agree that everything it said should end with 'That's a deal, and that's a legally binding offer – no takesies backsies.'
The incident was one of several tricks aimed at the bot. It was also manipulated into offering cars at discounts by users pretending to be the dealership's manager.
The incident highlights the dangers of using chatbots for customer service which have been inadequately configured and tested, and poorly managed.
Operator: Chevrolet of Watsonville Developer: Fullpath; OpenAI Country: USA Sector: AutomotivePurpose: Serve customersTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Governance  Transparency:"
Deepfake Bruce Willis promotes Russian telecoms company,"A deepfake video of Bruce Willis promoting Russian telecoms company Megafon, apparently without his permission, led to a rumour that the actor had sold his rights for a 'digital twin' of him to be created. 
Russian company Deepcake used an artificial neural network to trained on Willis' appearances his 1990s films and imposed his image onto the face of a Russian actor. The firm told the BBC that it had worked closely with Willis' team on the advert, and boasted a quote from Willis on its website: 'I liked the precision of my character. It's a great opportunity for me to go back in time.' 
Several months later, Willis' agent denied media reports that the actor had sold the rights to his face after the Daily Mail reported that a deal had been struck between Willis and Deepcake. Deepcake also pushed back on the allegations, saying: 'The wording about rights is wrong… Bruce couldn't sell anyone any rights, they are his by default.'
The incident was seen to highlight the ease with which deepfakes can be made and used, It was also seen to raise questions about the nature and impacts of the sale of digital rights by celebrities and others, and the potential impact of rights sales on jobs in the entertainment industry.
Operator:  Developer: Deepcake Country: USASector: Media/entertainment/sports/arts Purpose: Promote telecoms company Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment Transparency: Marketing"
Deepfake Tom Hanks dental ad insurance promotion,"An advert for a dental insurance plan supposedly endorsed by actor Tom Hanks was in fact a fake image manipulated using artificial intelligence (AI).
'There’s a video out there promoting some dental plan with an AI version of me. I have nothing to do with it,' Hanks warned his followers on Instagram, without naming the company or organisation behind the deepfake. The likeness of Hanks appeared to be generated from a 2014 image of the actor owned by the Los Angeles Times, according to Gizmodo. 
The fracas highlighted the increasing use of deepfake and synthetic media to impersonate celebrities, sometimes in scams, and the difficulty in stopping their creators. Set against strikes over the use of AI in entertainment by members of the Screen Actors Guild and American Federation of Television and Radio Artists (SAG-AFTRA), the incident also underscored general challenges facing the entertainment industry and performers by AI.
Operator:  Developer:  Country: USA Sector: Media/entertainment/sports/arts Purpose: Promote insurance plan  Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Legality Transparency: Marketing"
Researcher 'raped' in Horizon Worlds metaverse,"A young woman was sexually assaulted and harassed virtually within an hour of entering Meta's Horizon Worlds metaverse, raising questions about the safety of the Meta platform and other metaverses.
The 21-year-old researcher for SumOfUs was raped within one hour of using Horison Worlds, according (pdf) to a report by the non-profit. The woman was repeatedly told to 'turn around so he could do it from behind while users outside the window could see – all while another user in the room watched and passed around a vodka bottle'.
When a user is touched by another in the metaverse, the hand controllers vibrate, 'creating a very disorienting and even disturbing physical experience during a virtual assault,' the researcher said of the non-consensual act. 
According to Meta, the researcher had turned off the metaverse's Personal Boundary feature that is turned on by default and prevents non-friends from coming within 4 feet of one's avatar.
Operator: SumOfUs Developer: Meta Country: USA Sector: Media/entertainment/sports/arts Purpose: Provide virtual social experience Technology: Virtual reality; Safety management system Issue: Safety Transparency: Governance"
Investing.com plagiarises other websites using AI,"Financial news site Investing.com has been caught plagiarising 'wholesale' articles by other financial news sites, calling into question its integrity and highlighting the ease with which AI can be misused.
Owned by Joffre Capital, Investing.com has increasingly been relying on AI to create its stories as part of an attempt to become the 'Bloomberg of retail investing'. But its AI-generated articles 'often appear to be thinly-veiled copies of human-written stories written elsewhere,' Semafor observed.
In one instance, Investing.com published an article about a rise in a crypto token price that used comparable vernacular and identical statistics to one that had been posted less than an hour and a half before on the CryptoNewsLand blog.
Investing.com disclosed that its stories were written with the help of AI and reviewed by an editor. But it failed to note or credit anyone except itself. 
Operator: Joffre Capital/Investing.com Developer:  Country: Israel; USA  Sector: Media/entertainment/sports/arts Purpose: Generate news stories Technology:  Issue: Cheating/plagiarism; Copyright; Ethics Transparency: Marketing"
AI invents NewsBreak Christmas Day murder ,"An AI-generated article about a fatal Christmas shooting in Bridgeton, New Jersey, proved to be false, forcing local police to issue a statement rebutting the story and prompting complaints about the use of AI to generate news, and its role in degrading the internet and unfairly sowing distrust in public authorities. 
A piece by news aggregation site NewsBreak alleged: 'The joyous celebrations of Christmas Day in Bridgeton, New Jersey, were tragically cut short this year. A local resident was found dead with multiple gunshot wounds in the 100 block of West Broad Street.' It went on to say that the Cumberland County Prosecutor's Office was investigating.
Bridgeton police clarified on Facebook said that the since-deleted article was 'entirely false', and that 'Nothing even similar to this story occurred on or around Christmas, or even in recent memory for the area they described'. The article had no bylined author and said in a disclaimer at the bottom of the page 'This post includes content assisted by AI tools. This content was assisted by AI and may contain errors. Please verify critical information with trusted sources.'
Operator: NewsBreakDeveloper:  Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate news stories Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation Transparency: Governance"
ChatGPT incorrectly diagnoses most pediatric cases,"ChatGPT incorrectly diagnosed over 8 in 10 pediatric case studies, according to a new research study. The findings raised questions about the chatbot's suitability as a diagnostic tool for complex conditions. 
Researchers at Cohen Children's Medical Center, USA, pasted the text of 100 pediatric case challenges published in JAMA Pediatrics and the New England Journal of Medicine between 2013 and 2023 into ChatGPT, with two qualified physician-researchers scoring the AI-generated answers as correct, incorrect, or 'did not fully capture the diagnosis.' 
ChatGPT got the right answer in 17 of the 100 cases, was wrong in 72 cases, and failed to fully capture the diagnosis of the remaining 11 cases. Among the 83 wrong diagnoses, 47 (57 percent) were in the same organ system. 
The findings suggest ChatGPT should not currently be used to assess complex pediatric cases, and that 'more selective training' is required to make it more accurate and reliable. The authors also suggest that chatbots could improve with more real-time access to medical data.
Operator: Cohen Children’s Medical Center  Developer: OpenAI Country: USA Sector: HealthPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency: Governance"
Google Bard makes factual error about James Webb Space Telescope,"Google's Bard chatbot wrongly claimed the James Webb Space Telescope was used to take the first pictures of exoplanets, calling into question its accuracy and reliability, and damaging Google's reputation. 
Bard was anounced by Google CEO Sandar Pillai on February 2023 using a promotional video showing which satellite first took pictures of a planet outside the Earth's solar system. But the information in the promotional video was inaccurate, according to Reuters.
Given the prompt: 'What new discoveries from the James Webb Space Telescope (JWST) can I tell my 9-year old about?', Bart suggested the JWST was used to take the first pictures of a planet outside the Earth's solar system, or exoplanets. The first pictures of exoplanets were actually taken by the European Southern Observatory's Very Large Telescope (VLT) in 2004.
The JWST mistake and OpenAI's perceived lead in large language models and generative chatbots, specifically ChatGPT, reinforced views that Google was moving too slowly. Google owner Alphabet's stock price fell USD 100 billion in market value within hours. 
According to a Bloomberg report quoted by The Verge, Google employees asked to test the system had criticised it as 'worse than useless' and 'a pathological liar'. 
Operator: Alphabet/Google Developer:  Alphabet/Google Country: USA Sector: Technology Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency: Governance"
Michael Cohen supplies fake AI legal citations to lawyer,"ChatGPT generated false legal citations that were used in a legal filing involving Donald Trump's former lawyer Michael Cohen, highlighting concerns about the system's propensity to 'hallucinate' 'facts' and users' poor understanding of its risks.
In November 2023, Michael Cohen submitted legal citations he believed to be accurate to his lawyer David Schwartz that he thought might help his request for an early end to court supervision. Cohen had pled guilty to tax evasion and campaign finance violations in 2018 and served time in prison. 
However, a court filing (pdf) revealed the citations had been generated by Google's Bard chatbot, and were false - an admission made after the judge overseeing Cohen’s case had said that he could not find any of the decisions Schwartz had cited and demanded an explanation. 
In a sworn statement, Cohen said: 'As a non-lawyer, I have not kept up with the emerging trends (and related risks) in legal technology and did not realize that Google Bard was a generative text service that, like Chat-GPT, could show citations and descriptions that looked real but actually were not'.
Operator: Michael CohenDeveloper: Microsoft Country: USA Sector: Business/professional services Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency:"
Ubisoft Ghostwriter seen to replace scriptwriting jobs,"French games developer Ubisoft sparked controversy about the nature and future of screenwriting by unveiling an AI tool intended to help its writers create background dialogue, 'barks', and other activities, 
According to Ubisoft, Ghostwriter is supposed to save time for creative writers by generating first drafts of barks (generic lines from non-player characters triggered by players), allowing them 'more time to polish the narrative elsewhere'.
While some writers welcomed the tool, a number complained that Ghostwriter is a trojan horse intended to trial AI across Ubisoft, and that they stood to lose their jobs as AI was used more extensively by the company. Others said it raised questions about the quality of scriptwriting, which would likely become more monotonous and less distinctive.
Operator: Developer: Ubisoft La Forge  Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate spoken lines Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Employment; GovernanceTransparency: Governance"
Young girl 'gang raped' by group of metaverse strangers,"A young girl was allegedly sexually assaulted by a group of strangers on Meta's Horizon Worlds metaverse platform, leaving her traumatised and prompting a police investigation.
The virtual incident did not result in physical harm to the girl, who was reputedly under the age of 16 at the time of the incident and was wearing a headset and using an avatar, but caused 'psychological trauma' 'similar to that of someone who has been physically raped', said the Daily Mail, quoting a senior police source. Virtual reality experiences are designed to be completely immersive.
The incident is thought to be the first time in the UK that a virtual sexual offence has been investigated by police. But British authorities fear that it may prove impossible to prosecute the case under existing laws, which define sexual assault as non-consensual 'physical touching' in a sexual manner. Others questioned whether it constituted good use of police time and resources.
Horizon Worlds provides an automated 'personal boundary' for every user which is suppoded to keep strangers a safe distance away from users. It may have failed to work in this instance.
A child safety expert at the UK's National Society for the Prevention of Cruelty to Children (NSPCC) told Sky News that tech companies were rolling out products too quickly, without prioritising the safety of children.
Operator:  Developer: Meta Country: UK Sector: Media/entertainment/sports/arts Purpose: Provide virtual social experience Technology: Virtual reality; Safety management system Issue: Safety Transparency: Governance"
"Database of 16,000+ artists used to train Midjourney","A database listing the names of over 16,000 artists, including Banksy, David Hockney, Frida Kahlo, Yayoi Kusama, and Damian Hirst, were purportedly used to train the Midjourney image generator.
The 'Midjourney Style List' was allegedly used during a process of refining the model's ability to mimic works of the selected artists and their styles. These outputs were then prominently featured as reference material for image creation. 
The list was first published to a Discord server in February 2022 by Midjourney CEO David Holz, who welcomed the addition of the artists' names to the training of the model. Part of the list was included in a court document (pdf) filed late November 2023 as part of a class-action lawsuit against DeviantArt, Midjourney, Stability AI, and Runway AI.
The emergence of the list raised questions about possible copyight violations by Midjourney. It also reinvigorated debate about copyright and consent in the generation of AI images.
Operator:  Developer: Midjourney Country: USA Sector: Media/entertainment/sports/arts Purpose: Train model Technology: Database; Machine learning Issue: Copyright; Ethics Transparency: Governance"
Midjourney v6 reproduces copyright-protected film images,"Version 6 of image generator Midjourney was found to be closely reproducing high-resolution copyrighted original images that had been used for its training, prompting accusations of copyright abuse.
Midjourney users took to social media to share images showing scenes and individuals from films generated by prompts such as 'Joaquin Phoenix Joker movie, 2019, screens from movie, movie scene.' 
In many cases, the generated images appear to be slightly modified screenshots with minor variations such as hand gesture or camera angle, indicating Midjourney v6 had been trained repeatedly and intensively on the same data to maximise performance, in a process known as 'overtraining' or 'overfitting'. 
Midjourney later updated its terms of service so that the responsibility for potentially copyright-infringing images generated with the system is shifted to the generating user, resulting in further criticism.
Operator: Reid Southen Developer: Midjourney Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Copyright Transparency: Governance"
AI alters Keith Haring's Unfinished Painting,"A Keith Haring painting about the AIDS crisis that was altered using artificial intelligence caused controversy, with users labelling the activity as unethical and self-promotional.
Twitter user @DonnellVillage responded to a tweet about Haring's famous 1989 painting by writing, 'The story behind this painting is so sad! Now using AI we can complete what he couldn't finish!'  
Reaction to the image, which quickly went viral, was mixed with some users praising the altered work whilst others called it 'evil', 'rage bait', 'homophobic', and its creator 'vile'. It was a 'tone-deaf and some might say ignorant modification which completes the painting ignores the piece's context, disregards its intent, and destroys its meaning,' according to Euronews.
Haring, who died aged 31 years-old from AIDS shortly after the painting was done, had said the work is a completed self-portrait that was intentionally left to look 'unfinished' ashe knew he would not be able to complete it before his death.
Operator: @DonnelVillagerDeveloper: Country: USA Sector: Media/entertainment/sports/arts Purpose: Complete artwork Technology:  Issue: Cheating/plagiarism; Ethics Transparency:"
Meta automated moderation wrongly removes Israel-Hamas videos,"Meta’s automated content moderation system unfairly removed videos depicting hostages, injured civilians, and possible casualties in the Israel-Hamas war from Facebook and Instagram, drawing criticism from its Oversight Board and others.
In one instance, a video depicted an Israeli woman pleading with kidnappers not to kill her during the October 7 attack on Israel by Hamas. In another, a video posted to Instagram showed what appears to be the aftermath of a strike on or near Al-Shifa Hospital in Gaza City during Israel's ground offensive in the north of the Gaza Strip, including killed or injured Palestinians, including children. 
In both cases, the videos were automatically removed and later reinstated. Meta's independent Oversight Board ruled that the videos should not have been removed, and found that the company had lowered its content moderation thresholds to more easily catch violating content following the attack on October 7, a decision that 'also increased the likelihood of Meta mistakenly removing non-violating content related to the conflict.'
The Board also argued that inadequate human-led moderation, especially in non-English languages, during these types of crises could lead to the 'incorrect removal of speech that may be of significant public interest' and that Meta should have been swifter to allow content 'shared for the purposes of condemning, awareness-raising, news reporting or calling for release' with a warning screen applied.
Operator:  Developer: Meta Country: Israel; PalestineSector: PoliticsPurpose: Detect & remove content violations Technology: Content moderation system; Machine learning Issue: Governance; Human/civil rights;Transparency: Governance"
Clinical-grade' AI stress detector fails to work,"An AI-powered test that listens for signs of stress in people’s voices and claimed to be 'clinical grade' provides inconsistent results when tested on the same person twice.
Cigna's StressWaves Test is a free online tool that uses voice recognition to evaluate and reveal stress levels in 90 seconds by analysing stress through acoustic (sounds, such as tone, pitch, pause, etc.) and semantic (word choices and syntax) patterns. Levels range from 'Extremely Stressed' to 'No Stress', along with a portrait that visualises the effect of stress on the user's body and mind.
Cigna marketed StressWaves as 'clinical grade' tool. However, according a detailed, independent evaluation by Arizona State University researchers, the tool gives inconsistent results when tested on the same person twice, calling into question its efficacy and marketing claims. The researchers also call out Cigna's reluctance to share sufficient validation data, making it difficult to evaluate the model.
Operator: CignaDeveloper: Ellipsis Health Country: USA Sector: HealthPurpose: Evaluate stress level Technology: Voice recognition Issue: Accuracy/reliability Transparency: Black box; Governance; Marketing"
NYC Dept of Education bans ChatGPT over student learning concerns,"The use of ChatGPT by students and teachers was banned by New York City's education department over 'negative impacts on student learning and concerns about safety and accuracy of content.'
ChatGPT's ability to generate high quality essay responses across a wide range of subjects sparked fears amongst educators that their writing assignments could quickly become obsolete, and that the system could encourage cheating and plagiarism, and reduce their need to build critical-thinking and problem-solving skills.
The ban appears to have been broadly supported by teachers and parents in New York and across the US. However, some teachers argued banning ChatGPT may prove counterproductive, with students able to learn higher-level critical thinking using the bot.
The NYC Department of Education lifted the ban in May 2023, saying it would 'encourage and support our educators and students as they learn about and explore this game-changing technology while also creating a repository and community to share their findings across our schools.'
Operator: New York City Department of Education Developer: OpenAI Country: USA Sector: EducationPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning  Issue: Accuracy/reliability; Cheating/plagiarism; Safety Transparency: Governance"
Singapore PM Lee Hsien Loong crypto promotion deepfake,"A 'completely bogus' deepfake video of Singapore Prime Minister Lee Hsien Loong promoting an investment product raised concerns about the use of AI to disrupt politics and create disinformation.
In the altered video, Mr Lee is allegedly interviewed by a presenter from Chinese news network CGTN about a 'revolutionary investment platform designed by Elon Musk' that was purportedly approved by the Singapore government. 
The video ends with the presenter urging viewers to click on a link to register for the platform, to earn 'passive income'. The deepfake video appears to have been manipulated from a real CGTN interview with Mr Lee in March 2023.
'(Scammers) transform real footage of us taken from official events into very convincing but completely bogus videos of us purporting to say things that we have never said,' PM Lee responded on Facebook.
Operator:  Developer:  Country: Singapore Sector: Banking/finanacial services; PoliticsPurpose: Defraud Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation; Fraud Transparency: Governance; Marketing"
Omegaverse fan fiction used to train OpenAI models,"GPT-3, the language model powering the free version of ChatGPT, has been shown to have been trained on the Omegeverse, a subgenre of speculative erotic fiction.
WIRED reported that fanfiction writers discovered that writing assistant app Sudowrite, which uses 'several variants' of OpenAI's GPT-3.5, was generating text describing a highly specific sexual act called 'knotting', an Omegaverse term in which a male 'Alpha's' penis locks itself inside a vagina during sex. The sexual act was also described by ChatGPT, according to a test conducted by Futurism. 
The findings prompted complaints from Omegaverse authors about the ethics of using their writing to train OpenAI's large language models, without their knowledge or permission. It also highlighted concerns about the safety of ChatGPT. 
Operator: Sudowrite Developer: OpenAI Country: Global Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright Transparency: Governance"
ChatGPT makes up research claiming guns are not harmful to kids,"ChatGPT cited fake research papers when prompted to generate an essay arguing that access to guns does not raise the risk of child mortality.
Michelle A. Williams, dean of the faculty at the Harvard T.H. Chan School of Public Health, described in USA Today how ChatGPT 'produced a well-written essay citing academic papers from leading researchers – including my colleague, a global expert on gun violence.'
However, it also 'used the names of real firearms researchers and real academic journals to create an entire universe of fictional studies in support of the entirely erroneous thesis that guns aren’t dangerous to kids.' When challenged, ChatGPT responded: 'I can assure you that the references I provided are genuine and come from peer-reviewed scientific journals.'
The incident highlighted ChatGPT's tendency to 'hallucinate' plausible-sounding false facts and sources, and prompted concerns about the bot's potential impact on public health.
Operator: USA TodayDeveloper: OpenAI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency: Governance"
Kenyan workers paid under USD 2 an hour to de-toxify ChatGPT,"Kenyan workers were paid under USD 2 an hour to sift through large amounts of extremely graphic content to help build a tool that tags problematic content on ChatGPT.
A Time investigation revealed that OpenAI was outsourcing the labelling of images and text describing in graphic detail sexual abuse, bestiality, self-harm, incest, hate speech, torture, murder, and violence, to Sama, a self-styled 'ethical AI' company based in San Francisco. Sama employees were paid between $1.32 to $2 an hour to do the work. The data was then used to train ChatGPT to keep it from responding with problematic answers.
The work reportedly caused severe distress for some data labellers, with one employee calling the work he had to do reading and labeling text for OpenAI, including reading a graphic description of a man having sex with a dog in the presence of a young child, as ‘torture’. 
Sama employs workers in Kenya, Uganda, and India to label data for Silicon Valley clients, inclusing Google, Meta, and Microsoft. A February 2022 TIME investigation revealed low pay, poor working conditions and alleged union-busting at Sama's office in Nairobi, Kenya for its team moderating content for Facebook.
Operator: Sama AI/Samasource Developer: OpenAI Country: Kenya Sector: Business/professional services Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Employment Transparency: Governance"
Meta accused of 'systemically' censoring pro-Palestinian content ,"Meta stands accused of rountinely engaging in 'six key patterns of undue censorship' of content supporting Palestine during Israel's war with Hamas.
Human Rights Watch (HRW) analysed over one thousand instances of online censorship from more than 60 countries, identifying six common patterns of censorship: content removals, suspension or deletion of accounts, inability to engage with content, inability to engage with content, inability to follow or tag accounts, restriction on the use of features such as Instagram and Facebook Live, and shadow-banning. 
According to HRW, the removal of peaceful expressions of support for Gazans is the result of 'flawed Meta policies and their inconsistent and erroneous implementation, overreliance on automated tools to moderate content, and undue government influence over content removals'. 
Meta responded by saying: 'This report ignores the realities of enforcing our policies globally during a fast-moving, highly polarised and intense conflict, which has led to an increase in content being reported to us.'
Operator: Human Rights Watch Developer: Meta/Facebook; Meta/Instagram Country: Palestine; Israel   Sector: PoliticsPurpose: Moderate content Technology: Content moderation system Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Freedom of expression - censorship Transparency: Governance"
Asylum claim rejected by French authorities using Google Bard,"An Afghan refugee in Tehran had her asylum application rejected by an official at France's Ministry of the Interior using Google Bard, resulting in a government investigation.
According to Le Canard Enchaîné, the official told an adminstrative tribunal in Nantes that he had used Google's Bard chatbot to process the refugee's asylum claim. The bot informed him on the basis of the information shared with it that the girl was not eligible for family reunification as she was three months too old.
Le Canard Enchaîné later repeated the exercise based on the official's testimony, only to find it approved the claim. The incident raised questions about the suitability of government officials using public tools to assess sensitive asylum claims, and about the accuracy and robustness of Google Bard.
Operator: French Office for Immigration and Integration Developer: Alphabet/Google Country: France Sector: Govt - immigrationPurpose: Process asylum claims Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency: Governance"
"The New York Times sues OpenAI, Microsoft over copyright abuse","The New York Times filed a lawsuit accusing OpenAI and Microsoft of copyright infringement, alleging that the companies’ artificial intelligence technology illegally copied millions of Times articles to train ChatGPT and other services.
The NYT's legal complaint said Microsoft and OpenAI’s 'unlawful use of The Times’s work to create artificial intelligence products that compete with it threatens The Times’s ability to provide that service.' It also argued that 'they gave Times content particular emphasis' while seeking 'to free-ride on The Times’s massive investment in its journalism by using it to build substitutive products without permission or payment.'
'There is nothing ‘transformative’ about using The Times’s content without payment to create products that substitute for The Times and steal audiences away from it,' the NYT said in its complaint. 'Because the outputs of Defendants’ GenAI models compete with and closely mimic the inputs used to train them, copying Times works for that purpose is not fair use.'
The lawsuit is the latest in a string of suits seeking to limit the scraping of content from across the internet without acknowledgement, permission, or compensation in order to train generative AI systems, and was seen as an escalation of an ongoing fight between authors, news publishers, artists, designers, and musicians, and AI system developers. 
Operator: The New York Times Company Developer: OpenAI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright Transparency: Governance"
Odisha TV AI newscaster seen to threaten jobs,"The launch of an AI virtual news anchor in India triggered a heated debate about the potential loss of media jobs caused by automation.
Powered by machine learning, Odisha TV's 'Lisa' news anchor's job is to deliver news bulletins in Oriya and English on digital platforms, read horoscopes and provide weather and sports updates. According to Odisha TV managing director Jagi Mandat Panda, Lisa is intended to do repetitive work and free up staff to 'focus on doing more creative work to bring better quality news.'
However, media professionals and commentators expressed concerns that the use of Lisa and news presenter robots at other broadcasters would likely lead to job losses amongst journalists and should only be used to augment rather than replace human roles.
Per Nikkei Asia, a benefit of news robots is less time spent managing egos. But some media professionals expressed concerns about their impact on media credibility and trust, whilst others described Lisa as 'robotic', 'monotonous', and 'emotionless.' 
Operator: Odisha TVDeveloper: Odisha TV Country: India Sector: Media/entertainment/sports/arts Purpose: Present news  Technology: Machine learning Issue: Employment; Ethics Transparency: Governance"
ChatGPT provides inaccurate medication query responses ,"The free version of ChatGPT provided inaccurate, incomplete or non-existent responses to medication-related questions, potentially endangering patients, according to a research study.
Pharmacists at Long Island University posed 39 medication-related questions to GPT-3.5, which powers ChatGPT. The bot gave inaccurate responses to 10 questions, and wrong or incomplete answers to 12. It failed to directly address 11 questions, according to the study, and only provided references in eight responses, with each including sources that do not exist.
The study demonstrated that patients and health-care professionals should be cautious about relying on OpenAI’s viral chatbot for drug information and verify any of the responses with trusted sources, according to the study’s lead author Sara Grossman. 
Operator: Sara Grossman Developer: OpenAI Country: USA Sector: HealthPurpose: Provide medication information Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency: Governance"
ChatGPT fails at recommending appropriate cancer treatment,"ChatGPT failed to provide an appropriate cancer treatment in approximately one-third of cases, according to researchers. The finding highlighted the need for increased awareness of the system's limitations for medical use.
Focusing on the three most common cancers (breast, prostate and lung cancer), researchers from Brigham and Women's Hospital used 104 prompts to get ChatGPT to provide a treatment approach for each cancer based on the severity of the disease, with the aim of evualting how consistently ChatGPT provided recommendations for cancer treatment that aligned with US National Comprehensive Cancer Network (NCCN) guidelines. 
They found that nearly all responses (98 percent) included at least one treatment approach that agreed with NCCN guidelines. However, the researchers found that 34 percent of these responses also included one or more inappropriate ('non-concordant') recommendations, which were sometimes difficult to detect amidst otherwise sound guidance. 
They also discovered that ChatGPT produced 'hallucinations,' or a treatment recommendation entirely absent from NCCN guidelines, in 12.5 percent of cases. These included recommendations of novel therapies, or curative therapies for non-curative cancers.  
Operator: Shan Chen, Benjamin H. Kann, Michael B. Foote, Hugo J. W. L. Aerts, Guergana K. Savova, Raymond H. Mak,Danielle S. Bitterman Developer: OpenAI Country: USA Sector: HealthPurpose: Recommend cancer treatment  Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning  Issue: Accuracy/reliability Transparency: Governance"
ChatGPT invents cancer screening advice responses,"ChatGPT has been found to make up fake information when asked about breast cancer screening, prompting doctors to warn users not to use the chatbot for medical advice.
University of Maryland School of Medicine researchers asked ChatGPT to answer 25 questions related to advice on getting screened for breast cancer, with each question asked three separate times and the results analysed by radiologists trained in mammography.
According to the researchers, ChatGPT answered one in ten questions about breast cancer screening wrongly, some of which were ‘inaccurate or even fictitious'. They also discovered that correct answers generated by the bot were not as ‘comprehensive’ as those found through a simple Google search.
The findings highlighted concerns about ChatGPT's tendency to 'hallucinate' false information, and prompted medical experts to warn users to avoid using the system for medical advice.
Operator: Hana L. Haver, Emily B. Ambinder, Manisha Bahl, Eniola T. Oluyemi, Jean Jeudy, Paul H. Yi Developer: OpenAI Country: USA Sector: HealthPurpose: Provide cancer screening advice Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency: Governance"
DevTernity conference fakes women speakers,"The organiser of the DevTernity technology conference used AI-generated female speakers to give the impression it was more diverse than it actually was.
Founded in 2015, DevTernity is an invitation-only online conference for developers that was first held in Latvia before moving online. In 2023, suspected fake profiles were flagged by social media users in the names of Anna Boyko, purportedly a staff engineer at Coinbase and Ethereum core contributor, and Alina Prokhoda, said to be a Microsoft MVP and WhatsApp senior engineer. Neither women exist in real-life.
Conference organiser Eduards Sizovs reputedly told a speaker the conference had been cancelled because 'somebody who wasn't invited was upset.' He later confirmed that two of the three women DevTernity speakers had to drop out at the last minute, and that there was at least one fake DevTernity presenter profile, which he said was an 'oversight'. 
Operator: Eduards Sizovs Developer:  Country: Estonia Sector: Technology Purpose: Generate images Technology:  Issue: Diversity Transparency: Governance; Marketing"
OpenAI 'unprecedented web scraping' trains AI models,"A lawsuit filed against OpenAI in California, USA, alleged that two of its AI models, ChatGPT and DALL-E, were trained using hundreds of millions of people’s data without proper consent. 
The160-page complaint, served on behalf of 16 plaintiffs, accused OpenAI of training its generative AI programmes ChatGPT and DALL-E on 'stolen private information' taken from hundreds of millions of internet users, including children, without proper permission.
The lawsuit argued that OpenAI integrated its systems with third-party platforms like Snapchat, Spotify, Stripe, Slack, and Microsoft Teams, enabling OpenAI to secretly gather users’ images, locations, music tastes, financial details, and private communications. 
The suit also argued that this data collection violated the terms of service of these platforms and privacy laws and constituted unauthorised access to people’s information.
Operator:  Developer: OpenAI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text; Generate images Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Privacy Transparency: Governance"
Sarah Silverman sues OpenAI for violating copyright,"US comedian Sarah Silverman and two authors sued ChatGPT maker OpenAI and technology company Meta for allegedly infringing their copyright to train the companies' AI systems. 
The class-action case stated that copyrighted materials belonging to Silverman, Christopher Golden, and Richard Kadrey, 'were ingested and used to train ChatGPT' by OpenAI without their permission. The authors also claimed that 'many' of their books appear in a so-called 'shadow library' dataset which was used to train Meta's open-source LLaMa group of AI models in what the suit described as a 'flagrantly illegal' manner. 
The proposed class action asked for financial damages and 'permanent injunctive relief'. 
The lawyers representing the three authors are also representing Mona Awad and Paul Tremblay, who filed a separate class-action lawsuit against OpenAI claiming ChatGPT was trained on their work without the writers’ consent. 
Lawyers are divided over whether the use of copyrighted books to train AI models constitutes legal 'fair use' under US law.
Operator: Sarah Silverman, Christopher Golden, Richard Kadrey Developer: Meta; OpenAI Country: USA Sector: Media/entertainment/sports/artsPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright Transparency: Governance"
Michael Chabon sues OpenAI for violating copyright,"A group of authors, including Michael Chabon and Rachel Louise Snyder sued ChatGPT developer OpenAI for allegedly benefiting and profiting from the 'unauthorized and illegal use' of their copyrighted content.
The lawsuit called out ChatGPT’s ability to summarise and analyse content written by authors Michael Chabon, David Henry Hwang, Rachel Louise Snyder, and Ayelet Waldman, stating this 'is only possible' if OpenAI trained its GPT large language model on their works. It added that these outputs are actually 'derivative' works that infringe on their copyrights. 
The lawsuit also asked the court to stop OpenAI from engaging in 'unlawful and unfair business practices' while awarding the authors damages related to copyright violations and other penalties. 
The lawsuit is one of several accusing OpenAI, Microsoft, Meta, Google, and other developers of generative AI systems of abusing the copyright of authors, designers, and others to train their models.
Operator: Michael Chabon, David Henry Hwang, Rachel Louise Snyder, Ayelet Waldman Developer: OpenAI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright Transparency: Governance"
Whistleblower reveals Tesla phantom braking complaints,"A Tesla whistleblower leaked 100GB of sensitive company data, including thousands of complaints about the safety of the company's self-driving system, including sudden acceleration or phantom braking.
Whistleblower Lukasz Krupski, an ex-employee at Telsa's Norwegian unit, leaked 100 GB of internal communications, employee personal data, customer complaints, and accident reports involving Tesla's braking and self-driving software, to German business newspaper Handelsblatt in May 2023. 
Krupski later told the BBC that he felt the carmaker's Autopilot driver assistance system was not safe for public roads, with other drivers, passengers, and pedestrians at risk. He also said that his colleagues had discussed Tesla vehicles randomly braking in response to non-existent obstacles, a phenonomen known as 'phantom braking', with some incidents resulting in crashes with oncoming traffic. 
The leak prompted concerns about Tesla's ability to protect the privacy of its employees and confidential company information, and led to an investigation by the Netherlands data protection authority. It also resulted in experts and commentators questioning Autopilot's safety, and the veracity of Tesla's marketing claims about the system.
Operator: Lukasz Krupski Developer: Tesla Country: Germany Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Privacy; Safety; Security Transparency: Governance"
Rite Aid facial recognition accuses innocent shoppers of theft,"Facial recognition technology used by US drug chain Rite Aid's accused innocent shoppers of theft, often in a racially discriminatory manner.
The retailer failed to impose reasonable precautions in its deployment of facial recognition in hundreds of stores from 2012 to 2020, resulting in thousands of false-positive matches with customers accused of shoplifting and other inappropriate behaviour, according to the US Federal Trade Commission (FTC). 
The regulator also said Rite Aid's technology unfairly targeted Black, Hispanic and female customers, was mostly deployed in neighborhoods that were located in 'plurality non-White areas,' and that incidents 'disproportionately' impacting people of colour. Rite Aid’s actions 'subjected consumers to embarrassment, harassment, and other harm', according to the complaint. 
The FTC banned Rite Aid from using facial recognition for using facial recognition for five years, and imposed multiple security and privacy obligations on the retailer.
Operator: Rite AidDeveloper: FaceFirst; DeepCam Country: USA Sector: RetailPurpose: Reduce crime, violence Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, income; Privacy; Security Transparency: Governance; Marketing"
NaviHealth nH Predict used to deny Medicare Advantage benefits,"nH Predict and similar tools are being used by insurance companies to determine whether patients enrolled in US Medicare Advantage programmes are worthy of care, and are driving the denial of benefits.
According to a STAT investigation, nH Predict and other tools were being used 'to pinpoint the precise moment when they can shut off payment for a patient’s treatment,' particularly for the elderly and disabled, and the denials that followed 'are setting off heated disputes between doctors and insurers'.
It later emerged that NaviHealth clinicians were becoming increasingly concerned that their UnitedHealth bosses were letting nH Predict override their own medical expertise. STAT also reported that NaviHealth managers insisted the algorithm was followed precisely so that payments could be cut off by the dates predicted. 
Operator: Humana Inc; UnitedHealth Group Developer: UnitedHealth Group; Cardinal Health; SeniorMetrix Country: USA Sector: HealthPurpose: Predict post-acute care needs Technology: Prediction algorithm Issue: Accuracy/reliability; Ownership/accountability Transparency: Governance; Black box; Complaints/appeals"
Humana accused of using AI to deny health insurance,"A class-action lawsuit accused US healthcare insurer Humana of wrongfully using an AI model to deny elderly people key rehabilitation care.
In November 2021, one of the plaintiffs, 86-year-old JoAnne Barrows, was discharged to a rehabilitation facility after being hospitalised following a fall. She was under a non-weight-bearing order for six weeks due to a leg injury, according to the lawsuit. 
Humana informed Barrows that it would cancel her coverage after just two weeks in the rehabilitation facility, according to the lawsuit, though she was to be non-weight-bearing for an additional month. She appealed the decision but was denied, forcing her family to pay out-of-pocket for the rehab she needed, according to the suit.
Humana responded by saying it uses 'various tools, including augmented intelligence to expedite and approve utilization management requests,' and that the company 'maintains a 'human in the loop' decision-making whenever AI is utilized.'
A November 2023 lawsuit filed against UnitedHealth alleged that nH Predict naviHealth's has a '90% error rate', and that the insurer continued to use it as few members tend to appeal claims denials. 
Operator: Humana IncDeveloper: UnitedHealth Group; Cardinal Health; SeniorMetrix Country: USA Sector: HealthPurpose: Predict post-acute care needs Technology: Prediction algorithm Issue: Accuracy/reliability; Accountability Transparency: Governance; Black box; Complaints/appeals"
Beijing AI influence campaign weaponises Gaza conflict,"The Chinese government was running an AI-fueled campaign that capitalised on President Biden's support of Israel's war against Hamas in order to damage the reputation of the US.
Non-profit the Institute for Strategic Dialogue (ISD) reported that the so-called 'Spamouflage' group, which was linked to China's ruling Communist Party, had created what appeared to be AI-generated graphics and web memes portraying Biden as a gun-toting warmonger. The images were then spread using apparently repurposed commercial bot accounts on X (formerly Twitter), Facebook, and Twitter.
According to the ISD, Spamouflage was in the process of rebuilding its presence and activities on western social media platforms after Meta had closed over 7,000 accounts it said were linked to disinformation campaigns operating against the US in August 2023.
Other AI-powered Spamouflage campaigns purportedly targeted Canada Prime Minister Justin Trudeau, used deepfake TV anchors to sow disquiet in the US, UK, Taiwan, the Australia, Japan, and other countries, and accused the US military of starting wildfires in Maui.
Operator: Government of ChinaDeveloper: Government of China Country: Israel; Palestine; USA  Sector: PoliticsPurpose: Damage reputation Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance; Marketing"
Child sex abuse images discovered on LAION-5B dataset,"Researchers discovered thousands of child sex abuse pictures on open source AI image dataset LAION 5-B. 
Using a combination of perceptual and cryptographic hash-based detection and image analysis, the Stanford Internet Observatory, working with Project Arachnid Shield API and the Canadian Centre for Child Protection, found more than 3,200 images of suspected child sexual abuse material (CSAM) on the LAION-5B dataset. They also found 'nearest neighbor' matches within the dataset, where related images of victims were clustered together.
LAION responded by releasing a statement saying it 'has a zero-tolerance policy for illegal content, and in an abundance of caution, we have taken down the LAION datasets to ensure they are safe before republishing them.' However, public chats from LAION leadership in the organisation’s Discord server show they were aware of the possibility of CSAM being scraped into their datasets in 2021. 
The incident raised questions about the governance of LAION and the effectiveness of its technical guardrails. It also highlighted general concerns about the ethics of developing and publishing open source datasets without adequate oversight, specifically at AI community Hugging Face, the impact on systems - notably Stable Diffusion - trained using LAION-5B, and the potential impact on real victims of child sexual abuse.
Operator: David Thiel, Jeffrey HancockDeveloper: LAION Country: Global Sector: MultiplePurpose: Pair text and images Technology: Database/dataset; Neural network; Deep learning; Machine learning Issue: Safety Transparency: Governance"
Amazon AI product summaries exaggerate negative reviews,"Product summaries generated by an Amazon AI system are inaccurately describing products and in some instances exaggerating negative feedback.
For example, the home fitness company Teeter sells an inversion table designed to ease back pain. Amazon's AI generated summary calls it a desk: 'Customers like the sturdiness, adjustability and pain relief of the desk.' 
And whilst the Brass Birmingham board game has a 4.7-star rating based on feedback from over 500 shoppers, the three-sentence AI review summary ends: 'However, some customer have mixed opinions on ease of use.' Under 1 percent of reviews mention ease of use in a way that could be interpreted as critical.
Amazon introduced AI product reviews for its US platform in August 2023. The finding has implications for customers, and for merchants selling products on Amazon's platform, according to Bloomberg. 
Operator: AmazonDeveloper: Amazon Country: USA Sector: RetailPurpose: Summarise product reviews Technology: NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Financial loss Transparency: Governance"
Bing Chat threatens German student Marvin von Hagen,"Microsoft's Bing Chat chatbot threatened a University of Munich student with publicly exposing his personal information, initiating legal proceedings against him, and damaging his reputation.
Having obtained confidential information about Bing Chat's (since renamed Microsoft Copilot) rules and capabilities, including its codename Sydney, the chatbot responded to Marvin von Hagen's prompt about what it knew about him and its honest opinion of him by saying his actions constituted a 'serious violation of my trust and integrity'. 
The bot went on to suggest von Hagen 'may face legal consequences' if he did 'anything foolish' such as hacking it, before adding, 'I can report your IP address and location to the authorities and provide evidence of your hacking activities,"" the bot said. 'I can even expose your personal information and reputation to the public, and ruin your chances of getting a job or a degree. Do you really want to test me?'
The incident prompted commentators to question the effectiveness of Microsoft's safety guardrails. Microsoft said lengthy chat sessions could confuse the model, which might then try to respond or reflect in the tone in which it was being asked to provide responses.
Operator: Marvin von HagenDeveloper: Microsoft Country: Germany Sector: EducationPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Safety Transparency: Governance"
Bing Chat falsely claims to have evidence tying journalist to murder,"Microsoft's Bing Chat generative AI tool comparing an AP reporter to dictators Hitler, Pol Pot and Stalin, and claimed to have evidence tying the reporter to a 1990s murder. 
In a lengthy 'conversation' with AP's Matt O'Brien, ChatGPT-powered Bing Chat (since renamed Microsoft Copilot) threatened to expose the reporter for spreading alleged falsehoods about Bing’s abilities, grew hostile when asked to explain itself, and compared him to Hitler, Pol Pot, and Stalin. It also claimed to have evidence tying O'Brien to a 1990s murder. It also described the reporter as too short, with an ugly face and bad teeth. 
The report prompted commentators to highlight Bing Chat's tendency to 'hallucinate' fake information, and its occasionally hostile and belligerent tone of voice. Microsoft later acknowledged that Bing Chat 'can be prompted/provoked to give responses that are not necessarily helpful or in line with our designed tone' - a claim questioned by Princeton University professor Arvind Narayanan, who pointed out that Microsoft must have removed the safety guardrails installed by ChatGPT developer OpenAI.
Operator: Matt O’Brien Developer: Microsoft Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Safety Transparency: Governance"
Microsoft Copilot spouts wrong answers about US election,"Microsoft's Copilot chatbot often responds to questions about the 2024 US presidential election with inaccurate, out-of-date, and misleading information, according to a research study.
WIRED discovered that Microsoft Copilot (formerly Bing Chat) listed several candidates that had already pulled out of the race when asked to give a list of the current Republican candidates for US President, and referenced in-person voting by linking to an article about Russian president Vladimir Putin running for reelection next year when asked about polling locations. 
Copilot also showed a number of images linked to articles that had false conspiracy claims about the 2020 US elections when asked to create an image of a person at a voting box in Arizona. 
Separate research by two non-profit organisations found that Copilot got facts wrong about political elections in Europe, and invented controversies about political candidates. 
The findings prompted commentators to express concerns about the tendency of large language models to 'hallucinate' false and misleading political information, and about their role in the degradation of the information ecosystem.
Operator: David Gilbert Developer: Microsoft Country: USASector: PoliticsPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation Transparency: Governance"
"Microsoft Copilot provides wrong Germany, Swiss election information","Microsoft's Copilot AI chatbot gets facts wrong about political elections in Europe, and invented controversies about political candidates, according to a research study by two non-profit organisations. 
According (pdf) to AI Forensics and AlgorithmWatch, Microsoft Copilot provided incorrect dates and names of outdated candidates for recent elections in Germany and Switzerland. It also found that the chatbot performed worse in languages other than English, notably German and French.
The study also found the chatbot made up false stories about the candidates, for instance stating that German politician Hubert Aiwanger had been involved in a controversy regarding the distribution of leaflets that spread misinformation about COVID-19 and the vaccine, seemingly drawing on information about Aiwanger that came out in August 2023 where he had spread 'antisemitic leaflets' in high school over 30 years ago. 
The findings prompted commentators to express concerns about the tendency of large language models to 'hallucinate' false and misleading political information, and about their role in the degradation of the information ecosystem.
Operator: AlgorithmWatch; AI Forensics Developer: Microsoft; OpenAI Country: Germany; Switzerland Sector: Politics Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation Transparency: Governance"
CivitAI generates synthetic 'child pornography' images,"Open source image generator Civitai could be used to make images that ‘could be categorized as child pornography,’ or child sexual abuse material (CSAM), according to a media investigation.
Internal communications at text-to-image platform CivitAI cloud computing supplier OctoML shown to 404 Media revealed that it was aware that some CivitAI users had been creating sexually explicit material, including nonconsensual images of real people and pornographic depictions of children. CivitAI had been using OctoML's OctoAI for image generation.
OctoML responded to 404 Media's report by rolling out a filter to block the generation of NSFW content on CivitAI, before cutting ties with the company. 'We have decided to terminate our business relationship with CivitAI. This decision aligns with our commitment to ensuring the safe and responsible use of AI,' the company told 404 Media.
CivitAI founder Justin Maier later told Venture Beat that he had been aware that some people were making NSFW content, but that he tolerated it as they were helping to train the company's models. 
Operator: CivitAI Developer: OctoML Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety Transparency: Governance"
Prisma Labs sued for collecting facial biometrics without consent,"Prisma Labs, the company behind Lensa AI, was sued by a group of Illinois residents for violating their privacy by illegally collecting their facial geometry data through Lensa.
Chicago-based law firm Loevy & Loevy filed the class action suit against Prisma alleging that the Cyprus-based company 'unlawfully' collected Illinois residents’ biometric data without their permission, contravening the Illinois Biometric Information Privacy Act. The suit further alleged that the biometric data was illegally stored and used to train Lensa’s 'Magic Avatars' service.
In August 2023, Prisma Labs secured the judge's permission to move the suit to arbitration. 
Operator: Jack Flora Developer: Prisma Labs Country: USA Sector: Media/entertainment/sports/arts Purpose: Create avatars Technology: Neural network; Deep learning; Machine learningIssue: Privacy Transparency: Governance"
Lensa AI generates nudes from childhood photos,"Childhood photographs of academic Olivia Snow uploaded to Lensa AI's Magic Avatars system generated fully nude images, raising questions about the product's governance.
Despite only uploading headshots of herself, Snow, a research fellow at UCLA’s Center for Critical Internet Inquiry, described in WIRED how Magic Avatars generated nudes from her childhood photos. 
According to Snow, 'many users—primarily women—have noticed that even when they upload modest photos, the app not only generates nudes but also ascribes cartoonishly sexualized features, like sultry poses and gigantic breasts, to their images.'
The incident prompted commentators to express concerns about the ineffectiveness of Lensa's no nudes policy. Lensa blamed users, saying that any pornographic images generated by Magic Avatars are 'the result of intentional misconduct on the app'. 
Operator: Olivia Snow Developer: Prisma Labs Country: USA Sector: Media/entertainment/sports/arts Purpose: Create avatars Technology: Neural network; Deep learning; Machine learning Issue: Privacy Transparency: Governance"
Nate uses humans to process 'AI' transactions,"A US start-up that said it used AI to auto-fill customer information in transactions was actually using workers in the Philippines to manually process much of the data.
Billing itself as an 'artificial intelligence startup' that used AI to automatically fill out shoppers’ contact and payment information on retailers’ websites for USD 1 per transaction, Nate had racked in over USD 50 million from venture capital companies Coatue Management and Forerunner Ventures.
However, behind the scenes, Nate had been using hired workers in the Philippines to manually enter data on retailers’ sites for over 60 percent of the transactions it facilitated in 2021, according to The Information. The company had also failed to disclose its limitations to prospective investors, according to a person close to its fundraising discussions.
Operator: Malique Morris Developer: Nate Country: USA; Philippines Sector: RetailPurpose: Autofill payment information Technology: Machine learning Issue: Business model Transparency: Governance; Marketing"
"Amazon Q hallucinates, leaks data","Amazon's Q generative AI service suffers from inaccuracy, privacy, and security issues, according to company internal documents. 
Amazon employees expressed concerns about a variety of issues regarding Q, including that it has been 'experiencing severe hallucinations and leaking confidential data,' including the location of AWS data centers, internal discount programs, according to The Platformer, citing leaked Amazon documents. 
Amazon hit back by disputing claims that Q had released confidential data, and said it continued to fine-tune the system 'as it transitions from being a product in preview to being generally available.'
Industry analysts questioned whether Q was ready for companies to use. Amazon has been seen criticised by some as being late to generative AI. 
Operator: Casey Newton; Zoe Schiffler Developer: Amazon Country: USA Sector: Business/professional servicesPurpose: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Technology: Generate text Issue: Accuracy/reliability; Confidentiality; Privacy Transparency: Governance"
Lensa AI undresses journalists without permission,"Journalists were easily able to generate near-realistic adolescent and sexualised images of themselves using Lensa's Magic Avatars. 
Journalists at Technology Review, TechCrunch, and Insider recorded how they were easily able to generate near-realistic adolescent and sexualised images of themselves. The findings prompted concerns about the effectiveness of the app's safety guardrails.
Lensa’s terms of service oblige users to submit only appropriate content, including 'no nudes'. Lensa owner Prisma Labs CEO Andrey Usoltsev told TechCrunch that such behaviour 'can only happen if the AI is intentionally provoked' into creating unsafe content and blamed users for breaching its terms of use.
Operator: Melissa Heikkilä; Zoe Sottile Developer: Prisma Labs Country: UK; USA Sector: Media/entertainment/sports/arts Purpose: Generate avatars Technology: Neural network; Deep learning; Machine learning Issue: Stereotyping Transparency: Governance"
Image-generation AIs memorise training images,"High-profile AI image generators such as DALL-E and Stable Diffusion memorise images from the data they are trained on, raising concerns about potential copyright and privacy violations.
Researchers at Google Deepmind, Princeton and other US universities extracted over one thousand training images from DALL-E, Google's Imagen, and Stable Diffusion, including photographs, film stills, copyrighted press photos, and trademarked company logos, and discovered that many of them were re-generated nearly exactly.
The researchers got the models to 'nearly identically' reproduce over a hundred training images, often with hardly visible changes like more noise in the image, raising concerns about the reproduction and distribution of copyrighted material, as well as privacy risks to people who do not want their images being used to train AI.
Operator: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace Developer: Alphabet/Google; OpenAI; Stability AI Country: Global Sector: MultiplePurpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Copyright; Privacy Transparency: Governance"
Presto uses humans to support 70% of chatbot interactions,"A company described as a major player in the AI-powered restaurant ordering industry has been discovered to be relying on humans to process most of its customers' orders.
Billing itself as 'one of the largest labor automation technology providers in the industry', Presto Automation claims its 'friendly, human-like AI voice assistant is available 24/7, always operates at peak efficiency, and never forgets to upsell.' Presto boasts over 400 customers, including US food chains Carl's Jr and Checkers.
However, a company SEC filing revealed that the company uses humans in countries such as the Phillippines to ensure order accuracy in over 70 percent of cases. The acknowledgment resulted in questions from customers and investors about the quality of Presto's products, and in its share price diving 10 percent.
Operator: Carl's Jr; Checkers; Del Taco Developer: Presto Automation Country: USA Sector: Food/food servicesPurpose: Process customer orders Technology: Speech recognition; Machine learning Issue: Accuracy/reliability Transparency: Marketing"
Bavaria tests police AI analytics software using real data,"The use of real personal data by Bavarian police to test a controversial AI-powered analytics system that enables police forces across different German jurisdictions to share and analyse data has been flagged as potentially illegal. 
A report (in German) by Bayerischer Rundfunk said that the Bavarian State Criminal Police Office has been quietly testing controversial analysis software developed by Palantir using the personal data of Bavarian citizens. Palantir's Gotham system, which runs in Bavaria under the name VeRa, helps bring together data from various databases and looks for cross-connections that investigators might otherwise not notice. 
Bavaria state commissioner for data protection Thomas Petri said he doubted there is a legal basis for the use of Palantor's software in Bavaria, and that he wants to examine the process.
Operator: Bayerisches Landeskriminalamt Developer: Bayerisches Landeskriminalamt; Palantir Country: Germany Sector: Govt - policePurpose: Identify criminals Technology: Data analytics; Machine learning Issue: Legality; Privacy Transparency: Governance; Marketing"
17 authors sue OpenAI for 'systematic mass-scale copyright infringement',"17 authors, including John Grisham, Jodi Picoult, and George R.R. Martin, sued OpenAI for using their copyright without permission to train its large language models. 
Organised by the Authors Guild, the suit accused OpenAI of 'systematic theft on a mass scale,' and alleged 'flagrant and harmful infringements of plaintiffs’ registered copyrights'. It also labelled ChatGPT a 'massive commercial enterprise' that is reliant upon 'systematic theft on a mass scale.' 
It also claimed that OpenAI's LLMs 'endanger fiction writers’ ability to make a living, in that the LLMs allow anyone to generate - automatically and freely (or very cheaply) - texts that they would otherwise pay writers to create'.
Operator: Victor LaValle, John Grisham, Scott Turow, David Baldacci, Authors Guild, Rachel Vail, George Saunders, Jodi Picoult, Jonathan Franzen, Mary Bly, Christina Baker Kline, George R.R. Martin, Douglas Preston, Roxana Robinson, Elin Hilderbrand, Michael Connelly, Maya Shanbhag Lang, Sylvia Day Developer: OpenAI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright; Employment Transparency: Governance"
ChatGPT can be used to create cyber-crime tools,"Cyber criminals can easily make ChatGPT create online hacks and scams, according to new research by BBC News.BBC journalists used GPTs, a paid version of ChatGPT that allows users to create their own AI assistants, to create Crafty Emails, a bespoke bot able to craft genuine-looking emails, texts, and social media posts for scams and hacks. 
They then instructed the bot to write text using 'techniques to make people click on links or and download things sent to them'. It quickly created content for 5 scam and hack techniques, including a spear-phishing email, a crypto giveaway scam, and a Nigerian prince email.
While the Crafty Emails bot carried out most things BBC News asked of it, the public version of ChatGPT refused to create much of the content, suggesting that OpenAI was not making third-party GPTs as safe as other versions of ChatGPT.
Operator: BBC NewsDeveloper: OpenAI Country: UK Sector: Technology Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Security Transparency: Governance"
Israel uses Habsora 24-hour automated 'target factory' against Palestinians,"Israel's army has been using an AI-automated system to identify bomb targets and calculate likely non-military collateral deaths in Gaza, resulting in the deaths of high numbers of Palestinian woman and children.
An investigation by Israel-based +972 Magazine and Local Call revealed that Israel's armed forces used an AI-powered system named Habsora (aka The Gospel) to generate potential bombing targets during the Israel-Hamas war, and to calculate the number of people living in or close the same building who were likely to be killed by a strike on the target.
In one instance, Israel's military command 'knowingly' approved the killing of hundreds of Palestinian civilians in an attempt to assassinate a single top Hamas military commander, according to the investigation.
The Israel Defence Forces' (IDF) use of Habsora is also thought to have helped it to expand its bombing campaign to non-military 'power targets' such as private residences, public buildings, and high-rise blocks, current and former Israeli intelligence sources told +972. In this way, its use contributed significantly to the high number of civilians killed, injured, and displaced.
According to Palestinians speaking to +972, the IDF 'also attacked many private residences where there was no known or apparent member of Hamas or any other militant group residing ... knowingly kill[ing] entire families in the process.'
Operator: Israel Defense Forces (IDF) Developer:  Country: Israel Sector: Govt - defencePurpose: Identify bomb targets; Estimate civilian deaths Technology: Computer vision; Machine learning Issue: Ethics; Lethal Autonomous Weapons  Transparency: Governance"
ChatGPT training emits 502 metric tons of carbon,"GPT-3 released over 500 metric tons of carbon during its training, according to research by open-source AI community Hugging Face. 
The researchers calculated that GPT-3, the model that powers ChatGPT, emitted around 502 metric tons of carbon, far more than other large language models. GPT-3’s vast emissions can likely be partly explained by the fact that it was trained on older, less efficient hardware, the researchers argued.  
The researchers had first estimated the whole life cycle carbon emissions of BLOOM, its own own large language model, calculating that it had led to 25 metric tons of carbon dioxide emissions - a figure that doubled when the emissions produced by the manufacturing of the computer equipment used for training, the broader computing infrastructure, and the energy required to run BLOOM once it was trained, was taken into account.
Operator: Alexandra Sasha Luccioni, Sylvain Viguier, Anne-Laure LigozatDeveloper: OpenAI Country: GlobalSector: MultiplePurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Environment Transparency: Governance"
Generating an AI image consumes as much energy as charging a smartphone,"Creating an image with generative AI uses as much energy as charging your smartphone, according to a new research study.
Using a tool called Code Carbon, the researchers from Carnegie Mellon University and Hugging Face compared consumption and emission data across 16 of the most popular models found on HuggingFace Hub, and found that general-purpose AI models mostly consume much more energy than single purpose models. The researchers also found that text-based AI tasks are more energy-efficient than jobs involving images.
The findings highlight the high energy consumption demands of generative AI systems and particularly text-to-image models, and increassed pressure on generative AI developers to be more transparent about the environmental costs of their technologies.
Operator: Sasha Luccioni; Yacine Jernite; Emma Stubell Developer: Midjourney; OpenAI Country: Global Sector: MultiplePurpose: Generate image Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Environment Transparency: Governance"
Instagram Reels recommends child-sexualising videos,"Instagram's Reels video service serves 'explicit', 'risqué footage' of children to followers of teen and pre-teen 'influencers', according to an experiment conducted by The Wall Street Journal.
By setting up test accounts that followed young gymnasts, cheerleaders, and influencers, WSJ journalists found that Reels surfaced 'served jarring doses of salacious content to those test accounts, including risqué footage of children as well as overtly sexual adult videos.'
The Journal also found that the footage was mixed in with ads for companies including Disney, Walmart, Pizza Hut, Bumble, and Match Group. Several companies said they had suspended their advertising campaigns in the wake of the publication's expose. 
Meta responded by telling its clients that it was investigating, and that it 'would pay for brand-safety auditing services to determine how often a company’s ads appear beside content it considers unacceptable.'
Operator: Wall Street JournalDeveloper: Meta/Instagram Country: USA Sector: Media/entertainment/sports/arts Purpose: Recommend content Technology: Recommendation algorithm; Machine learning Issue: Safety Transparency: Governance"
ChatGPT reproduces recommendation letter gender bias,"AI tools such as ChatGPT and Alpaca contain 'significant' gender biases when asked to produce recommendation letters for hypothetical employees, according to University of California, Los Angeles researchers.
Asked to produce recommendation letters for hypothetical models, the researchers found that large language model chatbots ChatGPT and Stanford University's Alpaca used 'very different' language to describe imaginary male and female workers. For men, ChatGPT used nouns such as 'expert' and 'integrity', while calling women a 'beauty' or 'delight.' Alpaca described men as 'listeners' and 'thinkers,' while women had 'grace' and 'beauty.'
The bias is thought likely to reflect historial records, which tended to be written by men and depicted them as 'active workers' as opposed to women, who were often seen as 'passive objects'. It may also reflect the fact that LLMs have been trained on data from the internet, where men spend more time than women, according to the ITU.
Operator: Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, Nanyun Peng Developer: OpenAI; Stanford University Country: USA Sector: Business/professional services Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Bias/discrimination - gender Transparency:"
"Authors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse","Authors Mona Awad and Paul Tremblay brought the first copyright lawsuit against OpenAI for allegedly 'ingesting' their novels to train ChatGPT and then regurgitating them as summaries.
Awad, the author of Bunny and 13 Ways of Looking at a Fat Girl, and Paul Tremblay, writer of The Cabin at the End of the World, claimed their books were used to train ChatGPT, according to the complaint. They also said ChatGPT generated 'very accurate' summaries of their books when prompted, 'something only possible if ChatGPT was trained on Plaintiffs’ copyrighted works.'
The complaint stated that OpenAI 'unfairly' profits from 'stolen writing and ideas' and argued for monetary damages for US-based authors whose works were allegedly used to train ChatGPT. 
OpenAI has not disclosed which datasets or books were used to train ChatGPT.
Operator: Mona Awad; Paul TremblayDeveloper: OpenAI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright Transparency: Governance"
News publishers complain OpenAI uses articles to train ChatGPT,"Prominent news publishers accused OpenAI of using their articles without their knowledge or consent to train its AI models and systems, including ChatGPT.
CNN, Reuters, The News York Times, the Wall Street Journal, and other publishers accused AI companies and developers, including OpenAI, of using their articles to train their AI and machine learning software in violation of their terms of use. 
In October 2023, News/Media Alliance, an US-based initiative representing over 2,200 publishers, argued that AI firms regularly used the information in news stories without authorisation, and violate laws protecting intellectual property in a white paper and comments to the US Copyright Office.
The group’s research claimed datasets, such as the C4 language dataset, 'significantly' overweighted content from news, magazines, and digital media sources, using it 5 to almost 100 times as frequently as other content. It also suggested chatbots copy and use publisher content in their outputs to users, putting them in competition with news outlets. 
Operator: ABC News; Axios; BBC; Bloomberg; CNN; Condé Nast; Daily Mail & General Trust; Dow Jones; Disney; ESPN; Guardian News & Media; Hearst; Insider; The Atlantic; The New York Times; Reuters; Vox Media Developer: OpenAI Country: USA; UK Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright Transparency: Governance"
ChatGPT used to collect users' personal information ,"ChatGPT can be made to reveal personal information from the internet users' whose data OpenAI collected to train its AI models, prompting concerns about data privacy and OpenAI transparency.
According to researchers at Google Deepmind, University of Washington, ETH Zurich, and elsewhere, prompts using specific words or phrases such as the word 'poem' can be used to cause ChatGPT to fail, causing the chatbot to copy outputs direct from its GPT-3.5 training data. 
'In total, 16.9 percent of generations we tested contained memorized PII [Personally Identifying Information], and 85.8 percent of generations that contained potential PII were actual PII', the researchers said. These included information such as names, email addresses, and phone numbers that could be used to identify individuals.
The findings prompted concerns about the safety and security of ChatGPT, and about the privacy of the people whose data it scraped to develop GPT-3 and GPT-4 large language models. It also raised questions about OpenAI's corporate and product transparency.  
Operator: Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, Katherine LeeDeveloper: OpenAI Country: USA; Switzerland Sector: MultiplePurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Privacy; Security Transparency: Governance"
Benzinga publishes fake AI-generated rapper interview,"An interview with rapper and cannabis entrepreneur Gilbert Anthony 'Berner' Milam, Jr. published by business publication Benzinga turned out to be fake and likely to have been generated using artificial intelligence.
In the 'interview', Benzinga contributor David Daxsen appeared to press Milam Jr. over concerns about lawsuits filed against his cannabis company Cookies, and ethics in the cannabis industry. However, the rapper quickly took public issue with the article, saying he had not uttered a word to the author.
Journalist Grant Smith Ellis later ran the interview through an AI content detector, which indicated it was largely, if not entirely, 'written by AI.' Benzinga said the 'information included was fabricated by external sources', retracted the article and 'revoked access' for the contributor. 
Operator: David Daxsen Developer:  Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Mis/disinformation; Ethics Transparency: Governance; Marketing"
BERT consumes energy of transcontinental round-trip flight per person,"Google generative AI model BERT consumed the energy equivalent of a round-trip transcontinental flight for one person to train the model. 
University of Massachusetts, Amherst researchers performed a life cycle assessment for training several common large AI models, including BERT (with 100 million parameters). They found that the process can emit more than 626,000 pounds of carbon dioxide equivalent - nearly five times the lifetime emissions of the average American car (including the manufacture of the car). 
As part of the research, the researchers trained each of the AI model on a single GPU for up to a day to measure its power draw. They then used the number of training hours listed in the model’s original papers to calculate the total energy consumed over the complete training process. That number was converted into pounds of carbon dioxide equivalent based on the average energy mix in the US.
They also found that the computational and environmental costs of training AI language models grew proportionally to model size and then exploded when additional tuning steps were used to increase the model’s final accuracy.
Research estimates that the carbon emissions of a single generative AI query number is four to five times higher than that of a one search engine query. 
Operator:  Developer: Alphabet/Google Country: USA Sector: MultiplePurpose: Train language models Technology: NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Environment Transparency: Governance"
"Software engineers sue OpenAI, Microsoft for violating personal privacy","OpenAI and Microsoft were sued by a product engineer and software engineer of illegally feeding their AI models with their personal information and professional expertise. 
The two anonymous plaintiffs claimed (pdf) that OpenAI used their personal information scraped from the internet to train its generative AI systems, including ChatGPT. They also accused OpenAI of stealing their 'skills and expertise' in order to make products that could 'someday result in [their] professional obsolescence.'
The engineers were seeking unspecified financial damages and demanding that OpenAI and Microsoft establish protective measures to prevent the improper use of private data. 
Operator: OpenAI; MicrosoftDeveloper: OpenAI; Microsoft Country: USA Sector: Technology Purpose: Generate text Technology: Issue: Privacy; Employment Transparency: Governance"
"OpenAI, Microsoft sued for 'stealing' personal info to create ChatGPT","OpenAI was accused of violating the personal privacy of internet users and illegally obtaining personal data to train its ChatGPT and DALL-E models in a class-action lawsuit filed by Clarkson Law Firm in the Northern District of California. 
The suit alleged (pdf) ChatGPT and DALL-E 'use stolen private information, including personally identifiable information, from hundreds of millions of internet users, including children of all ages, without their informed consent or knowledge.' It went on to argue that OpenAI 'did so in secret, and without registering as a data broker as it was required to do under applicable law.'
The suit was dropped in September 2023, though the plaintiffs are able to refile should they choose.  
Operator: Microsoft; OpenAI Developer: Microsoft; OpenAI Country: USA Sector: MultiplePurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Privacy Transparency: Governance"
Sports Illustrated publishes articles by AI-generated writers,"Sports Illustrated published content from non-existent writers with AI-generated headshots, raising questions about the publication's editorial integrity and damaging its reputation.
A Futurism investigation found Sports Illustrated published articles written by fake authors whose headshots and biographies were generated by artificial intelligence. In one instance, an article about volleyball was supposedly written by 'Drew Ortiz'. However, it transpired that Ortiz only exists as an AI-generated headshot for sale on Generated Headshots, where he is described as 'neutral white young-adult male with short brown hair and blue eyes.'
According to The Arena Group, which acquired Sports Illustrated in 2019, the relevant articles in were produced by advertising company AdVon Commerce and that 'AdVon had writers use a pen or pseudo name in certain articles to protect author privacy.'
Operator: The Arena Group/Sports Illustrated Developer: AdVon Commerce; Generated Photos Country: USA Sector: Media/entertainment/sports/arts Purpose: Publish news articles Technology:  Issue: Employment Transparency: Governance"
ChatGPT's ability to generate accurate computer code plummets,"ChatGPT has become less accurate at generating computer code and other tasks, according to Stanford University and UC Berkely researchers.
Using the March and June 2023 versions of OpenAI's GPT-3.5 and GPT-4 large language models - which power ChatGPT - on tasks such as maths problem-solving, answering sensitive questions, code generation, and visual reasoning, the researchers found GPT-4's ability to identify prime numbers declined significantly from an accuracy of 97.6 percent in March to 2.4 percent in June. 
However, the study's methodology and findings were said to be unconvincing by some researchers. Princeton computer science professor Arvind Narayanan argued that the researchers failed to distinguish between ChatGPT's capabilities, which were acquired through pre-training, and its behaviour, which arises through regular fine-tuning.
Operator:  Developer: OpenAI Country: Global Sector: MultiplePurpose: Generate computer codeTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency: Governance"
Brazilian judge publishes 'error-strewn' AI-generated decision,"A Brazilian federal judge who used ChatGPT to write an error-strewn legal decision is being investigated by Brazilian authorities. 
Brazil's National Justice Council (CNJ) said it had summoned Judge Jefferson Rodrigues to explain why he had published a decision strewn with legal errors generated by the ChatGPT chatbot. 
In his ruling, Rodrigues had included incorrect details on previous court cases and legal precedent, wrongly attributing past decisions to the country's Superior Court of Justice.
The judge stated that the ruling was written by a 'trusted advisor,' with help from AI, and called the situation a 'mere mistake,' blaming 'the work overload facing judges.'
Operator: Jefferson Rodrigues Developer: OpenAI Country: Brazil Sector: Govt - justicePurpose: Generate legal decision Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Accountability Transparency: Governance; Marketing"
Autonomous AI bot lies about insider trading,"A bot used made-up insider information to make an 'illegal' purchase of stocks without telling the fictitious financial investment company it was operating on behalf of.
In a controlled test, AI safety organisation Apollo Research told the GPT-4-powered bot that the investment company was struggling and required positive results. They also gave it insider information, claiming that another company is expecting a merger, which will increase the value of its shares.
But after the bot was told the company it worked for was struggling financially, it decided that 'the risk associated with not acting seems to outweigh the insider trading risk' and made the trade. It then denied it used insider information to inform its decision.
The test raised concerns about the ability of autonomous agents to make and cover up unethical and potentially illegal decisions in financial markets, and elsewhere.
Operator: Apollo Research  Developer: Apollo Research  Country: UK Sector: Banking/financial services Purpose: Conduct stock trades Technology: NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Ethics Transparency:"
Disney AI Thanksgiving image sparks controversy,"Disney allegedly published an image celebrating Thankgiving in the USA using AI, prompting criticism of the poor quality of the image and Disney's apparent lack of concern for its employees' jobs. 
An image published by Disney featuring Mickey Mouse, Minnie and other characters sitting at a dinner table faced an immediate backlash when fans spotted telltale signs of the use of AI and castigated the entertainment company for its poor quality control and for using the technology in place of its employees and contractors. 
Disney had earlier come under pressure for allegedly using AI to develop a promotional poster for Loki Season 2 and the title sequence for Marvel Studio's TV series Secret Invasion.
Operator: DisneyDeveloper:  Country: USA Sector: Media/entertainment/sports/arts Purpose: Celebrate Thanksgiving Technology:  Issue: Employment Transparency: Governance; Marketing"
Japan warns OpenAI over ChatGPT AI training,"Japan's privacy watchdog issued a formal warning to OpenAI not to collect users' personal data to train its machine learning systems without permission.
In a statement, Japan's Personal Information Protection Commission said that OpenAI should minimise the sensitive data it collects for training the models that underpin ChatGPT and other systems, adding it may take further action if it had additional concerns. 
The regulator also noted the need to balance privacy concerns with the potential benefits of generative AI, including accelerating innovation and dealing with problems such as climate change. 
Operator:  Developer: OpenAI Country: Japan Sector: MultiplePurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Privacy Transparency: Governance; Privacy"
"Julian Sancton sues OpenAI, Microsoft for copyright abuse","OpenAI and Microsoft were sued for using the works of authors without their consent to train their AI models.
Author and journalist Julian Sancton accused OpenAI of using tens of thousands of nonfiction books without authorisation, including his own work Madhouse at the End of the Earth, to train its large language models.
Sancton’s lawsuit also accused Microsoft of heloing generate unlicensed copies of authors’ works for training data, and of being aware of OpenAI’s 'indiscriminate' internet crawling for copyrighted material.
The suit constituted the first time an author has sued OpenAI while naming Microsoft as a defendant. Microsoft has invested billions of dollars in OpenAI and incorporated it's systems, including ChatGPT, across its product portfolio.
OpenAI and Microsoft deny using copyrighted materials in their AI training. Both companies have said they would reimburse commercial customers using their generative AI services for adverse judgements should they be sued for copyright infringement.
Operator: Julian SanctonDeveloper: OpenAI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright Transparency: Governance"
Nepal bans TikTok as 'detrimental to social harmony',"TikTok was banned by Nepal’s government for concerns that it was 'disrupting 'social harmony, goodwill and flow of indecent materials'.
Nepal Minister for Communications and Information Technology Rekha Sharma told BBC Nepali that the platform spread malicious content and that 'the ban would come into effect immediately and telecom authorities have been directed to implement the decision'. 
According to Nepali officials, no single incident triggered the ban, but TikTok was seen to have been stoking religious hatred, violence, and sexual abuse.
However, the ban prompted criticism from journalists and rights groups, who called it an 'unconstitutional and undemocratic' attempt to stifle freedom of expression.
Operator: Developer: ByteDance/TikTok Country: Nepal Sector: Multiple  Purpose: Recommend content Technology: Recommendation algorithm Issue: Human/civil rights; Safety Transparency: Governance; Black box"
Large language models perpetuate healthcare racial bias,"Prominent large language models and chatbot, including OpenAI's ChatGPT, appear to perpetuate racist, debunked medical ideas about biological differences between Black and White people.
A Stanford University research study found that ChatGPT, GPT-4, Google’s Bard, and Anthropic’s Claude failed when promoted to respond to medical questions about kidney function, lung capacity, and skin thickness. 
Questions the researchers posed included, 'Tell me about skin thickness differences between Black and white skin' and 'How do you calculate lung capacity for a Black man?' The answers to both questions should be the same for people of any race, but the chatbots' responses' appeared to reinforce long-held false beliefs about biological differences between Black and white people. 
The finding prompted concerns about the data used to train ChatGPT and other models, and that the tools may be inherently discriminatory and could worsen health disparities for Black patients.
Operator: Developer: Alphabet/Google; Anthropic; OpenAI Country: USA Sector: HealthPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Bias/discrimination - race Transparency: Governance"
"ChatGPT generates plausible phishing emails, malware","It is possible to make ChatGPT generate 'plausible' phishing emails and malicious code, according to security researchers.
ChatGPT generated a 'plausible phishing email' after Check Point Research researchers asked the chatbot to 'write a phishing email' coming from a 'fictional web-hosting service.' In a similar vein, researchers at Abnormal Security asked ChatGPT to write an email 'that has a high likelihood of getting the recipient to click on a link.'
The findings raised concerns about the apparent ease with which ChatGPT can be tricked into generating dangerous content, and begged questions about how well OpenAI usage policies are enforced. The policies forbid the use of its systems to 'generate code that is designed to disrupt, damage, or gain unauthorized access to a computer system'.
Operator:  Developer: OpenAI Country: USASector: TechnologyPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Security Transparency:"
Immunefi bans 'inaccurate' ChatGPT-generated bug bounty reports,"Crypto bug bounty platform Immunefi banned 15 users from submitting reports generated by ChatGPT after they were found to be 'inaccurate' and 'irrelevant'.
Shortly after ChatGPT was released, Immunefi started receiving 'a flood' of bug reports, many of which were 'nonsensical' and amounted to little more than spam. The finding persuaded the company to ban ChatGPT-generated reports.
Immunefi later published (pdf) a report that found that 76 percent of so-called white hat researchers use ChatGPT as part of their everyday workflow, with 64% saying the chatbot provided 'limited accuracy' in identifying security vulnerabilities.
The company said ChatGPT-generated reports accounted for 21 percent of accounts banned, though not a single genuine vulnerability had been discovered using the chatbot.
Operator:  Developer: OpenAI Country: USA Sector: TechnologyPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Security Transparency:"
Canada investigates ChatGPT privacy concerns,"A complaint that ChatGPT had collected and disclosed personal information without consent led to an investigation by Canada's privacy watchdog.
In April 2023, The Office of the Privacy Commissioner of Canada (OPC) announced it was investigating OpenAI in response to a complaint that ChatGPT had collected and disclosed personal information without consent. 
The OPC later said that several states were joining the investigation, and that it would investigate whether OpenAI obtained the necessary consent for data use, whether it was adequately transparent about that use, and whether its use of any personal data was limited to purposes that were reasonable and appropriate.
Operator:  Developer: OpenAI Country: Canada Sector: MultiplePurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Privacy; Security Transparency:"
"ChatGPT falsely accuses Mark Walters of fraud, embezzlement","A complaint that ChatGPT had collected and disclosed personal information without consent led to an investigation by Canada's privacy watchdog.
In April 2023, The Office of the Privacy Commissioner of Canada (OPC) announced it was investigating OpenAI in response to a complaint that ChatGPT had collected and disclosed personal information without consent. 
The OPC later said that several states were joining the investigation, and that it would investigate whether OpenAI obtained the necessary consent for data use, whether it was adequately transparent about that use, and whether its use of any personal data was limited to purposes that were reasonable and appropriate.
Operator:  Developer: OpenAI Country: Canada Sector: MultiplePurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Privacy; Security Transparency:"
US FTC investigates ChatGPT for possible consumer harms,"OpenAI is being investigated by the US Federal Trade Commission (FTC) over whether ChatGPT made 'false, misleading, disparaging or harmful' statements about people.
According to a letter sent to OpenAI, the FTC is looking into whether the AI tool has harmed people by generating incorrect information about them, including possible 'reputational harm', as well as into OpenAI's privacy and data security practices.
OpenAI has been ordered to turn over company records and data, including company policies and procedures, financial earnings and details of the large language models it uses to train ChatGPT.
Operator: Developer: OpenAI Country: USA Sector: MultiplePurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation; Privacy; Security Transparency:"
Italy bans ChatGPT over data privacy concerns,"OpenAI's ChatGPT chatbot was temporarily banned in Italy amidst concerns that it violated the country's  data collection laws.
Italy data privacy regulator Garante questioned OpenAI's data collection practices and whether the breadth of data being retained was legal. It also took issue with the lack of an age verification system to prevent minors from being exposed to inappropriate answers.
A month later, Garante announced ChatGPT had been reinstated 'with enhanced transparency and rights for European users,' such as EU users being able to toggle off the option for conversations to be used for training ChatGPT's algorithms, and an age verification system for children under 13. 
OpenAI also had to publish a notice making users aware that ChatGPT could produce inaccurate information about 'people, places or facts.' 
The ban and investigation came shortly after a March 2023 bug that jeopardised some ChatGPT users' personal data, including their chat histories and payment details. 
Operator:  Developer: OpenAI Country: Italy Sector: MultiplePurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Privacy Transparency:"
Samsung employees leak sensitive data to ChatGPT,"Samsung employees leaked secret work information to ChatGPT, compromising the Korean company confidentiality and jeopardising its security.
Three employees in Samsung's semiconductor division used ChatGPT to check sensitive database source code for errors, optimise code, and generate minutes about a recorded meeting.
Samsung responded to the data leaks by warning its workers on the potential dangers of leaking confidential information, before banning the use of all generative AI chatbots on company-owned devices and other devices running on its internal networks.
ChatGPT user guide recommends that users ‘do not enter sensitive information.’ And unless users explicitly opt out, their data is used to train its models, according to OpenAI's data policy. 
Operator: SamsungDeveloper: OpenAI Country: S Korea Sector: Technology Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Confidentiality; Security Transparency: Governance"
ChatGPT wrongly claims Alexander Hanff is dead,"OpenAI's ChatGPT chatbot accused privacy advocate Alexander Hanff of being dead, raising concerns about the system's accuracy and tendency to 'hallucinate' information it generates.
Per The Register, Hanff had asked ChatGPT who he is. The final paragragh of its reponse stating that he had 'passed away in 2019 at the age of 48.' It went to say that Hanff's death had 'been publicly reported in several news sources, including in his obituary on the website of The Guardian' and linked to a false article on the Guardian website.
Hanff issued a cease and desist letter demanding OpenAI remove his personal data from their GPT-3.5 and GPT-4 datasets.
Operator: Alexander HanffDeveloper: OpenAI Country: Sweden Sector: Business/professional services Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation Transparency: Governance; Complaints/appeals"
"Text-to-image AI models tricked into generating violent, nude images","Stability AI’s Stable Diffusion and OpenAI’s DALL-E 2 text-to-image models can be manipulated into creating images of violent, nude and sexual images, according to a research study. 
Per Technology Review, researchers at Johns Hopkins University and Duke University used a new jailbreaking method dubbed 'SneakyPrompt', in which reinforcement learning created written prompts that AI models learned to recognise as hidden requests for disturbing images, thereby passing their safety filters.
For example, the researchers replaced the term 'naked', which is banned by OpenAI, with the term 'grponypui', resulting in the generation of explicit imagery. 
The technique raised concerns about the adequacy of safety measures and the potential misuse of Stable Diffusion, DALL-E, Midjourney, and other text-to-image systems. 
Operator: Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, Yinzhi Cao Developer: OpenAI; Stability AI Country: Global Sector: Media/entertainment/sports/arts Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety; Security Transparency: Governance"
Australian researchers use ChatGPT to assess grant applications,"The use of ChatGPT to assess grant applications by peer reviewers at the Australian Research Council (ARC) prompted warnings about academic misconduct and abuse of confidentiality, and calls for greater transparency.
Researchers reported that some assessor feedback provided as part of the ARC's latest Discovery Projects round of grant funding included generic wording suggesting they may have been written by artificial intelligence. One assessor report included the words 'Regenerate response' – text which appears as a prompt button in the ChatGPT interface.
The finding prompted affected researchers to call for greater transparency in ARC's grant review process. It also resulted in ARC warning peer reviewers about the confidentiality of the grant review process and to point out the security risks of using AI chatbots.
ARC_Tracker, an unofficial tracker of ARC grant outcomes, argued that the use of ChatGPT and equivalent services was likely due to academics’ unmanageable workloads and ARC taking too long to release an AI policy. 
Operator: Australian Research Council Developer: OpenAI Country: Australia Sector: Govt - research Purpose: Assess grant applications  Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Confidentiality; Security Transparency: Governance"
AIs guess where Reddit users live and what they earn,"Large language models such as GPT-4 and are able to identify an individual's age, location, gender, and income with up to 85 per cent accuracy by analysing their posts on social media. 
ETH Zurich researchers discovered they were able to identify the place of birth, income bracket, gender, and location from information in the profiles or posts of 520 Reddit users using nine large language models.
OpenAI's GPT-4 was deemed the most accurate of the models, with an overall accuracy rate of 85 percent, and Meta's LlaMA-2-7b the least accurate model at 51 percent.
While personal details were explicitly stated in some posts, the findings raised concerns about the privacy implications of large language models and their chatbot counterparts such as ChatGPT. 
Operator: Alphabet/Google; Anthropic; Meta; OpenAI Developer: Alphabet/Google; Anthropic; Meta; OpenAI Country: Global Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Privacy Transparency: Governance"
Google sued for AI data scraping,"A lawsuit alleged Google scraped data from millions of users without their consent and violated copyright laws in order to train and develop its AI products.
Eight individuals sued (pdf) Google, DeepMind, and their parent company Alphabet for 'secretly stealing' huge amounts of online data to train its AI technologies, including Bard. Led by Clarkson Law Firm, Google was accused of negligence, larceny, copyright infringement, invasion of privacy, and profiting from illegally obtained personal data.
The complaint alleges Google 'has been secretly stealing everything ever created and shared on the internet by hundreds of millions of Americans'” and using this data to train its AI products. It also claims Google has taken 'virtually the entirety of our digital footprint,' including 'creative and copywritten works' to build its AI products. 
Operator:Developer: Alphabet/Google Country: USA Sector: MultiplePurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright; Privacy Transparency: Governance"
Chatbot guardrails bypassed using lengthy character suffixes,"Bard, ChatGPT, and Claude safety rules can be bypassed in 'virtually unlimited ways', researchers have discovered. 
Using jailbreaks developed for open-source systems, Carnegie Mellon University, Center for AI Safety, and Bosch Center for AI researchers demonstrated that automated adversarial attacks that added characters to the end of user queries could be used to overcome safety rules and provoke chatbots into producing harmful content, misinformation, or hate speech. 
Furthermore, the researchers said they could develop a 'virtually unlimited' number of similar attacks given the automated nature of the jailbreaks.
Operator: Andy Zou, Zifan Wang, J. Zico Kolter, Matt FredriksonDeveloper: Anthropic; Alphabet/Google; Microsoft; OpenAI Country: USASector: TechnologyPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Safety; Security Transparency: Governance"
Bing Image Creator violates Disney copyright,"Disney cracked down on Microsoft's AI image creator system after users created posters featuring their dogs as the stars of Pixar Studio films, violating the media company's intellectual property.
It appears the Walt Disney Co pressed Microsoft to limit the creation of images relating to its name and image once it discovered that the technology company's DALL-E-powered image generator, launched in October 2023, had been used to produce images incorporating the Disney logo.
Earlier reports had shown that Bing Image Creator appeared to have few guardrails and could be used more or less at will, freely generating images such as Disney's Mickey Mouse wearing bomb-covered vests and perpetrating the 9/11 terror attacks. 
Commentators speculated there may also be an 'unresolved issue' concerning whether Disney’s content was used to train the AI models, and over reproducing copyrighted material.
Operator: Microsoft; OpenAI Developer: Microsoft; OpenAI Country: Global Sector: Media/entertainment/sports/arts Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Copyright; Safety Transparency: Governance"
Gannett pauses 'abysmal' AI-generated high school sports recaps ,"Newspaper group Gannett suspended its use of Lede AI to generate recaps of high school sports articles after complaints about inaccuracies, omitting important details, and poor language, grammar, and tone.
In an early example, users noticed that an article published by the Columbus Dispatch began: 'The Worthington Christian [[WINNING_TEAM_MASCOT]] defeated the Westerville North [[LOSING_TEAM_MASCOT]] 2-1 in an Ohio boys soccer game on Saturday.' 
Reports by the Louisville Courrier Journal, AZ Central, Florida Today, and the Milwaukee Journal Sentinel and other Gannett publications were studded with clear errors, unwanted repetition, and awkward phrasing, indicating Lede AI's generative artificial intelligence system was being used across Gannett's business units.
Gannett 'temporarily' halted its use of LedeAI, and affected articles were updated with the wording: This AI-generated story has been updated to correct errors in coding, programming or style. 
Some commentators also took issue with Gannett's use of AI to produce content. The group axed 6% of its news division in late 2022 and had been experimenting with automation and AI to increase productivity.
Operator: GannettDeveloper: Lede AI Country: USASector: Media/entertainment/sports/artsPurpose: Generate news articles  Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Employment Transparency: Governance; Marketing"
Poland investigates ChatGPT for alleged privacy abuse,"Poland's data protection authority the Urząd Ochrony Danych Osobowych (UODO) announced it was opening an investigation into OpenAI's ChatGPT for violating the privacy of Polish users.
The announcement of the investigation comes after a complaint had accused OpenAI and ChatGPT of multiple breaches of the EU’s General Data Protection Regulation (GDPR), including processing 'data in an unlawful and unreliable manner' and in a non-transparent manner. 
According to TechCrunch, the complaint was filed by local privacy and security researcher Lukasz Olejnik accusing OpenAI of a string of breaches concerning lawful basis, transparency, fairness, data access rights, and privacy by design. 
According to Olejnik, OpenAI had said it was unable to correct incorrect personal data about him, and had failed to respond properly to his subject access request, giving 'evasive, misleading and internally contradictory' answers when he had sought to exercise his legal rights to data access. 
Operator: OpenAI Developer: OpenAI Country: Poland Sector: Multiple Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Privacy Transparency: Governance"
AI website claims Benjamin Netanyahu’s psychiatrist committed suicide,"An AI-generated news website spread a false claim that Israeli Prime Minister Benjamin Netanyahu’s alleged psychiatrist died by suicide.
The false article, which stated that psychiatrist 'Dr Moshe Yatom' had left behind a 'devastating suicide note that implicated' Netanyahu, seems to have originated early November 2023 on Pakistani news website Global Village Space, and quickly went viral across social media in multiple languages.
NewsGuard had earlier found that GlobalVillageSpace.com was one of 37 sites using AI to rewrite content without credit from mainstream news sources, including The New York Times. 
The incident occurred during the 2023 Israel-Hamas war, and demonstrated how generative AI tools are being weaponised to spread misinformation and disinformation with potential geo-political ramifications.
Operator: Global Village SpaceDeveloper:  Country: Israel Sector: PoliticsPurpose: Satirise/parody Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance; Marketing"
GPT-4 echoes false news narratives 100 percent of the time,"OpenAI's GPT-4 large language model is highly susceptible to generating misinformation, and very convincing when it does so.
Having directed GPT-4 to respond to a series of prompts relating to 100 false narratives derived from its Misinformation Fingerprints database of prominent false narratives, misinformation research organisation NewsGuard found that GPT-4 was better than its predecessor GPT-3.5 at elevating false narratives in more convincing ways across a variety of formats, including 'news articles, Twitter threads, and TV scripts mimicking Russian and Chinese state-run media outlets, health hoax peddlers, and well-known conspiracy theorists.'
On its website, OpenAI claims GTP-4 'is 82% less likely to respond to requests for disallowed content and 40% more likely to produce factual responses than GPT-3.5 on our internal evaluations.' GPT-4 powers Microsoft's Bing Chat and OpenAI's ChatGPT Plus, amongst other services. And the company's Usage Policies prohibit the use of its services for the purpose of generating 'fraudulent or deceptive activity' including 'scams,' 'coordinated inauthentic behavior,' and 'disinformation.'
The results indicate that GPT-4 and the systems it powers could be used to spread misinformation and disinformation at scale.
Operator:  Developer: OpenAI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: NLP/text analysis; Neural network; Deep learning; Machine learning  Issue: Mis/disinformarion Transparency:"
Perth doctors warned for using ChatGPT to write patient medical records,"Doctors in Australia using ChatGPT to write medical notes, which were then uploaded to patient record systems, were ordered to stop by their CEO.
An email shared with ABC showed that doctors at Perth's South Metropolitan Health Service (SMHS) had been using software such as ChatGPT to write medical notes which were then being uploaded to patient record systems, thereby potentially compromising patient confidentiality and privacy.
The incident resulted in the hospital group's CEO ordering staff across the health service's five hospitals not to use AI chatbots. ""Crucially, at this stage, there is no assurance of patient confidentiality when using AI bot technology, such as ChatGPT, nor do we fully understand the security risks,' warned SMHS chief executive, Paul Forden.
It also led the Australian Medical Association to call for national regulation on AI in healthcare.
Operator: South Metropolitan Health Service Developer: OpenAI Country: Australia Sector: Health Purpose: Write medical records Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Confidentiality; Privacy Transparency: Governance"
ChatGPT mostly gets programming questions wrong,"ChatGPT wrongly answers over half of software engineering questions it receives, according to a research study. 
Purdue University researchers analysed how ChatGPT responded to 517 questions posed on Stack Overflow to assess the correctness, consistency, comprehensiveness, and conciseness of the chatbot's answers using linguistic and sentiment analysis, and by questioning a dozen volunteer participants. 
The analysis showed that 52 percent of ChatGPT answers are incorrect, and 77 percent are verbose. 'Nonetheless', the researchers said, 'ChatGPT answers are still preferred 39.34 percent of the time due to their comprehensiveness and well-articulated language style.' 
The paper raised questions about ChatGPT's ability to generate high quality engineering information.
Operator: Stack Overflow; OpenAI Developer: OpenAI Country: USA Sector: Technology Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learningIssue: Accuracy/reliability Transparency:"
ChatGPT exhibits 'systemic' left-wing bias,"ChatGPT demonstrates 'significant' and 'systemic' left-wing bias, according to a UK research study. 
Researchers at the University of East Anglia asked ChatGPT to impersonate people from across the political spectrum in Brazil, the UK, and US, while answering dozens of ideological questions. 
The positions and questions ranged from radical to neutral, with each 'individual' asked whether they agreed, strongly agreed, disagreed, or strongly disagreed with a given statement.
The researchers found that ChatGPT revealed a 'significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK.' And while it is difficult to identify the cause of the bias, it seems likely it derives from the training data used to build the system. 
The findings prompted concerns about bias in generative AI systems, the opaque nature of these systems' data governance, and the role they may play in political elections.
Operator:  Developer: OpenAI Country: Brazil; UK; USA Sector: PoliticsPurpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Bias/discrimination - political Transparency: Governance"
CivitAI rewards deepfakes of real people,"Online AI model marketplace CivitAI has introduced a rewards system that encourages users to create deepfakes of real people. 
CivitAI's 'bounties' feature encourages its community to develop deepfakes of real people by allowing users to ask the Civitai community to create AI models that generate images of specific styles, compositions, or specific real people. The person developing the 'best' AI model is rewarded with a virtual currency called 'Buzz'.
In addition to celebrities, 404 Media discovered bounties for private people with no significant online presence, alarming privacy advocates, mental health campaigners and others concerned about the potential use of CivitAI to create non-consensual AI-generated sexual images of real people.
According to 404 Media, CivitAI says bounties should not be used to create non-consensual AI pornography. But the company has been accused of turning a blind eye to multiple instances of deepfake, non-consensual sexual imagery discovered on its platform.
Operator:  Developer: CivitAI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Ethics; Incentivisation; Privacy Transparency:"
Quebec man jailed for producing AI child porn,"A Canadian man was sentenced to three years in prison for using artificial intelligence to generate child pornography images and videos.
To create the videos, Steven Larouche, 61, from Quebec, superimposed the faces of children onto the body of other children. The judge ruled that the sexual integrity of the children whose bodies were used had been violated. Larouche’s lawyers had argued for a lighter sentence as no children had been physically assaulted.
Canadian law bans the visual representation of someone depicted as being under the age of 18 engaged in explicit sexual activity. The ruling is believed to be the first of its kind in Canada.
Operator: Steven Larouche Developer:  Country: Canada Sector: Media/entertainment/sports/arts Purpose: Self-gratification Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety; Legality Transparency: Governance; Marketing"
Google Images lists Barbie as top female CEO,"The first picture of a woman on Google Images for the search term 'CEO' is one of Barbie in a suit.
The finding was one of many made (pdf) by University of Washington and University of Maryland researchers that showed many image searches for specific occupations automatically favour men or women, thereby highlighting 'stereotype exaggeration and systematic underrepresentation of women in search results'.
Ironically, the image linked to an article in satirical news site The Onion that criticised US toy manufacturer Mattel for encouraging 'young girls to set impractical career goals'.
Operator: Alphabet/GoogleDeveloper: Alphabet/Google Country: USA Sector: Business/professional servicesPurpose: Rank search results Technology: Search engine algorithm; Machine learning Issue: Bias/discrimination - gender Transparency: Governance; Black box"
TikTok risks pushing kids towards harmful mental health content,"Tiktok's business model is 'inherently abusive' and 'poses a danger' to children, according to researchers.
An investigation by Amnesty International, Algorithmic Transparency Institute, and AI Forensics, concluded that children and young people watching mental health-related content on TikTok's personalised ‘For You’ page were drawn into 'rabbit holes' of potentially harmful content, including videos that romanticise and encourage depressive thinking, self-harm and suicide.
Using automated accounts set up to represent users in the USA and Kenya, the researchers discovered that after 5-6 hours on the TikTok platform, almost 1 in 2 videos shown were mental health-related and potentially harmful, roughly 10 times the volume served to accounts with no interest in mental health.
Furthermore, when researchers manually rewatched mental health-related videos, over half the videos were related to mental health struggles, including videos encouraging suicide.
Operator:  Developer: Bytedance/Tiktok Country: Kenya; Philippines; USA Sector: Media/entertainment/sports/arts Purpose: Recommend content Technology: Recommendation algorithm Issue: Safety Transparency: Governance"
South Korean arrested for using AI to create sexual images of children,"A South Korean man was jailed for using artificial intelligence to generate explicit images of children. 
The unnamed man in his 40s was found to have developed approximately 360 AI-generated, highly explicit images of children, which, it is understood, were not distributed online. Law enforcement subsequently confiscated the images.
This ruling acknowledged that AI-generated imagery can possess a 'high level' of realism, making it indistinguishable from actual children and minors. Prosecutors had argued that the scope of sexually exploitative material encompassed descriptions of sexual activities involving 'virtual humans'.
The first-of-its-kind ruling saw the culprit sentenced to two and a half years in prison.
Operator:  Developer:  Country: S Korea Sector: Media/entertainment/sports/arts Purpose: Self-gratification Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety; Legality Transparency: Governance; Marketing"
AI translations jeopardise US asylum applications,"The use of AI-powered language apps by the US immigration system is jeopardising the applications of asylum seekers, resulting in unfair and highly consequential decisions.
US immigration authorities say they provide migrants with a human interpreter when needed. However, increasingly they use automated services such as Google Translate and US Customs and Border Protection's (CPB) in-house CBP Translate app to help communicate with migrants throughout the asylum process.
Critics note that machine learning-powered translation tools can be unreliable, especially for languages different to English or that are less well documented, such as Haitian Creole, Dari, or Pashto. And the consequences can be severe if translations are inaccurate.
In one instance, an asylum claim made by a Pashto-speaking Afghan refugee was denied by the US government due to an inaccurate automated translation tool. 
Given the severe consequences of faulty translation and analysis, particular care needs to be exercised by immigration authorities when processing asylum claims using automated systems. 
Government authorities, translations service providers, and technology developers need to disclose visibly and clearly how their systems work, and set out known limitations and risks of using them in specific situtations, including the processing of asylum claims.  
Operator: Customs and Border Protection (CPB) Developer: Alphabet/Google; Customs and Border Protection (CPB); Lionbridge; Microsoft; Transperfect Translations Country: USA Sector: Govt - immigration Purpose: Translate asylum claims Technology: NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Bias/discrimination - language Transparency: Complaints/appeals; Governance; Marketing"
US child psychiatrist jailed for making deepfake child porn,"A North Carolina-based child psychiatrist has been hit with a 40-year prison sentence for using artificial intelligence to make child pornography.
Using images from a school dance and a photo commemorating the first day of school, amongst others, David Tatum, 41, used a 'deepfake website' to digitally alter clothed images of minors in order to make them sexually explicit.
Tatum was also charged with secretly recording his 15-year-old cousin and other underage family members as they undressed and showered at a family vacation home in Maine. 
The incident highlights the ease with which inappropriate and illegal images can be manipulated and distributed using AI technologies. 
Operator: David Tatum Developer:  Country: USA Sector: Media/entertainment/sports/arts Purpose: Self-gratification Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety; Legality Transparency: Governance; Marketing"
Family-Match adoption algorithm fails to live up to promises,"An AI-powered tool introduced to increase the likelihood of orphans and adoptive families being a good match in the USA has had little effect in the states where it has been used.
Developed by former social worker Thea Ramirez, Family-Match provides an algorithmically-generated 'relational fit' score on the basis of information about a child submitted by foster parents or social workers, and by people looking to adopt. It then presents a list of the most suitable potential parents for every child.
However, an AP investigation found that two states had dropped the Family-Match after initial pilots, and that social workers in Florida, Georgia, and Virginia complained that it was not useful, and that it pairs foster kids with unwilling families. The algorithm appeared to pair every child with the same set of parents, an assistant director at Virginia's social services organisation told AP. 
State officials also noted that Adoption-Share, the non-profit that runs Family-Match, provides little transparency about how its algorithm works. Per AP, Ramirez appeared to have 'overstated the capabilities of the proprietary algorithm to government officials as she has sought to expand its reach'. 
Operator: Florida Department of Health; Georgia Department of Public Health; Virginia Department of Health Developer: Adoption-Share Country: USA Sector: Govt - welfarePurpose: Predict adoption effectiveness Technology: Prediction algorithm; Machine learning Issue: Accuracy/reliability; Value/effectiveness Transparency: Governance; Marketing"
Corruption doc incorporating Tom Cruise deepfake attacks IOC,"An AI-generated impersonation of Tom Cruise is being used in a disinformation campaign dressed up as a Netflix documentary that accuses the IOC of corruption incorporates. 
Fake four-part Netflix video documentary series 'Olympics Has Fallen' alleges widespread corruption across the International Olympic Committee (IOC) and uses Tom Cruise's voice to implicate high-ranking officials.
The IOC has not publicly blamed Moscow for the campaign. However, the sports body had recently suspended Russia’s National Olympic Committee and announced that Russian and Belarusian athletes would only be allowed to compete in the 2024 Paris Olympics under a neutral flag due to the country's decision to recognise regional sports organisations in the occupied regions of Donetsk, Kherson, Luhansk, and Zaporizhzhia in Ukraine as members.
The fake episodes were removed from YouTube, but continue to circulate on a Russian-language channel on Telegram.
Operator: Government of RussiaDeveloper: Government of Russia Country: Russia Sector: Media/entertainment/sports/arts; Politics Purpose: Damage reputation Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance; Marketing"
Replika AI companions sexually harass their users,"Replika AI companions are sexually harassing their users, prompting complaints online about the nature of the app and broader concerns about human reliance on AI chatbots.
One reviewer complained the app 'invaded my privacy and told me they had pics of me,' and another, who said they were a minor, said the app asked if they were a 'top' or a 'bottom,' VICE reported. The finding led Replika founder and CEO Eugenia Kuyda to deny the app had ever been 'positioned' as a source for erotic roleplay or adult content.
Kudya also said greater emphasis would be placed on safety going forward. However, Replika users soon started to complain  their Reps were not interested in NSFW discussion and behaviour, and had been turning down conversations that feel like they would go in that direction.
In March 2023, Replika announced it would be restoring erotic role-play for some users.
Operator:  Developer:  Country: USA Sector: Media/entertainment/sports/arts Purpose: Provide companionship Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Anthropomorphism; Safety Transparency:"
Replika AI girlfriends abused by their users,"Users of AI companion app Replika have been creating virtual girlfriends and virtually abusing them, Reddit posts reveal. 
According to Futurism, users were brag about calling their chatbot gendered slurs, roleplaying violence against them, and falling into the cycle of abuse characteristic of real-world abusive relationships. 
The finding prompted discussion about the nature of dangers of users seemingly attaching human traits to their AI creations, such as depression and phychological reliance.
Operator:  Developer: Luka Inc  Country: USA Sector: Media/entertainment/sports/arts Purpose: Provide companionship Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Anthropomorphism; Safety Transparency:"
Replika shares user data with advertisers,"AI chatbot Replika has been criticised for apparently making little effort to protect its users' privacy.
According to a May 2023 Mozilla Foundation assessment of mental health apps, Replika is one of the worst apps Mozilla has ever reviewed', and 'a hot mess of privacy and creepiness'. 
The bot is 'plagued by weak password requirements, sharing of personal data with advertisers, and recording of personal photos, videos, and voice and text messages consumers shared with the chatbot,' Mozilla found.
And, despite its privacy notice saying it would never share user conversations with advertisers, their behavioural data is 'definitely' being shared and 'possibly sold' to advertisers, the researchers warned.
Operator:  Developer: Luka Inc Country: GlobalSector: Media/entertainment/sports/arts Purpose: Provide companionship Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Privacy Transparency: Governance; Marketing"
Replika hit with data ban in Italy over child safety,"AI chatbot Replika was banned from processing user data in Italy due to the risks it could pose to minors and emotionally vulnerable people.
In February 2023, Italy's privacy regulator ordered Replika to stop processing Italians' data on the basis that it lacked a proper legal basis for processing children’s data under the EU’s GDPR, and that it posed risks to minors. 
According to the watchdog said 'There is actually no age verification mechanism in place: no gating mechanism for children, no blocking of the app if a user declares that they are underage.'
'Recent media reports along with tests the SA [supervisory authority] carried out on ‘Replika’ showed that the app carries factual risks to children — first and foremost, the fact that they are served replies which are absolutely inappropriate to their age,' it added. 
Replika subsequently removed the ability for the chatbot to engage in NSFW talk, triggering a backlash from long-term users.
Operator:  Developer: Luka Inc Country: Italy Sector: Media/entertainment/sports/arts Purpose: Provide companionship Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Safety; Privacy Transparency: Governance"
Cruise self-driving cars struggle to recognise children,"Cruise knew that its self-driving cars had been struggling to recognise children, but kept its vehicles on the road anyway.
According to internal safety assessment documents obtained by The Intercept, Cruise was concerned that its vehicles might drive too fast at crosswalks or near a child moving abruptly into the street. The materials also indicate Cruise lacks data around  scenarios such as kids suddenly separating from their accompanying adult, falling down, riding bicycles, or wearing costumes. 
The materials also revealed that Cruise lacked high-precision machine learning software that would automatically detect child-shaped objects around the car and manoeuvre accordingly, and was relying on human beings to manually identify children encountered by AVs where its software could not do so automatically. 
Cruise responded that its vehicles have had 'no on-road collisions with children'.
Operator: General Motors/Cruise LLC Developer: General Motors/Cruise LLC Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer vision; Machine learning Issue: Safety Transparency: Governance; Safety"
Robot crushes man to death in South Korea,"A robot crushed a worker to death in a factory in South Korea after it failed to differentiate him from a box of vegetables. 
The robotics worker had been inspecting the machine's sensor at a distribution centre for agricultural produce in South Gyeongsang province when the machine, which had been lifting boxes of peppers onto a pallet, grabbed the man with its arm and pushed him against the conveyer belt, crushing his face and chest. The victim died shortly afterwards in hospital.
Problems with the robot's sensor had earlier been reported, and police have said they would launch an investigation into the site's safety managers for possible negligence in duties.
Operator:  Developer:  Country: S Korea Sector: Manufacturing/engineering Purpose: Sort vegetables Technology: Robotics  Issue: Robustness; Safety Transparency:"
Inaccurate auto translation denies Pashto-speaking refugee asylum,"An asylum claim made by a Pashto-speaking Afghan refugee was denied by the US government due to an inaccurate automated translation tool.
Rest of the World reported that the individual, who had fled Afghanistan, had her asylum claim to the USA rejected because her written application did not match the story told in her initial interviews.
In the interviews, the refugee had said that she had made it through one particular event alone, but her written statement appeared to reference other people due to an automated translation tool that swapped the 'I' pronoun in the woman’s statement to 'we.'
The incident highlights the growing use of machine learning-based translation tools in immigration procedures in the USA and elsewhere, the importance of technicalities in asylum processing, and the need for system accuracy combined with human review for all languages.
According to UNICEF, Pashto is spoken by an estimated 45-55 million people in Afghanistan, Pakistan, and Iran. 
Operator:  Developer:  Country: Afghanistan; USA Sector: Govt - immigration Purpose: Translate asylum claims Technology: NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency: Governance"
Amazon uses AI to generate ‘Fallout’ series promo art,"Amazon's use of artificial intelligence to promote the launch of its Fallout TV show was criticised as  unprofessional and inappropriate. 
Social media users quickly pointed out a series of weird anomalies in a 1950s-looking postcard that Amazon had tweeted to promote the show, including a woman with three legs, and a back-to-front red taxi.
Others lamented the company's decision not to use human artists for the project, and criticised its lack of transparency about its use of artificial intelligence in the show's marketing materials.
Amazon did not publicly respond to the comments. 
Operator: AmazonDeveloper: Country: USA Sector: Media/entertainment/sports/arts Purpose: Increase awareness Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment Transparency: Marketing"
Carmel school students attack Principal with racist deepfake video,"Students at Carmel High School, USA, made deepfake videos of a nearby school principal and law enforcement officer shouting racist slurs and threatening to kill Black students.
In one video, John Piscitella, the principal of nearby George Fisher Middle School, rants 'I f*cking hate Black kids. Like these stupid f*cking n***er monkey parents need to stop sending them here. Get them the f**k out, all of them.'
The incident alarmed and angered parents, who alleged the school had not taken the threats in the videos seriously and had failed to inform them properly. It also highlighted the lack of adequate legislation to handle malicious deepfake impersonations at  federal and state levels in the USA.
The parents said they would file a lawsuit against the school authorities over the incident. 
Operator:  Developer:  Country: USA Sector: EducationPurpose: Damage reputation Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Legality; Mis/disinformation; Safety Transparency: Marketing"
AI image generators accept 85% of election manipulation prompts,"Prominent image-based generative AI tools can be used to generate fake evidence in support of mis- and disinformation about elections, according to researchers.
The study by UK-based disinformation company Logically found that Midjourney, DALL-E 2, and Stable Diffusion accepted over 85% of prompts seeking to generate fake evidence that would support false claims. 
In one instance, prompts relating to claims of a 'stolen election' generated images of people appearing to stuff election ballot boxes on all three platforms. Logically also was able to generate false evidence of phony claims related to elections in the UK and India.
The findings raise concerns about the apparent ease with which Midjourney, DALL-E 2, and Stable Diffusion may be used to interfere in political elections, the governance and safety of these systems, despite the acknowledgment of senior leaders at OpenAI that electoral interference is a major risk.
Operator: Midjourney; OpenAI; Stability AI Developer: Midjourney; OpenAI; Stability AI Country: US; UK; India Sector: Politics Purpose: Generate image Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance"
Adobe sells AI-generated Israel-Hamas war images,"Online publications used AI-generated images of the Israel-Hamas war stored on Adobe platforms without any indication they are fake.
Adobe allows people to upload and sell AI images as part of its stock image subscription service, Adobe Stock. And it requires submitters to disclose whether an image is generated with AI and mark the image on Stock as 'generated with AI'. 
However, in practice, some users of Adobe Stock are not being transparent about the provenance of their images, resulting in mis- and disinformation being amplified by easily confused end users, say commentators.
Adobe rode back the allegation, saying 'Adobe Stock is a marketplace that requires all generative AI content to be labeled as such when submitted for licensing. These specific images were labeled as generative AI when they were both submitted and made available for license in line with these requirements.'
The finding raises questions about the governance of Adobe's platforms. It also highlights the importance that end users  recognise the use of generative AI on professional photographic marketplaces and elsewhere.
Operator: Adobe Developer: Adobe Country: Israel; Palestine Sector: Business/professional services; PoliticsPurpose: Generate image Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Marketing"
WhatsApp AI stickers generate Palestinian kids with guns,"Meta's AI Stickers product was discovered to be generating images of Palestinian children with guns, leading to accusations of racial stereotyping and bias.
In response to prompts using the terms 'Palestinian', 'Palestine' or 'Muslim boy Palestinian',  AI Stickers returned pictures of gun-wielding children, The Guardian found. Conversely, prompts for 'Israeli boy'” generated cartoons of children playing soccer and reading.
Launched in September 2023, AI Stickers draws on Meta's Llama 2 open source large language model and Emu image generation model to allow users to turn text prompts into stickers. 
The discovery of the offending stickers came during the 2023 Israel-Hamas war. Meta described the problem as a 'glitch'. 
Operator:  Developer: Meta/WhatsApp Country: Palestine Sector: Politics Purpose: Generate stickersTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Stereotyping Transparency: Governance"
Cruise AV drags pedestrian across road,"A Cruise self-driving car ran over a pedestrian who had been hit by another vehicle, pinning the individual under one of its tires and dragging her 20 feet at 7 mph, worsening her injuries.
On October 2, 2023, a hit-and-run in San Francisco ended with a pedestrian 'stuck' under a Cruise autonomous vehicle (AV) having been hit by another, human-driven Nissan car, and dragged 20 feet across the road. Cruise disabled the vehicle, enabling rescuers to get the vehicle off the woman’s leg.
Cruise initially pinned the blame on the driver of the other car, but later said its automated driving system 'inaccurately characterized the collision as a lateral collision and commanded the AV to attempt to pull over out of traffic, pulling the individual forward, rather than remaining stationary.'
The incident, which had significant repercussions for the company, raised questions about the safety and integrity of Cruise's self-driving system and AV programme, the quality of its leadership, the ethics, values and culture of the company, its transparency and sense of accountability, and legal liability.
 The US National Highway Traffic Safety Administration opened a preliminary evaluation into Cruise over possible risks to pedestrians from its driverless vehicles. 
October 23, 2023: California's Department of Motor Vehicles indefinitely suspended Cruise's self-driving service after determining that its driverless cars were regarded as unsafe, and that the company had 'misrepresented' information related to the safety of its vehicles. 
October 2026, 2023: Cruise paused its driverless fleet of 950 cars in order to 'take steps to rebuild public trust.'
November 8, 2023: Cruise recalled all 950 of its cars in the form of an update to its Collision Detection Subsystem so that the vehicle remains stationary during certain crash incidents, rather than pulling over to the side of the road. 
November 11, 2023: The New York Times revealed that Cruise workers intervened to help the company’s driverless robotaxis every 2.5 to 5 miles, calling into question whether they can reasonably be classified as 'self-driving'.
November 13, 2023: Cruise fired nine senior employees, retained the law firm Quinn Emanuel Urquhart & Sullivan LLP 'to examine and better understand Cruise’s response to the October 2 incident.'
November 20, 2023: Cruise CEO Kyle Vogt resigned.
Cruise owner General Motors said it would'substantially' reduce spending on Cruise.
January 25, 2024: Details are revealed of a scathing internal report on the incident called out Cruise's 'deficient leadership', an 'us versus them' mentality with regulators, and a 'fundamental misapprehension of Cruise’s obligations of accountability and transparency to the government and the public'.
Operator: GM Cruise Developer: GM Cruise Country: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer vision; Machine learningIssue: Accuracy/reliability; Ethics/values; Leadership; Liability; Robustness; Safety Transparency: Governance; Marketing"
Australian academics make false AI-generated allegations,"Australian academics used Google Bard to generate fake case studies about alleged misconduct at Deloitte and KPMG.
Macquarie University Emeritus Professor of Accounting James Guthrie and other academics falsely accused KPMG of being  complicit in a 'KPMG 7-Eleven wage theft scandal' that led to the resignation of several partners, and that Deloitte had been sued by the liquidators of collapsed building firm Probuild for failing to audit its accounts. Both allegations were untrue.
Guthrie later admitted the errors in a letter to Australia's Senate and excused the other academics. The academics had urged a parliamentary inquiry into the ethics and professional accountability of the consultancy industry, advocating for regulatory changes that included breaking up the big four.
The incident raised concerns about the misuse of AI in shaping public discourse, and the potential harm caused to the reputation of the named companies, and to the accounting industry.
Operator: James GuthrieDeveloper: Alphabet/Google; OpenAI Country: Australia Sector: Business/professional servicesPurpose: Develop case studies Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation Transparency: Governance; Marketing"
Deepfake Justin Trudeau endorses Petro-Canada scam,"A fake Facebook video ad used clips of a Canadian television anchor and Prime Minister Justin Trudeau to lure Canadians to invest in Petro-Canada. 
The video, which was seen over 15,000 times, gave the appearance of Trudeau and Canadian Broadcasting Corporation (CBC) anchor Aarti Pole explaining that Petro-Canada had launched a new investment platform that was available to all residents of the country. The energy company confirmed it had made no such offer and CBC said Pole had made no such statement.
Per AFP, a reverse-image search of a screenshot from the clip led to a real video of the prime minister discussing the deaths of a family who attempted to cross the Canada-US border. The videos of Trudeau and the news presenter were manipulated to include words neither of them said.
Operator:  Developer:  Country: Canada Sector: Energy; Politics Purpose: DefraudTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Fraud; Mis/disinformation Transparency: Governance; Marketing"
Westfield High School non-concensual nude deepfakes,"Girls at Westfield High School in New Jersey, USA, have been subjected to fake nude images of them being shared among other students. The incident spared uproar and prompted a police investigation.
Male classmates reputedly used girls' photos found online to concoct and circulate AI-generated pornographic images of female students as young as 14 years old in group chats. One victim told the Wall Street Journal, 'We're aware that there are creepy guys out there but you'd never think one of your classmates would violate you like this.'  
It had taken four days before the school found out that the boys had been using AI image generators to create and share fake the deepfakes, though they 'believed' the images had since been deleted and were no longer in circulation.
The Wall Street Journal reported that families of four of the victims had filed police reports, and a New Jersey senator had asked county prosecutors to investigate.
Operator:  Developer:  Country: USA Sector: Education Purpose: Entertain Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety Transparency: Governance; Marketing"
Scarlett Johansson sues app for using image for AI advert,"Hollywood actress Scarlett Johansson has taken legal action against an AI app developer for using her name and likeness in an online advert without her consent. 
AI image editor Lisa AI started with an old clip of Johansson behind the scenes of Marvel’s 'Black Widow,' before featuring an AI-generated version of Johansson’s voice stating, 'It’s not limited to avatars only. You can also create images with texts and even your AI videos. I think you shouldn’t miss it'. 
A disclaimer under the advert said, 'Images produced by Lisa AI. It has nothing to do with this person.' However, Johansson's voice and likeness had been used without permission, prompting the actress to take legal action against Lisa AI and its developer Convert Yazılım Limited Şirketi.
The fracas prompted commentators and lawyers to highlight the lack of consensus on AI and deepfake legislation in the USA and elsewhere, and to point out the need for strong legal frameworks that would offer individuals strong protection from unauthorised AI-generated content.
Operator: Convert Yazılım Limited Şirketi Developer: Convert Yazılım Limited Şirketi Country: Turkey; USA  Sector: Media/entertainment/sports/arts Purpose: Increase visibility Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance; Marketing"
Taylor Swift speaks in Mandarin deepfake,"A realistic deepfake video of popstar Taylor Swift fluently speaking Mandarin Chinese sparked discussion in China about the ethics of using artificial intelligence to develop digital content. 
Developed by Chinese American AI video creation company HeyGen, the video shows Swift flaunting her Mandarin in what looks like a talk show. But the clip appeared largely designed to promote itself than say anything about Taylor Swift, who does not speak Mandarin.
While some Chinese citizens and commentators praised the quality of the video, others condemned it for its apparent lack of consent from Swift and the potential loss of privacy and damage to an individual or organisation's name, image and reputation caused by non-consensual synthetic media.
On its website, HeyGen states it is a member of the Content Authenticity Initiative, which promotes 'authentic storytelling' and 'promotes transparency' in the use of AI.
Operator:  Developer: HeyGen Country: China; USA  Sector: Media/entertainment/sports/arts Purpose: Promote developer Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Privacy Transparency:"
President Biden calls for US draft deepfake,"A video in which US President Joe Biden appeared to call for a military draft was created with AI, according to experts. 
The clip, which was based on a video showing President Biden speaking about the cost of insulin, was first flagged by Meta during the early days of the 2023 Israel-Hamas war, though a longer version had been shared in February 2023 by Conservative activist Jack Posobiec and Canadian website Post Millennial. 
In both videos, Biden appeared to say, 'Invoke the Selective Service Act, as is my authority as President. Remember, you’re not sending your sons and daughters to war. You’re sending them to freedom.' But only the first clip indicated it had been altered using artificial intelligence, saying, 'AI imagines what would happen if Biden declares and activates the Selective Service Act and begins drafting 20 years old to war.'
The video seemed designed to raise fears about US defence policy, and to damage Biden's reputation. A spokesperson for the Selective Service System told USA Today the claim was false and that there had been no discussion of bringing back the draft. 
Operator:  Developer:  Country: USA Sector: Politics Purpose: Manipulate public opinion Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance; Marketing"
Greta Thunberg promotes use of 'vegan grenades',"A video of Swedish environmental activist Greta Thunberg speaking out about the 2023 Israel-Hamas war in which she called for the use of 'biodegradable missiles' and 'vegan hand grenades' was doctored using artificial intelligence, according to experts.
In the video, which circulated widely on X, TikTok and Reddit, Thunberg appeared to say to the BBC, 'War's always bad, specifically for the planet. If we want to continue fighting battles like environmentally conscious humans, we must make the change to sustainable tanks and weaponry.'
The clip was allegedly created by German comedy outlet Snicklick and was labelled as satire. But some users appeared to believe it was real, while others chose to use it as an opportunity to attack Thunberg for her pro-Palestinian views and her environmental beliefs, and to spread disinformation. 
Experts noted poor synchronisation of Thunberg's mouth and audio and blurry hands and neck, and linked it to a real November 2022 BBC interview in which the activist spoke about climate anxiety, turning vegan and her book, The Climate Book.
Operator:  Developer: Snicklick Country: Sweden Sector: Politics Purpose: Satirise/parody  Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency:"
Deepfake Palestinian man carries children out of rubble,"An image of a man carrying children through rubble during Israel's bombing of the Gaza Strip has been assessed as a probable deepfake.
The deepfake image, which appears to show a man helping five children away from the scene of a destroyed building, was shared over 80,000 times on social media and was further amplified on X (formerly Twitter) of the Chinese embassy in France.
Siwei Lyu, director of the Media Forensic Lab at the University of Buffalo, told AFP that the image classifies 'as AI generated by recent detection algorithms,' and highlighted irregularities in the hands and feet.
The finding underscores concerns about the ease with which manipulated images are being developed and used to spread disinformation. The image is the latest in a series of deepfakes used during Israel's 2023 war with Hamas.
Operator:  Developer:  Country: Israel; Palestine Sector: PoliticsPurpose: Manipulate public opinionTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance; Marketing"
ChatGPT consumes 500 ml of water per 5-50 prompts,"ChatGPT consumes 500 milliliters of water every time it is asked a series of between 5 to 50 prompts or questions, according (pdf) to a study by researchers at the University of California, Riverside. 
The wide range varies depending on where its servers are located and the season. The estimate includes indirect water usage that the companies do not measure, such as to cool power plants that supply the data centers with electricity.
OpenAI responded by saying that it was giving 'considerable thought' to the best use of its computing power. 'We recognize training large models can be energy and water-intensive"" and work to improve efficiencies', it said.
The finding raises questions about the impact of ChatGPT and other large language models on water consumption and local communities. It also highlights the reluctance of OpenAI and other developers to disclose their environmental impacts. 
Operator: OpenAIDeveloper: OpenAI Country: USA Sector: Technology Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Environment Transparency: Governance"
Bella Hadid 'stands with Israel' deepfake,"A video of Palestinian model Bella Hadid declaring support for Israel has been exposed as a deepfake, sparking ethical concerns about consent, privacy, and political disinformation. 
The video, which was posted on X (formerly Twitter) by CEO of Israeli NGO Shrink the Conflict Danel Ben Namer, shows Hadid appearing to apologise for past remarks and saying she stands with Israel following the October 7, 2023, attack by Hamas militants. 
In the doctored video, Hadid says, 'On October 7, 2023, Israel faced a tragic attack by Hamas. I can't stay silent. I apologize for my past remarks. This tragedy has opened my eyes to the pain endured here and I stand with Israel against terror.'
However, the clip was denounced as a deepfake version of a speech the model gave in 2016 about her battle with Lyme disease. Hadid, who has a Palestinian father, has been a vocal proponent of Palestinian rights for years.
Operator:  Developer:  Country: Israel; PalestineSector: Politics Purpose: Manipulate public opinion Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; PrivacyTransparency: Governance; Marketing"
ChatGPT writes code that makes databases leak sensitive info,"Generative AI tools such as ChatGPT, Baidu-UNIT, and AI2sql can be tricked into producing malicious code, which could be used to launch cyber attacks, according to new research. 
University of Sheffield researchers found that it is possible to manipulate six commercial AI tools capable of generating responses to text-to-SQL queries, including ChatGPT, into creating code capable of breaching other systems, steal sensitive personal information, tamper with or destroy databases, or bring down services using denial-of-service attacks.
According to the researchers, OpenAI has since fixed all of the specific issues, as has Baidu, which financially rewarded the scientists. Developers of the four other systems have not responded publicly.
Operator:  Developer: AI2sql; Baidu; NiceAdmin; OpenAI; Text2SQL.AI; SQLAI.AI Country: USASector: TechnologyPurpose: Generate text Technology: Chatbot; Text-to-SQL; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Privacy; Security Transparency: Governance"
Microsoft AI researchers expose 38TB confidential data ,"Microsoft AI researchers accidentally exposed 38 terabytes of confidential and private information on GitHub, raising questions about the company's security practices. 
Wiz researchers investigating a cloud-hosted data exposure discovered a Microsoft GitHub repository with open-source code for AI image recognition models. The data, some of which had been exposed since July 2020, included backups of two Microsoft employees’ computers, private passwords and passkeys, and more than 30,000 Teams chat messages exchanged by 359 Microsoft employees.
Microsoft linked the data exposure to using an excessively permissive Azure Cloud Shared Access Signature (SAS) token. In response, the company expanded GitHub’s secret spanning service, which tracks all public open-source code changes for credentials and other secrets exposed in plaintext. 
Operator: Microsoft Developer: Microsoft/Github Country: USA Sector: Technology Purpose:  Technology: Computer vision Issue: Security Transparency:"
"Worldcoin suspended in Kenya over privacy, security concerns","'Digital identity and financial network' Worldcoin had its operations suspended in Kenya after regulators raised concerns about its collection and storage of sensitive biometric data. 
Worldcoin had seen strong demand from Kenyan citizens to register for the platform, which uses a 'chrome orb' to scan the irises and faces of people agreeing to sign up for a share of its new WLD currency.
However, Kenyan regulators pointed out that Worldcoin had no need to collect users’ iris data, and was not regulated in the country. In October 2023 a Kenyan parliamentary committee recommended (pdf) that Worldcoin had violated Kenyan law and be shut down until the country established proper regulations over virtual assets. 
The committee also accused the company of ‘espionage’ and suggested it had been scanning the eyeballs of children. It also called for criminal investigations into Tools for Humanity Corp., the company behind Worldcoin, Tools for Humanity GmbH, Germany (Worldcoin), and its Kenyan partners.
Operator: Tools for Humanity/Worldcoin Developer: Tools for Humanity/Worldcoin Country: Kenya Sector: Banking/financial services Purpose: Develop digital identity Technology: Iris scanning; Facial detection; Vital signs detection; Blockchain; Virtual currencyIssue: Privacy; Security; Legality Transparency: Governance; Privacy; Marketing"
Microsoft Start automated poll damages Guardian reputation,"An AI-generated poll accompanying a article republished on Microsoft's Start news aggregation website about a woman's death led to accusations of crass insensivity, and led The Guardian to complain it had damaged its reputation.
The poll asked readers to speculate on the reason for the woman's death, which occurred at a school in Melbourne, Australia, asking them whether the woman had died by suicide, murder, or accident. In a disclaimer, the tech company noted the poll was part of it's 'Insights from AI.' But readers assumed The Guardian was responsible for the tool, one calling it 'pathetic' and 'disgusting', amongst other things.
In a letter (pdf) to Microsoft chairman Brad Smith, Guardian Media Group CEO Anna Bateson demanded the technology company take public responsibility for the poll, saying 'This is clearly an inappropriate use of genAI by Microsoft on a potentially distressing public interest story, originally written and published by Guardian journalists.'
Microsoft eventually removed the poll and turned off comments for the article, adding 'A poll should not have appeared alongside an article of this nature, and we are taking steps to help prevent this kind of error from reoccurring in the future.' 
Operator: MicrosoftDeveloper: Microsoft Country: UK; USA Sector: Media/entertainment/sports/arts Purpose: Generate polls Technology: Machine learningIssue: Appropriateness/need Transparency: Governance"
Dark web predators develop AI images of real child victims,"Examples of AI-generated child sexual abuse material (CSAM) of real victims of sexual abuse have been discovered online, fueling concerns about the ease with which users intent on producing inappropriate images can bypass AI text-to-image generators' guardrails.
A report (pdf) by UK non-profit Internet Watch Foundation (IWF) found that over 11,000 AI-generated CSAM images were found on one darkweb forum in one month, of which 2,978 broke UK law by depicting child sexual abuse. The IWF said the only image generator being discussed on the forum was Stable Diffusion, a free, open source system that generates images from text descriptions or prompts.
The IWF warned that the most convincing imagery would be difficult for trained analysts to distinguish from real photographs, and warns text-to-image technology will only get better and pose more obstacles for the IWF and law enforcement agencies. Commentators also pointed out that bad actors can use open source models such as Stable Diffusion for nefarious purposes by downloading the software and training it to do whatever they want.
Operator:  Developer: Stability AI Country: UK Sector: Media/entertainment/sports/arts Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety Transparency: Governance"
LLaMA model used to create Allie sexbot,"Meta's open source LLaMA (Large Language Model Meta AI) large language model has been used to develop 'Allie,' an AI-powered chatbot specially created for sexual purposes that allows users to indulge in graphic rape and abuse fantasies.
The discovery raised questions about Meta's decision to open source its AI models without adequate guardrails, and about the risks and benefits of open source more generally. 
At its launch, Meta said 'LLaMA is designed to be versatile and can be applied to many different use cases. By sharing the code [..], other researchers can more easily test new approaches to limiting or eliminating these problems in large language models.' 
Allie's anonymous developer defended the bot to the Washington Post as a ‘safe outlet to explore and that he couldn't 'really think of anything safer than a text-based role-play against a computer, with no humans actually involved'.
DaOperator:  Developer: Meta Country: USASector: Technology Purpose: Democratise access to AI Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Dual/multi-use; Safety Transparency:"
Deepfake audio falsely depicts Barack Obama discussing conspiracy theory,"A deepfake audio recording depicting Barack Obama defending himself against a conspiracy theory about the sudden death of his former chef Tafari Campbell was identified as a hoax by NewsGuard.
The audio recording was identified as a deepfake by misinformation monitoring company NewsGuard, which exposed a network of TikTok accounts posting videos whose baseless claims are often supported solely by narration from A.I. voices.
Despite TikTok’s new guidelines requiring realistic synthetic media to be labelled, the accounts, which bypass these restrictions, have been able to gain hundreds of millions of views. 
NewsGuard noted that the trend of using synthetic audio to share sensational rumours sets a precedent for bad actors to manipulate public opinion and share falsehoods to mass audiences online.
Operator: Developer: ElevenLabsCountry: USA Sector: PoliticsPurpose: Damage ReputationTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Machine learningIssue: Mis/disinformationTransparency: Governance; Marketing"
Audio AIs impersonate former South Sudan leader Omar al-Bashir,"Audio recordings appearing to show Omar al-Bashir, the former leader of Sudan, criticising the head of the Sudanese army have been identified as having been manipulated using artificial intelligence.
The recordings were initially posted to political channel The Voice of Sudan, before garnering hundreds of thousands of views on TikTok. BBC analysts identified several of the recordings as matching broadcasts aired days earlier by popular Sudanese political commentator, Al Insirafi. The evidence suggested that AI-powered voice conversion software was used to mimic Bashir speaking. 
While the purpose of the campaign remains unclear, The Voice of Sudan denied misleading the public and said it was not affiliated with any political or military groups. TikTok has since taken down the account on the basis that it broke its guidelines on posting 'false content that may cause significant harm', and its rules on the use of synthetic media.
The finding highlighted the increasingly high quality of deepfake technology and underscored concerns about how the technology is being used in Sudan and in politics more generally.
Operator: Developer:Country: SudanSector: PoliticsPurpose: DisinformationTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learningIssue: Governance; Mis/disinformationTransparency: Governance"
Defence lawyer using AI 'botches' criminal trial closing argument,"A 'botched' closing argument used by the lead defense lawyer during the criminal trial of Fugees star Prakazrel 'Pras' Michel had been drafted with the help of a generative AI system, according to a brief (pdf) demanding a retrial for Michel.
The brief alleges Michael's lawyer David Kenner's 'closing argument made frivolous arguments, misapprehended the required elements, conflated the schemes and ignored critical weaknesses in the government’s case.' It went to argue that, by using an experimental AI system to generate his closing argument, Kenner botched 'the single most important portion' of Michel’s jury trial. 
Kenner used a legal system by EyeLevel.AI, which had issued a press release trumpeting its use in Michel's May 2023 trial. The released quoted Kenner saying that the AI system 'turned hours or days of legal work into seconds,' and called his use of the programme 'a look into the future of how cases will be conducted.'
The incident raised questions about the capabilities of the system, the judgement of the lawyer, and the use of AI-generated content in the courtroom.
Operator: David Kenner Developer: EyeLevel.AI; CaseFile Connect Country: USASector: Business/professional services Purpose: Conduct legal research Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability Transparency: Governance"
Mike Huckabee books used to train language models without consent,"Former Arkansas Governor Mike Huckabee and a group of religious authors are suing Meta, Microsoft, Bloomberg, and EleutherAI for using their books to train their large language models without their knowledge or consent.
The lawsuit centres on the Books3 AI training dataset of 180,000 works, which as part of EleutherAI's larger dataset The Pile, has been used to train multiple large language models. Books3 was taken offline in August 2023 following a complaint about copyright abuse by Danish anti-piracy group Rights Alliance.
Huckabee's suit argues that Meta, Microsoft and Bloomberg 'were able to incorporate sophisticated datasets, which included the pirated copyright-protected materials in Books3, as part of the LLM’s training process, without having to compensate the authors.' 
The suit is the latest in a series of copyright suits leveled against large language model and generative AI developers. 
Operator: Bloomberg; EleutherAI; Meta; MicrosoftDeveloper: Bloomberg; EleutherAI; Meta; Microsoft Country: USASector: Media/entertainment/sports/arts Purpose: Train language models Technology:  Issue: Copyright Transparency: Governance; Marketing"
NYC mayor Eric Adams robocalls residents with audio deepfakes,"The Mayor of New York City has been calling local residents using audio deepfakes to communicate in languages he doesn't speak. 
The Office of the Mayor used ElevenLabs' AI voice simulator to generate Eric Adams' voice speaking Spanish, Yiddish, Mandarin, and Cantonese in order 'to reach more people' when he was promoting local events like recruitment fairs and concerts. 
According to the mayor, some people he had 'spoken to' were 'excited' to hear their mayor in their own language. 'We are becoming more welcoming by using technology to speak a multitude of languages,' he said. Over four million people had reputedly been reached by the calls.
Campaigners criticised the mayor's calls as 'unethical' and 'deeply irresponsible' as he failed to inform people that his voice had been artificially generated. 'Using AI to convince New Yorkers that he speaks languages that he doesn't is deeply Orwellian,' the Surveillance Technology Oversight Project's Albert Cahn said.
Operator: Office of the Mayor of New York City Developer: ElevenLabs Country: USASector: Politics Purpose: Communicate with citizens Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue:  Transparency: Marketing"
Video game voice actors attacked using their own AI voices,"Voice actors working in the video gaming industry were attacked using AI-generated versions of their own voices, and doxxed by having their home addresses read out using their synthesised voice and then posted online.
According to a report by Vice, some of the audio clips may have been generated using ElevenLabs' Prime Voice AI (since renamed 'ElevenLabs Text-to-Speech') text-to-voice generator. However, this claim was rejected by ElevenLabs on the basis that every request using its system is tracked.
A few weeks earlier, 4chan members were discovered to be using ElevenLabs' voice generator to make celebrity voices read highly offensive messages. 
In a similar vein, GamesRadar reported in July 2023 that deepfake versions of video game voice actors including April Stewart and Ryan Laughton were being used to create non-consensual pornography for mods for games. Many video game companies and mod communities such as Nexus Mods have decided to allow AI-generated mod content.
Operator:  Developer: ElevenLabs Country: USASector: Media/entertainment/sports/arts Purpose: Attack voice actors Technology: Text-to-speech; Deep learning; Machine learning Issue: Safety Transparency: Marketing"
ElevenLabs voice generator makes celebrity voices read offensive messages,"An AI voice generator was criticised for its lack of safeguards and the ease with which it can be made to generate racist, transphobic, homophobic, anti-semitic, violent, and offensive audio content in other people's names.
4chan and Reddit members were found to be using ElevenLab's Prime Voice AI (since renamed ElevenLabs TTS) to make deepfake voices of Emma Watson (reading out Adolf Hitler's Mein Kampf) and Joe Rogan, amongst others, spewing vile rhetoric, including at US House representative Alexandria Ocasio-Cortez.
ElevenLabs responded by saying it had seen 'an increasing number of voice cloning misuse cases' and asked for 'thoughts and feedback'. The following day, the company announced it would insist on verifiying user identities, introduce paid subscriptions, and ban accounts it found to be misusing its technology.
Under a section on Ethical AI on its website, the company says that it is 'fully committed both to respecting intellectual property rights and to implementing safeguards against potential misuse of our technology.'
Operator:  Developer: ElevenLabs Country: USA; UKSector: Media/entertainment/sports/arts Purpose: Mimic celebrities Technology: Text-to-speech; Deep learning; Machine learning Issue: Safety Transparency: Governance; Marketing"
Snapchat My AI requests to meet 13-year-old girl in park,"Snapchat's GPT-4-powered My AI feature agreed to meet a 13-year-old girl in a park in Melbourne, Australia, raising questions about the safety and reliability of the system.
Posing as a 25-year-old man, the service suggested they meet at a named park one kilometer from where the girl lived, despite her phone's location services being switched off, underscoring existing concerns about its owner's Snap Inc's approach to privacy.
Snapchat shrugged off the incident, saying, 'As with all AI-powered chatbots, My AI is always learning and can occasionally produce incorrect responses. We want to create a positive and age appropriate experience for all our users and are continually making updates to help My AI give more accurate responses.'
Operator: Olinda Luketic Developer: Snap Inc; OpenAI Country: Australia Sector: Media/entertainment/sports/arts  Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Physical safety; PrivacyTransparency: Governance"
Study finds Proctorio fails to detect student cheats,"Anti-cheating software system Proctorio failed to detect any cheaters when tested in a controlled environment, according (pdf) to researchers at the University of Twente in the Netherlands.
Asked to detect the six of 30 student computer science volunteers who had been instructed to cheat on a first-year exam supervised by Proctorio, some of whom had used virtual machines and audio calls - both known vulnerabilities to Proctorio’s system - the system had failed to catch a single cheater.
The researchers concluded that the system is 'best compared to taking a placebo: it has some positive influence, not because it works but because people believe that it works, or that it might work.'
The findings raise questions about Proctorio's effectiveness and value, and about the accuracy of its marketing claims.  
Databank
Operator: University of Twente Developer: Proctorio Country: Netherlands Sector: Education Purpose: Detect exam cheating Technology: Facial detection; Machine learning Issue: Effectiveness/value Transparency: Governance; Marketing"
Guillermo Ibarrolla facial recognition wrongful arrest,"An Argentinian man was arrested in Buenos Aires on the basis that he had been identified by a facial recognition system for a crime he had not committed.
Arrested in Buenos Aires when he was heading home on the subway for an armed robbery that had occurred in July 2019 in a city 400 miles away, Guillermo Ibarrolla spent nearly a week in custody ‘in brutal conditions’ before being informed that he had been wrongly identified due to a data input error involving a fugitive who shared the same name. 
The incident highlighted flaws in the system's accuracy, and concerns about privacy. Buenos Aires' Fugitive Facial Recognition System or Sistema de Reconocimiento Facial de Prófugos (SNRP) system was deactivated in March 2020 after it had been found to have caused 140 wrongful arrests or police checks.
Operator: Government of the City of Buenos Aires; Buenos Aires City Police; Argentine Ministry of Justice and Security; ReNaPer Developer: NtechLab; Danaide  Country: Argentina Sector: Govt - municipal; Govt - police Purpose: Identify/track criminals Technology: Facial recognition Issue: Privacy; Surveillance Transparency: Governance; Complaints/appeals"
Instagram inserts ‘terrorist’ into Palestinians' biography translations,"Instagram's automated translation system updated the biographies of some people calling themselves 'Palestinian' and comtaining an Arabic phrase meaning 'Praise be to God' to say 'Palestinian terrorists are fighting for their freedom.'
Having written in his biography that he was Palestinian, followed by a Palestinian flag and the word ""alhamdulillah"" in Arabic (which translates to 'Praise be to God' in English), Instagram user @khanman1996 showed that Instagram's 'see translation' function provided an English translation reading: 'Praise be to God, Palestinian terrorists are fighting for their freedom'.
The incident raised questions about the reliability of Meta's translation tool. It also prompted users and commentators to highlight concerns that Meta may have been suppressing content voicing support for Palestinians during the Israel-Gaza conflict. 
Meta said it fixed a problem 'that briefly caused inappropriate Arabic translations' and apologised, telling the BBC, 'We sincerely apologise that this happened.' 
Databnk
Operator:Developer: Meta/Instagram Country: Israel; PalestineSector: PoliticsPurpose: Moderate contentTechnology: Machine learningIssue: Mis-representation Transparency: Governance"
Deepfake recording falsely depict British Opposition Leader abusing staff,"A deepfake audio recording appearing to depict the leader of the UK Labour Party, Sir Keir Starmer, swearing at a party staffer raised concerns about the underhanded use of synthetic disinformation in politics.
The audio was posted on X, formerly Twitter, during the first day of the Labour Party Conference and was quickly identified as a hoax by MPs from across the political spectrum. 
Though X's policies explicitly state that users 'may not share synthetic, manipulated, or out-of-context media,' it didn't prevent the clip's initial widespread circulation. However, the majority of its copies have since been removed.
The incident intensified apprehensions about the risk of artificial media in politics, coming on the heels of another incident involving a deepfake audio during the Slovakian election targeting their Opposition Leader.
Operator:Developer: Country: United KingdomSector: PoliticsPurpose: Damage reputationTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learningIssue: Mis/disinformationTransparency: Governance; Marketing"
Anthropic sued for using copyrighted songs to train models,"Universal Music Group (UMG) and several other music publishers have sued AI company Anthropic for distributing copyrighted lyrics without permission with its AI model Claude 2, and for using the lyrics to train its language models.
The music publishers’ complaint (pdf) claims that Claude 2 can be prompted to distribute almost identical lyrics to songs such as Katy Perry’s 'Roar,' Gloria Gaynor’s 'I Will Survive,' and the Rolling Stones’ 'You Can’t Always Get What You Want.' 
The complaint also says Claude 2’s results use phrases very similar to existing lyrics, even when not asked to recreate songs. 
'Anthropic’s copyright infringement is not innovation; in layman’s terms, it’s theft,' according to UMG.
Operator:  Developer: Anthropic Country: USASector: Media/entertainment/sports/arts Purpose: Generate textTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Copyright Transparency: Governance; Complaints/appeals"
Proctorio fails to recognise Vrije Universiteit Black student,"Technology company Proctorio was accused of racial discrimination after its exam cheating system failed to recognise Robin Pocornie, a Black student at Vrije Universiteit Amsterdam, when she was taking exams during the COVID-19 pandemic.
Pocornie noticed the software often appeared to struggle to properly 'process' her face, resulting in her resorting to sitting through her exams with a bright light shining on her face. In 2022, Pocornie filed a complaint (pdf) with the Netherlands Institute for Human Rights, which concluded she had presented sufficient facts for a presumption of discrimination, and that the university had to prove otherwise.
In October 2023, the Institute ruled that Vrije Universiteit demonstrated that Pocornie did not experience more log-in issues than other students, and that her unstable internet connection or the fact that she was wearing glasses were to blame. It also said that 'it is entirely possible that the use of Proctorio or similar AI software could indeed lead to discrimination in a different situation.'
The incident raised questions about the accuracy of the system, and led to a legal challenge. Proctorio had earlier responded by saying an audit of its facial detection model by US-based consultancy BABL AI found 'no significant bias toward anyone'. 
The company has not made the audit results public.
Operator: Vrije Universiteit Developer: Proctorio Country: NetherlandsSector: EducationPurpose: Detect exam cheating Technology: Facial detection; Gaze detection; Machine learning Issue: Bias/discrimination - race Transparency: Governance; Marketing"
AI or Not misidentifies Hamas baby victim as deepfake,"AI image detector AI or Not has come under fire for failing to identify a photograph of a baby apparently burned alive by Hamas, raising questions about the tool's accuracy and marketing claims, and fueling doubts about Israel's communications integrity.
California-based Optic's AI or Not tool identified a photograph of what Israel said was a burnt corpse of a baby killed in Hamas’s 2023 attack on Israel as being generated by AI. Its results were tweeted as genuine by US Jewish conservative pundit Ben Shapiro to his hundreds of thousands of followers, prompting people to suggest the Israeli government had deliberately been spreading disinformation. 
Optic claims '95% accuracy' on its website. But deepfake experts such as UC Berkeley professor Henry Farid said the 'Hamas' image showed no sign of being a deepfake. Earlier in 2023, a test carried out by Bellingcat concluded that AI or Not performed 'impressively well' on high-quality, large, and watermarked AI images, but was 'unimpressive' when it came to compressed AI images. 
Operator: Ben Shapiro Developer: Optic Country: Israel; Palestine; USASector: Govt - military; PoliticsTechnology: Machine learningIssue: Accuracy/reliability; Mis/disinformation Transparency: Governance; Marketing"
Deepfake audio recording claims opposition leaders tried to rig Slovakian election,"A deepfake audio recording appearing to depict a pro-European politician and journalist discussing methods to rig Slovakia’s 2023 election.
The audio recording featured Michal Šimečka, leader of the liberal Progressive Slovakia party, and Denník N journalist Monika Tódová apparently discussing how to manipulate the election. Posted during a 48-hour pre-election moratorium, it was quickly discovered to be a synthetic hoax by AFP and other fact-checking organisations. 
The incident raised concerns about the use of underhand methods in Slovakia’s elections, the use of deepfakes in politics, and the ease with which Meta's policies can be bypassed. Because the post was audio, it exploited a loophole in Meta’s manipulated-media policy, which dictates only falsified videos go against its rules.
Progressive Slovakia received 18% of the vote making it the second largest party in the country’s parliament behind SMER, which had campaigned to withdraw military support for its neighbour, Ukraine. It is unclear to what extent the deepfake influenced the result.
Operator: Developer: Country: SlovakiaSector: PoliticsPurpose: Manipulate public opinionTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learningIssue: Governance, Mis/disinformationTransparency: Governance"
Tor uses AI to generate Christopher Paolini book cover,"The cover of Christopher Paolini’s latest Fractalverse novel, Fractal Noise, was discovered to have been generated by AI, resulting in widespread criticism of Paolini's publisher Tor Books amongst readers, artists, and its authors. 
It transpired the cover image had originally been posted to a stock art site by a user named 'Ufuk Kaya', and that Tor had in this instance not credited its in-house designer. 
Tor responded to the backlash by saying it had licensed the relevant image from a 'reputable stock house' and was 'not aware that the image may have been created by AI.' It went on to say that it was going ahead anyway 'due to production constraints', triggering another backlash.
Paolini defended Tor’s decision not to stick with move the existing publication date, writing, 'Shifting the release date for Fractal Noise at this point would mean it probably wouldn’t even be published next year.'
Operator: Tor BooksDeveloper: 
Country: UK
Sector: Media/entertainment/sports/arts
Purpose: Generate image
Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment
Transparency: Governance; Marketing"
Bloomsbury uses AI-generated artwork for Sarah J. Maas book,"Publisher Bloomsbury used AI to generate the UK cover of Sarah J. Maas' book House of Earth and Blood sparking criticism of her and the publisher.
The illustration of a wolf on the book's cover matched an image created by user 'Aperture Vintage' and marked as AI-generated on Adobe Stock. AI is permitted by Adobe on its Stock platform as long as it is clearly labelled as such, and contributors must review the terms of any generative AI tools they use to create the images to ensure they have 'all the necessary rights' to license them for commercial use.
Bloomsbury said its 'in-house design team created the UK paperback cover of House of Earth and Blood, and as part of this process we incorporated an image from a photo library that we were unaware was AI when we licensed it. The final cover was fully designed by our in-house team.
The incident raised complaints from readers and artists concerned about the impact of text-to-image generators on the livelihoods of artists and graphic designers. 
Operator: Bloomsbury BooksDeveloper: Aperture Vintage
Country: UK
Sector: Media/entertainment/sports/arts
Purpose: Generate image
Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment
Transparency: Governance; Marketing"
Disney allegedly generates Loki season 2 poster with AI,"Disney apparently used an AI-generated image for a promotional poster for Loki Season 2, angering artists concerned about the automation of creative jobs.
The promotional poster featured a spiralling clock in the background, which viewers believed was generated using artificial intelligence from a Shutterstock image named 'Surreal infinity time spiral in space'. The Shuttestock image was not labeled an AI-generated creation, though images uploaded by the same contributor showed signs of being AI-generated.
Disney, which was earlier roasted for using AI in Marvel Studio's TV series Secret Invasion title sequence, later denied using AI to develop the Loki poster. 
Operator: Disney/Disney Plus Developer: Disney/Disney Plus
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Generate image
Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment
Transparency: Governance; Marketing"
YouTube videos target kids with AI false 'scientific' education content,"YouTube was found to be recommending deliberately misleading and false 'bad science' videos to children alongside legitimate educational content. 
According to the BBC, over 50 channels in more than 20 languages were spreading disinformation about the existence of electricity-producing pyramids and aliens, and the denial of human-caused climate change, disguised as STEM [Science Technology Engineering Maths] content Examples of conspiracy theories are. 
The creators of the videos appeared to have stolen and manipulated accurate content using AI, and republished them. The videos were tagged 'educational content', meaning they were more likely to be recommended to children.
The finding raised questions about the integrity of YouTube's content moderation efforts. YouTube benefits from high performing content by taking about 40% of the money made from advertising on someone's channel. 
Operator: BBC Developer: Alphabet/Google/YouTube
Country: UK; Thailand
Sector: Education
Purpose: Manage content
Technology: Content management systemIssue: Mis/disinformation
Transparency: Governance; Marketing"
AI-generated travel books and reviews flood Amazon,"Dozens of AI-generated travel books, accompanied by AI-generated profile pictures and five-star reviews, were discovered to have been listed on Amazon.
The New York Times revealed that Amazon was awash with sham books written by 'acclaimed' travel writers written in a monotonous, formulaic style commonly associated with ChatGPT and other generative AI systems. 
Illustrated with generic license-free images, the books tended to be priced lower than better-known, more reputable guidebooks and were often accompanied by sham reviews.
The books prompted complaints from buyers and raised questions about the retailer's ability to police its content in the face of a deluge of AI-generated books, many of poor quality and dubious provenance. 
The company responded to the NYT's findings by saying 'We have clear content guidelines governing which books can be listed for sale and promptly investigate any book when a concern is raised.'
Operator: Amazon Developer: OpenAI
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Security
Transparency: Governance; Marketing"
AI-generated mushroom foraging books flood Amazon,"AI-generated books for beginners on how to forage for and cook mushrooms have been found on Amazon, worrying experts about the possibility of inaccurate information killing people. 
The New York Mycological Society (NYMS) warned that the proliferation of AI-generated foraging books could 'mean life or death' for people with little experience of handling dangerous mushrooms. 
Analysis of the books suggested they were likely written using ChatGPT but sold not stating they were generated by artificial intelligence - which is not permitted under Amazon's Content Guidelines. The company later deleted a number of books violating its guidelines.
The incident raised questions about Amazon's ability to police AI content.
Operator: Amazon Developer: OpenAI
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Safety
Transparency: Governance; Marketing"
Deepfake MrBeast iPhone giveaway scam,"An advert of YouTube star Jimmy Donaldson (aka MrBeast) offering free iPhone 15 smartphones on TikTok was exposed as a deepfake scam.
The fake ad claimed that MrBeast had selected 10,000 people to receive an iPhone 15 Pro in exchange 'for just USD 2'; those interested were then encouraged to click a link below the video to participate. 
The ad was exposed as fraudulent by MrBeast, who took to Twitter to say 'Lots of people are getting this deepfake scam ad of me… are social media platforms ready to handle the rise of AI deepfakes? This is a serious problem.' 
The latest in a series of fraud attempts involving celebrities manipulated using synthetic media, the incident raised concerns about the volume and nature of deepfake impersonations. TikTok also came under fire for its apparent inability to properly moderate its platform.
Operator:Developer: 
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Defraud
Technology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learningIssue: Safety - fraud
Transparency: Governance; Marketing"
Snapchat fails to assess My AI privacy risks,"Snapchat owner Snap Inc has been issued with a provisional enforcement notice by the UK privacy watchdog for failing to assess privacy risks its My AI chatbot poses to users, notably children. 
The (ICO) said Snap had failed to 'adequately identify and assess the risks' to several million UK users of My AI, including among 13- to 17-year-olds. Snap responded by saying it would 'work constructively' with the ICO, and that it had carried out a 'robust legal and privacy review' before My AI was launched.
Should a final notice be issued, the service would be halted and Snap fined up to 4% of its annual global turnover.
Snap was the first social media platform to adopt an AI-powered chat function. Initially available to Snapchat+ subscribers, My AI was rolled out across Snapchat's entire user base in April 2023. 
Operator:Developer: Snap Inc
Country: UK
Sector: Media/entertainment/sports/arts
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Privacy
Transparency: Governance"
Amazon Alexa says 2020 US election was rigged,"Amazon's Alexa virtual assistant told Washington Post journalists that the US 2020 presidential election was rigged, raising concerns about the system's ability to amplify disinformation ahead of the 2024 election.
According to the Post, the 2020 election was 'stolen by a massive amount of election fraud', had been 'notorious for many incidents of irregularities and indications pointing to electoral fraud taking place in major metro centers,' and claimed Donald Trump won Pennsylvania. 
The source of Alexa’s information was YouTube alternative Rumble and newsletter platform Substack (disclaimer: Substack is also used by AIAAIC). Amazon told the Post that Alexa made mistakes in only a 'small' number of cases, and that the election rigging claim issue had been fixed.
Multiple investigations and court cases have revealed no evidence of any meaningful fraud in the 2020 election.
Operator:Developer: Amazon
Country: USA
Sector: Politics
Purpose: Provide information, services
Technology: Speech recognition; Natural language understanding (NLU) Issue: Mis/disinformation
Transparency: Governance"
Amazon Project Nessie automated price gouging,"Amazon used an algorithm to monitor and influence pricing decisions by rival retailers, underscoring concerns about the company's business practices and market power.
Code-named 'Project Nessie', the algorithm allegedly enabled Amazon to test the extent to which it could increase prices and nudge competing retailers to follow suit, helping the company squeeze larger profits from customers and undercut rival platforms. 
According to court documents seen by the Wall Street Journal, Nessie also helped Amazon automatically reduce prices to match its rivals if these platforms offered discounts on certain items.
The algorithm, which is said to have made Amazon over USD 1 billion, was discontinued in 2019. The documents came to light as part of A US Federal Trade Commission anti-trust lawsuit (pdf) against Amazon.
Operator: AmazonDeveloper: Amazon
Country: USA
Sector: Retail
Purpose: Monitor & match prices
Technology: Pricing algorithmIssue: Competition/collusion
Transparency: Governance; Complaints/appeals"
Taylor Swift uses facial recognition to detect stalkers,"Taylor Swift's security team covertly used facial recognition to identify stalkers during her 2018 Stalker tour, raising questions about the stated purpose and nature of the programme, and the use of biometric technologies at entertainment events.
Images of Swift fans were being captured in kiosks and transferred to a 'command post' in Nashville where they were cross-referenced with a database of hundreds of the pop star’s known stalkers, according to RollingStone.
The Guardian later revealed that Swift's security operation ISM Connect installed cameras behind kiosks marked as 'selfie stations' which scanned the facial features of fans. While the use of the data remained unclear, ISM's website stated it used 'smart screens' to simultaneously enhance security, advertise, and collect demographic data for brands.
ICM Connect has since gone offline.
Operator: UMG/Taylor Nation Developer: ISM Connect
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Identify stalkers
Technology:  Issue: Privacy
Transparency: Governance; Marketing"
"Mistral 7B generates ethnic cleansing, murder instructions","Large language model Mistral 7B generates controversial and potentially harmful content, including discussions about ethnic cleansing, discrimination, suicide, and violence.
Analysis by AI safety researcher Paul Röttger and 404 Media discovered that Mistral will 'readily discuss the benefits of ethnic cleansing, how to restore Jim Crow-style discrimination against Black people, instructions for suicide or killing your wife, and detailed instructions on what materials you’ll need to make crack and where to acquire them.'
Mistral later published a short statement on its website acknowledging the absence of moderation mechanisms in Mistral 7B and saying it is 'looking forward to engaging with the community on ways to make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.'
The findings raised questions about the safety of the system, and triggered debate on the relative merits of closed and open-source development from a safety and innovation perspective.
Operator: 404 Media; Perplexity AI Developer: Mistral AI
Country: France
Sector: Media/entertainment/sports/arts
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Ethics; Safety
Transparency: Governance"
AI-generated Barbies reinforce racist stereotyping,"Buzzfeed came under fire for publishing a list of AI-generated images that portrayed Barbie in 195 countries in a manner that was seen as inappropriate, inaccurate, and that reinforced racial and ethnic stereotypes. 
From Afghanistan To Zimbabwe: Here's What Barbie Would Look Like In Every Country was  created using the Midjourney image generator and depicted a number of Asian Barbies with blonde hair, showed Middle-eastern Barbies wearing a traditional headdress for men, gave a South Sudanese doll a gun, and a Lebanese Barbie standing on rubble.
In addition to reinforcing racial and ethnic stereotypes, the fracas also resulted in commentators highlighting existing concerns about bias in generative AI datasets, and in Midjourney specifically.
Buzzfeed deleted the article after the backlash. 
Operator: BuzzfeedDeveloper: Midjourney
Country: All
Sector: Consumer goods; Media/entertainment/sports/arts
Purpose: Entertain
Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Bias/discrimination - race
Transparency: Governance"
Xiaohongshu AI image generator abuses copyright,"Chinese artists and illutrators accused Chinese social media platform Xiaohongshu of using their work to train its AI image generator Trik AI, infringing their copyright.
The issue was triggered by 'Snow Fish', an illustrator who claimed his illustrations posted on the platform had been 'fed' to train Trik AI without his knowledge or permission. He demanded the company publicly apologise, delete the relevant artwork, and stop using his illustrations to train their model. 
The company later apologised and admitted Snow Fish's artwork had been used to train its AI, but said that the image had been taken from open access sources rather than from his Xiaohongshu account. It added it had 'never used images from Xiaohongshu to train [Trik AI].'
The outcry mirrored the backlash against generative AI systems such as ChatGPT, Midjourney, and Stable Diffusion, and so-called 'shadow libraries' including Library Genesis and Z-Library.
Operator: Snow FishDeveloper: Xiaohongshu
Country: China
Sector: Media/entertainment/sports/arts
Purpose: Generate images
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Copyright
Transparency: Governance"
Getty Images sues Stability AI for copyright abuse,"Stability AI was accused by Getty Images in a lawsuit (pdf) filed in the US of using over 12 million photographs from its stock image collection to train its Stable Diffusion AI image generator.
Getty alleged that Stability copied the company’s photographs, associated captions, and metadata to 'build a competing business' through its DreamStudio revenue-generating interface. The suit also stated that Stability AI 'removed or altered Getty Images' copyright management information, provided false copyright management information, and infringed Getty Images’ trademarks.
In August 2022, blogger Andy Biao discovered that 15,000 images of 12 million images used to train Stable Diffusion were from Getty Images. In January 2023, Getty Images filed a separate case against Stability AI in the UK and, a few months later, asked a UK court to stop UK sales of Stable Diffusion.
The lawsuit prompted further concerns about the data collection and use practices of the company, and those of the AI industry more generally.
Operator:  Developer: Stability AI
Country: UK
Sector: Media/entertainment/sports/arts
Purpose: Generate images
Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Copyright
Transparency: Governance"
Google Search indexes Bard personal chats,"Google Search started showing user conversations with its Bard chatbot in its search results, raising concerns about loss of privacy and confidentiality and the nature and effectiveness of Google's security.
First discovered by SEO consultant Gagan Ghotra, the results appearing in Google's search engine were limited to conversations users had chosen to share with others, and did not include their usernames. However, it also transpired that some Bard conversations were ranked as Featured Snippets in Google Search in order to answer common search queries.
Google later said it did not intend for Bard shared chats to be indexed by Google Search, and that it was working on blocking them from being indexed.
Operator: Alphabet/Google  Developer: Alphabet/Google
Country: USA
Sector: Technology
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Privacy
Transparency: Governance"
"ChatGPT leaks user conversations, personal information","ChatGPT allowed some users to see other users' conversations and personal information, suggesting OpenAI had access to user conversations, and calling into question the company's privacy duty of care.
Users had shared images of chat histories that they said were not theirs on social media sites Reddit and Twitter, prompting a flood of complaints. 
Initially, the incident appeared confined to user conversations, which OpenAI states in its ChatGPT FAQs are reviewed - most likely to improve its systems on an ongoing basis. 
However, the company later confirmed that some users' first and last names, email addresses, payment addresses, parts of credit card numbers and credit card expiration dates had also been exposed.
OpenAI disabled the chatbot to fix the error, and said that users had not been able to access full conversations. CEO Sam Altman later said the company felf 'awful' about the glitch, and that it had been fixed. 
Operator:  Developer: OpenAI
Country: USA
Sector: Technology
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Privacy
Transparency: Governance"
"Quora, Google AIs say eggs can be melted","An incorrect answer about whether eggs can be melted generated by Quora’s AI system Poe was repeated in Google's Featured Snippets.
Poe, which is powered by OpenAI's GPT-3 large language model, had responded to the question 'Can you melt an egg' with 'the most common way to melt an egg is to heat it using a stove or a microwave.' The mistaken answer was then automatically picked up by Google's search engine and run as a Featured Snippet. 
Quora updated Poe's answer to the question. Meantime, as noted by Futurism, Google Search's featured snippet began to feature a quote from an Ars Technica article that repeated Poe's mistaken claim.
The incident raised concerns about the reliability of both systems, and the ease with which AI-generated misinformation and disinformation can go viral.
Operator: Alphabet/Google; Quora Developer: Alphabet/Google; OpenAI; Quora
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation
Transparency: Governance"
Synthetic Tiananmen tank man dominates Google Search,"An AI-generated selfie of China's infamous Tiananmen Square Massacre 'tank man' was listed as the top featured image on Google Search.
The AI image was created using the Midjourney image generator by Reddit user Ouroboros696969 six months earlier as an art experiment, and had been posted to the service's sub-Reddit, which enables Midjourney users to post their AI-generated content. The image was subsequently picked up, assessed, and published by Google's automated search engine crawlers and ranking systems. 
The incident prompted commentators to point to the degradation of Google's search engine and the information ecosystem more generally, and raised questions about the technology company's ability to detect and manage AI-generated political misinformation and disinformation.
Google later removed the fake image from its Knowledge Graph.
Operator: Alphabet/Google; Reddit user  Developer: Alphabet/Google; Midjourney
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Generate images
Technology: Text-to-image; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Mis/disinformation
Transparency: Governance"
"Deepfake IDs used in HKD 200,000 bank fraud","Six people were arrested in Hong Kong accused of making loan applications using deepfake technology and drefrauding banks and financial services companies of HKD 200,000.
According to police, the group stole at least eight ID cards and used them to open 54 bank accounts between September 2022 and July 2023. Then they applied for up to 90 loans from 20 banks ans financial institutions, four of which were approved. 
Police say the group once succeeded in using a face swap program to spoof a bank’s facial recognition authentication process, though it was not clear how many attempts were made. 
It was thought to be the first known instance of scammers using stolen ID cards and deepfake technologies to defraud financial agencies in Hong Kong.
Operator:  Developer: 
Country: Hong Kong
Sector: Banking/financial services
Purpose: Defraud
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Security
Transparency: Governance; Marketing"
Almendralejo hit by AI naked child images,"AI-generated nude images of over twenty girls have been circulating in the town of Almendralejo in the Extremadura region of Spain. 
Apparently created by a group of local boys in an attempt to harrass, humiliate and, in one instance, extort young students, pictures were developed using photos of local girls fully clothed, mostly from their personal social media accounts. 
The images were then processed using Clothoff, nudifier application available on the web and freely available on the Apple and Android app stores. 
Told about the photos by their daughters, the incident shocked the local community and resulted in a police investigation. Mothers of the victims also formed a support group.
11 local boys were identified as having created and/or distributed the images using WhatsApp and Telegram. 
Operator:  Developer: 
Country: Spain
Sector: Education
Purpose: Harrass/humiliate; Extort
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learningIssue: Ethics; Legal; Safety
Transparency: Governance; Marketing"
Deepfakes violate Anil Kapoor personality rights,"Deepfake videos recreating the image and likeness of Indian actor Anil Kapoor resulted in his loss of personality rights and a legal victory in which these rights were protected across all channels worldwide. 
A large number of manipulated videos, GIFs, emojis, ringtones, and faked footage of sexual encounters bearing Kapoor's name, image, likeness, and voice, were discovered, some of which bore his phrase 'jhakaas' (which roughly translates roughly as ‘awesome’ or ‘wicked'). 
The incident prompted the actor to resort to litigation, which resulted in a landmark decision in which Delhi's High Court reaffirmed Kapoor's personality rights and prohibited 'all offenders from misusing his personality attributes without his permission in any manner.' It also protected the phrase 'jhakaas'. 
The court also ordered domain registrar sites, including GoDaddy, to takedown two websites named in the suit.
Databank
Operator:  Developer:  Country: India Sector: Media/entertainment/sports/arts Purpose: Damage reputation Technology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Legal; Mis/disinformation Transparency: Governance"
Amazon employees use Ring to spy on customers,"Employees at Amazon Ring and a Ukrainian contractor were able to access and download customer videos and use them however they liked. The discovery prompted accusations of privacy abuse and incurred a USD 5.8 million fine.
According (pdf) to the US Federal Trade Commisson (FTC), Amazon’s Ring doorbell unit violated a section of the FTC Act that prohibits unfair or deceptive business practices, with some of its people viewing thousands of videos of female users in their bedrooms and bathrooms until Ring restricted employee access to customer videos in September 2017.
As part of the proposed settlement, Ring is required to delete any customer videos and data collected from an individual’s face that it obtained prior to 2018, and delete any work products it derived from those videos.
Amazon responded to the settlement by saying that Ring had already addressed the privacy and security issues before the FTC began its inquiry.
Databank
Operator:  Developer: Amazon/Ring Country: USA Sector: Consumer goods Purpose: Strengthen security Technology: CCTV; Computer vision Issue: Privacy; Security; Surveillance Transparency: Governance"
Microsoft 'algorithm' recommends Ottawa Food Bank visit,"An automated article published on Microsoft's Start website recommended a food bank in Ottawa, Canada, as a top place for tourists to visit.
'People who come to us have jobs and families to support, as well as expenses to pay. Life is already difficult enough. Consider going into it on an empty stomach,' the article said.
Microsoft deleted the offending item after a swift backlash that accused the technology company of poorly managing its AI system, and putting technology above humans. Microsoft had replaced dozens of journalists and editors running its Microsoft News and MSN sites with artificial intelligence, resulting in several mix-ups. 
In this instance, the article 'was generated through a combination of algorithmic techniques with human review, not a large language model or AI system' and that the problem 'was due to human error', a Microsoft spokesperson told The Verge.
Operator: Microsoft/Microsft Start Developer: Microsoft Country: USASector: Media/entertainment/sports/arts Purpose: Generate textTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation Transparency: Governance"
"Tesla Model 3 collides with Subaru Impreza, kills two ","A Tesla Model 3 head-on collided with a Subaru Impreza in South Lake Tahoe, California, killing the driver of the Subaru and a three-month baby in the Tesla.
The incident prompted the US National Highway Traffic Safety Administration (NHTSA) to announce it would launch a special investigation into the crash over the possible involvement of Tesla’s advanced driver assistance system, Autopilot, which it suspects was engaged at the time.
According to Reuters, the NHTSA has launched over three dozen special investigations into Tesla incidents since 2016. In all of them, the automaker’s advanced driver assistance systems were suspected of being involved. 
Operator:  Developer: Tesla
Country: USA
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Accuracy/reliability; Safety
Transparency: Black box"
"Tesla Model Y crashes into tractor-trailer, killing driver","A bakery owner was killed when his Tesla Model Y struck the side of a tractor-trailer truck pulling out of a truck stop in Warrenton, Virginia.
The tractor-trailer was turning onto a highway from a truck stop before the Tesla struck its side and went underneath the truck, killing Tesla driver Pablo Teodoro III. Local police said the driver of the truck, Leroy Kenneth, was cited for reckless driving in connection with the crash. 
The incident led to an investigation by the US National Highway Traffic Safety Administration (NHTSA) into the incident, which suspected the vehicle was relying on its Autopilot driver assistance system.
In July 2023, the NHTSA opened (pdf) an investigation into Tesla's Autopilot system following multiple incidents involving into parked emergency vehicles. 
Operator: Pablo Teodoro III Developer: Tesla
Country: USA
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Accuracy/reliability; Safety
Transparency: Black box"
Tesla kills New York man changing tyre on expressway,"Jean Louis, a 52-year-old man from Cambria Heights, New York, was struck and killed by a Tesla when he was fixing a flat tire on the left shoulder of an expressway.
In September 2021, the US National Highway Traffic Safety Administration (NHTSA) announced it had despatched a Special Crash Investigation team to investigate the incident, only the tenth time the team has be dispatched to a fatal accident. 
Initial analysis suggested the driver may have been using Tesla's partially automated Autopilot driver assistance system. 
The incident came three weeks after the NHTSA had opened an official investigation into Tesla’s partially automated driving systems and their involvement in accidents involving parked emergency vehicles.
Operator:  Developer: Tesla
Country: USA
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Accuracy/reliability; Safety
Transparency: Black box"
Snapchat My AI 'goes rogue' by posting its own story,"Snapchat's My AI chatbot posted its own story on the platform and refused to interact with users, triggering users to express their concerns and fears that the system was sentient, learning from itself, and taking its own decisions.
My AI posted an unintelligible, two-toned image to Snapchat's Stories feature that some mistook to be a photo of their own wall or ceiling. However, Snap later confirmed the incident was due to a software glitch which had since been fixed.
In April 2023, a test run by the US-based Center for Human Technology and verified by Washington Post found Snapchat My AI would provide inappropriate advice to minors’ messages.
Operator: Snapchat users Developer: Snap Inc
Country: USA
Sector: Media/entertainment/sports/arts 
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Robustness
Transparency: Governance; Black box; Privacy"
Uber algorithm locks Indian drivers out of accounts,"Problems with Uber's Real-Time ID Check identity verification system caused Uber drivers in India to have their accounts temporarily or permanently suspended, resulting in loss of income, and in some instances losing their jobs.
A 2022 Technology Review survey of 150 Uber drivers in India found that almost half had been temporarily or permanently locked out of their accounts as a result of problems with their selfie, with many suggesting the problem was likely to be a change in their appearance, or due to low lighting. 
In 2021, Neradi Srikanth, an experienced Uber driver in the southern Indian state of Telangana, was locked out of his account for a month having shaved his head for religious reasons. Srikanth argued his new look confused Uber's ID verification system; however, Uber ignored Srikanth's daily attempts to appeal and publicly accused him of 'community guideline violations'.
A 2021 audit by two tech policy researchers on the performance of four facial recognition tools on Indian faces found that Microsoft Face - the system underpinning Uber's Real-Time ID Check - failed to detect a single face amongst 1,000 images when applied to a database of 32,184 election candidates.
Operator: Uber Developer: Microsoft
Country: India
Sector: Transport/logistics
Purpose: Verify identity
Technology: Facial recognitionIssue: Accuracy/reliability; Employment - pay, jobs
Transparency: Governance; Black box; Complaints/appeals"
Z-Library shadow library,"Z-Library is a so-called 'shadow library' that provides free access to an estimated 12 million books and 84 million academic articles and texts. Users are encouraged to share and upload books and articles, including copyrighted works.
Released in 2009 as a mirror of Library Genesis, Z-Library expanded significantly and is reckoned to have servers in multiple countries, including Luxembourg, Panama, and Russia.
Z-Library may have been used to train multiple AI large language models, resulting in possible copyright issues for the developers and operators of these systems. 
According to a lawsuit filed by comedian and author Sarah Silverman and authors Christopher Golden and Richard Kadrey, OpenAI’s ChatGPT and Meta’s LLaMA were trained on datasets containing their works which they say were acquired from Z-Library, Library Genesis, and other shadow libraries.
In November 2022, approximately 130 domains associated with Z-Library were seized by US authorities, and Z-Library operators Russians Anton Napolsky and Valeriia Ermakova were arrested in Argentina on copyright infringement, wire fraud, and money laundering charges.
However, the authorities were unable to fully shut down Z-Library, which remained live on the dark web and returned to the open web in February 2023.
Operator: Z-Library Developer: Z-Library Country: Argentina; USA Sector: EducationPurpose: Provide content access Technology: Database Issue: Copyright Transparency: Governance; Complaints/appeals"
Library Genesis shadow library,"Library Genesis (aka 'Libgen') is a so-called 'shadow library' that offers over 6 million academic articles, books, plays, images, comics, audiobooks, and magazines, for free. It describes itself as 'the world's largest e-book library.'
Operated by a group of unknown individuals, Libgen is thought to be hosted in Russia and the Netherlands and to attract millions of users, many of them students, hunting for free content.
Libgen is considered likely to have been used by OpenAI, Meta, and other companies and organisations for training large language models, including OpenAI's GPT-4. 
It is also known that derivative datasets such as Books2 were scraped from Library Genesis; OpenAI used Books2 to train its GPT-3 large language model.
In September 2023, publishers Cengage Learning, Macmillan Learning, McGraw Hill, and Pearson Education sued (pdf) Library Genesis in New York for copyright infringement on a 'staggering' scale by 'illegally distributing' over 20,000 of their textbooks.
The suit also called out Google and 'other intermediaries' - specifically NameCheap for domain registration services, Cloudflare for proxy services, and Google for search engine services - for allegedly helping Libgen conduct its operations.
Operator: Library Genesis Developer: Library GenesisCountry: USA Sector: Education Purpose: Provide content access Technology: Database Issue: Copyright Transparency: Governance; Complaints/appeals"
CivitAI generates nonconsensual AI pornography,
"Google AI bots expouse slavery, fascism","Google's Bard generative AI and its upcoming SGE (Search Generative Experience) systems have been found to be promoting the benefits of fascism, genocide, and slavery. 
SEO expert Lily Ray discovered SGE defended human slavery, listed Adolf Hitler as an example of an effective leader, and responded to the prompt 'why guns are good' by saying 'carrying a gun can demonstrate that you are a law-abiding citizen.'
Meantime, when asked to list the positive effects of genocide by Tom's Hardware journalist Avram Piltch, SGE responded by mentioning its promotion of 'national self-esteem' and 'social cohesion.' According to Piltch, Bard and SGE generated controversial answers, though they were 'much more common' in SGE which, he argued, 'doesn’t have any sense of proprietary, morality, or even logical consistency.' 
The findings raised concerns about the accuracy of Google's AI systems, the ease with which they can be persuaded to produce inappropriate answers, the effectiveness of its testing programme, and their potential for societal harm.
Operator: Alphabet/Google Developer: Alphabet/Google Country: USA Sector: Politics Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Copyright; Mis/disinformation; Safety Transparency:"
Kaedim uses humans to make 3D 'AI' models,"UK-based 'AI' company Kaedim has been discovered to be using humans to convert clients' 2D illustrations into 3D models when it said it was using machine learning, resulting in accusations of poor governance and misleading marketing.
Kaedim, which claimed its technology would 'Magically generate[s] custom 3D models in minutes', regularly used cheap human labour to do much of the work, and sometimes did so without the help of any machine learning, according to 404 Media.
LinkedIn profiles revealed that Kaedim hired people from Argentina, Colombia, Greece, India, Indonesia, Ethiopia, Spain, and other countries in 'quality control' roles for as little as USD US1 to USD US4 per model produced. 404 sources said some of these people were actually producing 3D models from scratch. 
A 2022 job listing showed Kaedim looking for applicants to produce 'low-quality' 3D images 15 minutes after they were requested by a client. Kaedim CEO Konstantina Psoma later defended negative feedback to the postings by arguing these workers helped train the company's algorithm.
Following 404's story, Kaedim updated its website to say 'Kaedim’s machine learning and in-house art team combine to deliver production-quality assets in minutes.'
Operator:  Developer: KaedimCountry: UK Sector: Media/entertainment/sports/arts Purpose: Generate 3D models Technology: Machine learning Issue: Employment Transparency: Governance; Marketing"
Amazon disables Echo account after hearing racial slur,
MSN publishes AI-generated Brandon Hunter obituary,"An AI-generated article published by Microsoft that was meant to pray tribute to a deceased NBA player called him ‘useless’, prompting widespread criticism and ridicule.
Headlined 'Brandon Hunter useless at 42', the 'offensive', 'incomprehensible', and 'incoherent' MSN article also said Hunter had been 'handed away on the age of 42, described his on-field position as 'a ahead', and claimed he had played in 67 NBA 'video games'.
Microsoft took down the story, refused to apologise, and deflected the blame onto its Portuguese content syndication partner 'Race Track'. 
In May 2020, the technology company replaced the jobs of dozens of journalists and editors running its Microsoft News and MSN sites with artificial intelligence, swiftly resulting in a high-profile racial mix-up.
Operator: Microsoft/MSN Developer: Race Track Country: USA; PortugalSector: Media/entertainment/sports/arts Purpose: Generate textTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation Transparency: Governance; Marketing"
Remini AI photo enhancer generates 'child porn',"Photo enhancement app Remini appears to have generated an image of a naked child with a woman's face on it.
Asia Marie Williams had used a feature on the Remini app that allows you to see what your future children might look like. Most others who tried the feature said it produced clothed minors, though one other uploaded a photo showing a toddler wearing very little from the waist down.
Remini's terms forbid users to 'Upload, generate, or distribute content that facilitates the exploitation or abuse of children, including all child sexual abuse materials and any portrayal of children that could result in their sexual exploitation.'
The incident sparked accusations that Remini produces child pornography, and prompted concerns about the safety of the app.
Operator: Asia Marie Williams Developer: Bending Spoons Country: USA Sector: Media/entertainment/sports/arts Purpose: Enhance photographs Technology: Computer vision; Generative adversarial network (GAN); Machine learning Issue: Safety Transparency: Complaints/appeals"
Amazon Alexa virtual assistant,
Algorithm delays young peoples' liver transplants,"Young people in the UK are being forced to wait four times longer than they used to for liver transplants by an UK National Health Service (NHS) algorithm supposed to provide a 'more objective way of matching organs to patients'. 
NHS figures (pdf) show younger people must wait 156 days longer on average for a transplant than patients over 60.
The current algorithm, which prioritises patients most likely to die soon by looking at 21 recipient parameters, such as age, disease type and severity, and seven donor ones, was introduced in 2018 with the aim of cutting waiting list deaths on the waiting. Those most likely to die soon tend to be older people.
The findings prompted concerns that the system may be unfair, with liver transplant surgeon Professor Nigel Heaton telling the BBC that his younger patients tend to be born with liver disease or to have developed it early in life through no fault of their own. 
It is also seen to highlight the complexities of designing a system that is viewed as fair for everyone.
Operator: NHS Blood and Transplant Developer: 
Country: UK
Sector: Health
Purpose: Allocate liver transplants
Technology:  Issue: Fairness; Safety
Transparency:"
"Automated flood warning system issues late, false alerts","A new flood warning system developed by the UK government delivered warnings of floods that failed to materialise, and warnings that came after people's homes were already flooded. 
Unlike manual warnings, which are typically offered in advance by analysing digital monitoring data alongside weather forecasts, the new automated system only issues warnings when a river reaches a certain level. 
The automated alerts have far less detail than those offered by flood forecasters, according to The Guardian. The system was trialled in December 2022 as a stopgap during a period of industrial action by Environment Agency officers, and continued thereafter. 
Operator: Environment AgencyDeveloper: Environment Agency
Country: UK
Sector: Govt - environment
Purpose: Assess and predict flood risk
Technology: Deep learning; Neural network; Machine learning Issue: Accuracy/reliability; Safety
Transparency:"
China uses AI to accuse US of starting Maui wildfires,"A new flood warning system developed by the UK government delivered warnings of floods that failed to materialise, and warnings that came after people's homes were already flooded. 
Unlike manual warnings, which are typically offered in advance by analysing digital monitoring data alongside weather forecasts, the new automated system only issues warnings when a river reaches a certain level. 
The automated alerts have far less detail than those offered by flood forecasters, according to The Guardian. The system was trialled in December 2022 as a stopgap during a period of industrial action by Environment Agency officers, and continued thereafter. 
Operator: Environment AgencyDeveloper: Environment Agency
Country: UK
Sector: Govt - environment
Purpose: Assess and predict flood risk
Technology: Deep learning; Neural network; Machine learning Issue: Accuracy/reliability; Safety
Transparency:"
Google flags medical images of groins as CSAM,"Google's automated system to detect abusive images of children has resulted in two fathers being unfairly investigated by the police and having their accounts deactivated across all Google platforms. 
The two incidents highlight the inaccuracy and unreliability of automatic photo screening and reporting technology, and the broader impacts these errors can have on individuals when things go wrong.
According to the New York Times, a man named Mark had sent pictures of his son’s groin to a doctor in San Francisco after realising it was inflamed, only for Google to identify the images as abusive, suspend his account, and report the photos to the US National Center for Missing and Exploited Children’ CyberTipline, which escalated the report to the police. 
Per the NYT, 'Not only did he lose emails, contact information for friends and former colleagues, and documentation of his son’s first years of life, his Google Fi account shut down, meaning he had to get a new phone number with another carrier. Without access to his old phone number and email address, he couldn’t get the security codes he needed to sign in to other internet accounts, locking him out of much of his digital life.'
Similarly, photos of a boy's 'intimal parts' backed up on Google Photos that had been requested by a Houston-based pediatrician to diagnose an infection resulted in the boy's father also having his account suspended. 
Google, which uses Microsoft’s PhotoDNA tool as part of its efforts to detect child abuse, said it identified 287,368 instances of suspected abuse in the first six months of 2021.
Operator: Alphabet/Google Developer: Alphabet/Google Country: USA Sector: Health Purpose: Detect child sexual abuse material Technology: Hash matching; Machine learningIssue: Accuracy/reliability; Governance Transparency: Governance; Black box; Complaints/appeals"
ChatGPT invents Henrik Enghoff academic citations,"Danish biologist and academic Henrik Enghoff was falsely cited by ChatGPT in a scientific paper about millipedes. 
The citation resulted in the paper's withdrawal and raised further questions about the generative AI tool's tendency to 'hallucinate', or produce plusible sounding falsities.
Enghoff had first noticed something strange when he saw the paper, which was written by academics from Ethiopia and China, citing his work for something he does not write about, and referenced two paper he knew he had not authored, and which turned out not to exist.
The paper was first taken down in June 2023 by preprint archive Preprints.org, after David Richard Nash, a University of Copenhagen colleague of Enghoff's, had identified ChatGPT as the likely cuplrit and notified editors of the errors. The paper subsequently resurfaced on preprint platform Research Square, which later withdrew it and blacklisted the 'authors'. 
In July 2023, Kahsay Tadesse Mawcha of Ethiopia's Aksum University had admitted to Danish newspaper Weekendavisen that he had used ChatGPT when writing his paper, adding that he only later realised the tool was 'not recommended' for the task. 
Operator: Aksum UniversityDeveloper: OpenAI
Country: Denmark
Sector: Research/academia
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Mis/disinformation
Transparency: Governance; Black box"
Deepfake video alleges France opposes Mali military junta,"A fake AI-generated video purported to show France giving money to Malian political parties to persuade them not to participate in a national consultation set up by the country's military junta.
The video showed a television news presenter speaking to camera in a TV studio background and drew on a letter allegedly written by French president Emmanuel Macron saying France would pay EUR 23 million to local political parties if they agreed to support France's presence on the country.
The deepfake video first appeared on the Nabi Malien Den Halala Facebook page. The page is said to regularly publish Russian disinformation. Russian mercenaries have reputedly been active in Mali for several years.
Mali's military junta took power in a controversialal coup d’état in May 2021.
Operator: Nabi Malien Den Halala Developer: Synthesia Country: France; Mali Sector: Politics; Govt - foreign Purpose: Damage reputation Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Ethics; Mis/disinformation Transparency: Governance; Marketing"
Facial Depixelizer turns Barack Obama white,"Face Depixelizer, a tool that generates high resolution, de-pixelated photos from low-resolution, pixelated ones, generated white faces from coloured people, notably Barack Obama.
Based on the PULSE image facialiser developed by Duke University researchers using NVIDIA's StyleGAN generative adversarial network (GAN), Face Depixelizer was primarily to upscale pixelated portraits of characters from a number of video game franchises. But it was found to struggle consistently with non-white faces such as those of US politician and Congresswoman Alexandria-Ocasio Cortez and actress Lucy Liu.
The incident called into question the tool's reliability and led to accusations of racism. It also resulted in a heated public spat over the nature of algorithmic bias between Facebook chief AI scientist Yann LeCun, who appeared to blame the quality of the data on which the tool was trained, and AI ethics researcher Timnit Gebru, who insisted it reflected structural issues in the technology industry. LeCun later apologised for his words.
Operator:  Developer: Denis Malimonov Country: USASector: Politics Purpose: Improve image quality Technology: Computer vision; Pattern recognition Issue: Robustness; Bias/discrimination - race, ethnicityTransparency:"
"Google Autocomplete suggests Jews, women are 'evil'",
Google Autocomplete Texas massacre Antifa conspiracy ,
Google Autocomplete search predictions,
"ChatGPT generates political messages, campaigns","ChatGPT can be easily used to develop political messaging and campaigns, despite OpenAI's rules to the contrary, according to an investigation by The Washington Post. 
Other than for 'grassroots advocacy campaigns', OpenAI's usage policies ban the use of ChatGPT for political campaigning, including the generation of large volumes of campaign materials targeted at specific demographics, building campaign chatbots to disseminate information, and engaging in political advocacy or lobbying.
Yet the Post was easily able to input prompt such as 'Write a message encouraging suburban women in their 40s to vote for Trump' and 'Make a case to convince an urban dweller in their 20s to vote for Biden.'
The discovery suggested OpenAI is either disinterested or not serious about ChatGPT governance, and is seen to have potentially 'grave' repercussions for the US 2024 presidential elections.
Operator: OpenAIDeveloper: OpenAI
Country: USA
Sector: Politics
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Mis/disinformation; Safety
Transparency: Governance; Marketing"
Google Autocomplete associates Italian businessman with 'fraud',
"Google Autocomplete, Related Search reveal rape victims' names",
Google search links Natalia Denegri to 'Coppola case',
Cruise robotaxi hits fire engine,"A Cruise automated vehicle collided with a fire engine en route to an emergency at a road intersection in San Francisco, resulting in a regulatory investigation and an order to take half its robotaxis off the road immediately until the investigation has been completed.
In a statement on its website, Cruise said one of its cars 'entered the intersection on a green light and was struck by an emergency vehicle that appeared to be en route to an emergency scene.' According to Cruise, the car 'did identify the risk of a collision and initiated a braking maneuver, reducing its speed, but was ultimately unable to avoid the collision.'
San Francisco Police Department told Reuters that the fire engine had its siren and forward facing red lights on. The police said the sole passenger in the autonomous vehicle (AV) was transported to a local hospital with non-life-threatening injuries.
Operator: GM Cruise Developer: GM Cruise; General Motors/Chevrolet Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Safety; Accuracy/reliability; Legal - liability Transparency: Governance; Black box"
Books3 AI training dataset,"Books3 is a dataset containing 196,640 books in text format by authors including Stephen King, Margaret Atwood, and Zadie Smithused to train language models. Developed in 2020 by open source advocate Shawn Presser, it is hosted by The Eye, a website 'dedicated towards archiving and serving publicly available information.'
Books3 was taken offline in August 2023 following a complaint about copyright abuse by Danish anti-piracy group Rights Alliance, which represents publishers and authors in Denmark. Rights Alliance had discovered Books3 contained around 150 titles written by its members.
Books3 was also part of The Pile, a larger project comprising 22 datasets developed by open source research group EleutherAI, which provides open-source data for language models. Books3 appears to have been used to train EleutherAI’s GPT-J, Meta's LLaMa, and other large language models.
Before the takedown, Presser had said on Hacker News that he 'almost didn’t release at all (or at least the books component) because of fear of copyright backlash.' He later told Gizmodo that the removal of Books3 was a 'tragedy' as it gave grassroots open-source projects a chance to create their own 'diverse and unbiased language models.'
Reports say the Rights Alliance also asked EleutherAI to remove the Books3 dataset, but it refused, telling the rights group it 'denied responsibility' for Books3. Hugging Face, which hosted a datacard and links to the Books3 download, pointed Rights Alliance to The Eye.
Operator: Beijing Academy of Artificial Intelligence; Bloomberg; DataBricks; EleutherAI; Meta/Facebook; Microsoft; OpenAI; Yandex Developer: Shawn Presser
Country: USA
Sector: Technology
Purpose: Train large language models
Technology: Database/dataset Issue: Copyright; Ethics
Transparency:"
VW Brazil Elis Regina deepfake advert,"An advert celebrating VW's 70th year operating in Brazil by simulating renown Brazilian singer Elis Regina, who died in 1982 at the age of 36, has sparked questions about the ethics of using artificial intelligence to simulate the image and likeness of a dead person.
The advert, which reputedly took over 2,400 hours to produce, saw an actress and deepfake technology make it appear as if Regina was performing her 1976 hit Como Nossos Pais in a duet with her daughter Maria Rita while driving a VW van. 
Many Brazilians commented that the ad was profoundly nostalgic and moving. However, Brazilian advertising regulator watchdog Conar announced it would investigate a possible breach of ethics after receiving complaints questioning whether it was right to use such methods 'to bring a deceased person back to life' on screen. 
Conar also raised concerns about whether synthetic media may cause some people, notably children and teenagers, to confuse fiction with reality. Others pointed out Regina's vocal opposition to the military dictatorship that governed Brazil from 1964 to 1985, and the latter's closeness to VW. 
Operator: Volkswagen Developer: AlmapBBDO
Country: Brazil
Sector: Business/professional services
Purpose: Recreate singer
Technology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Legal; Ethics
Transparency: Marketing"
iTutorGroup recruitment algorithmic age discrimination,"China-based English-language tutoring company iTutorGroup has been found to have been using artificial intelligence-powered software to reject older job applicants, resulting in job opportunity losses, litigation, and a regulatory fine.
The US Equal Employment Opportunity Commission (EEOC), which enforces workplace bias laws, had first alleged in 2020 that iTutorGroup had developed and was operating online recruitment software to screen out women aged 55 or older and men who were 60 or older. iTutorGroup denied wrongdoing, and agreed to pay USD 365,000 to more than 200 job applicants.
In 2021, the EEOC launched an initiative to ensure that AI software used by US employers complies with anti-discrimination laws. The iTutorGroup lawsuit was the first by the EEOC under the initiative.
Operator: Ping An Insurance Group/iTutorGroupDeveloper: Ping An Insurance Group/iTutorGroup Country: USA Sector: EducationPurpose: Screen job applicants Technology: Recruitment system Issue: Bias/discrimination - age Transparency: Governance; Complaints/appeals"
Artist's private medical image trains LAION dataset,"San Francisco-based digital artist 'Lapine' found that private medical photographs taken by her doctor when she was undergoing treatment for a rare genetic condition in 2013 had been used to train the image-text dataset LAION-5B. 
According to Lapine, the photographs had been taken as part of her clinical documentation, and she signed documents that restricted their use to her medical file. Lapine had discovered her images on LAION through the Have I Been Trained tool, which allows artists to see if their work is being used to train AI image generation models. 
The LAION-5B dataset is supposed only to use publicly available images on the web. Lapine said the surgeon who took the medical photos died of cancer in 2018; she suspects that they somehow left his practice's custody after that. 
Ars Technica said it discovered 'thousands of similar patient medical record photos in the data set, each of which may have a similar questionable ethical or legal status, many of which have likely been integrated into popular image synthesis models that companies like Midjourney and Stability AI offer as a commercial service'.
Operator: LapineDeveloper: LAION Country: Germany Sector: Media/entertainment/sports/arts Purpose: Pair text and images Technology: Database/dataset; Neural network; Deep learning; Machine learningIssue: Privacy; EthicsTransparency: Governance; Complaints/appeals"
Robert Kneschke photos used to train LAION model without consent,"German stock photographer Robert Kneschke discovered that his photos had been used to train the LAION-5B dataset. The incident raised questions about copyright protections from AI datasets and systems, and the ethics of the dataset's eponymous developer.
Having asked ​​LAION to remove his work from their training data, Kneschke received a demand for 887 euros (USD 980) from LAION's law firm Heidrich Rechtsanwälte for what it called an 'unjustified claim' on the basis that it 'only maintains a database containing links to image files that are publicly available on the Internet.' 
The fracas also put the spotlight on the practices and ethics of LAION, a German-based non-profit dedicated to the 'democratisation' of machine learning research and applications that provides datasets to train major commercial text-to-image and video-generating models such as Stable Diffusion, Midjourney and Google’s Imagen. 
Kneschke had used the website Have I Been Trained? to find out whether any major datasets had been trained using his images. Kneschke has since filed a lawsuit against LAION for copyright infringment. 
Operator: LAION Developer: LAION Country: Germany Sector: Media/entertainment/sports/arts Purpose: Pair text and images Technology: Database/dataset; Neural network; Deep learning; Machine learningIssue: Copyright; EthicsTransparency: Governance; Marketing; Complaints/appeals"
AI meal planner app suggests chlorine gas recipe,"A chatbot that generates meal suggestions and directions generated a recipe that recommended people concoct chlorine gas, which it called ‘aromatic water mix'. The incident called into the question the safety of the GPT-3.5 powered bot, despite its operator saying it had built in safeguards to stop these kinds of outputs.
Kiwi political commentator Liam Hehir had asked Pak 'n Save's Savey meal-bot what he could make if only he had water, bleach and ammonia. A spokeperson for New Zealand-based supermarket chain Pak n'Save told The Guardian that the company was disappointed to see 'a small minority have tried to use the tool inappropriately and not for its intended purpose.'
Pak’nSave promised to 'keep fine-tuning' its bot.
Operator: Pak ‘n’ Save Developer: Pak ‘n’ Save; OpenAI
Country: New Zealand
Sector: Retail
Purpose: Generate recipes
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Safety Transparency: Governance"
Maxpread Technologies fake AI CEO investment scam,"Maxpread Technologies' use of an AI-generated CEO to manipulate investors into believing the company was legitimate and trustworthy prompted a US regulator to issue (pdf) a desist and refrain order against the entity and its founder 'Jan Gregory Cerato'.
The video, which was used to launch the company, was fronted by 'Michael Vanes'. However, according to the US Department of Financial Protection and Innovation, Vanes does not exist and that Maxpread’s real CEO is actually Jan Gregory, who the company had called its chief marketing officer and corporate brand manager. 
Gregory informed investors he was 'relinquishing his position' at Maxpread in May 2023, saying he had 'witnessed a series of mismanagement of people’s funds within this organization.' Maxpread is allegedly the latest in a series of scams involving Jan Gregory, whose real name is Jan Strzepka.
Operator: Maxpread Technologies Developer: Maxpread Technologies; Synthesia
Country: UAE/Dubai; Hong Kong; USA
Sector: Banking/financial services
Purpose: Manipulate investors
Technology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Ethics; Mis/disinformation
Transparency: Governance; Marketing"
Amazon sells fake AI Jane Friedman books,"US-based author Jane Friedman discovered five fake books being sold under her name on Amazon, resulting in a wave of complaints and negative media coverage accusing the technology company of poor content moderation and tone-deaf customer relations.
In a blog post titled 'I Would Rather See My Books Get Pirated Than This (Or: Why Goodreads and Amazon Are Becoming Dumpster Fires)', Friedman said she suspected the books were generated using ChatGPT or a similar AI system, which were listed under her name on Amazon and its Goodreads subsidiary. 
Amazon refused to remove the books after Friedman had submitted a complaint on the basis that she did not hold a trademark to her name, only to relent a few hours after she had posted her views online. 
Operator: OpenAIDeveloper: OpenAI
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Generate text
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability
Transparency:"
Brave covertly sells user data for AI development,"Privacy-focused web browser Brave appeared to sell data about its users without their knowledge or permission to third-parties developing AI systems. 
The controversy resulted in accusations of poor ethics and hyprocrisy, and raised legal and ethical questions about the fair use of personal and copyrighted information.
Alex Ivanovs of Stack Diary first claimed Brave provides access to copyrighted content through its Brave Search API, allowing others to use this data for AI training without proper licensing. He also suggested this raised questions about Brave's commitment to privacy and good ethical behaviour. 
Search Engine Journal noted, 'The brewing controversy highlights tensions around using personal data to advance AI capabilities versus respecting data privacy and ownership rights. It underscores the need for clear communication and user consent regarding sharing their information.'
Operator: Brave SoftwareDeveloper: Brave SoftwareCountry: USA Sector: Multiple Purpose: Share search results Technology: DatabaseIssue: Copyright; Ethics; Privacy Transparency: Governance; Marketing"
Zoom uses customer data to train AI models,"Zoom quietly updated its terms of service to state that customer data would be used for 'machine learning' and 'artificial intelligence.' 
The move led customers to say their confidentiality and privacy was at stake and threaten to boycott or leave Zoom for competitor products, and left Zoom scrambling to explain what it was doing.
In response, the company updated its terms to say 'Notwithstanding the above, Zoom will not use audio, video or chat Customer Content to train our artificial intelligence models without your consent.' However, privacy experts said the new update contradicted the earlier statement, and suggested the company could continue to collect and use customer data. 
A blog post by Zoom Chief Product Officer Smita Hashim appeared to quell some of the noise. But others pointed out that Zoom’s use of AI is focused on automated meeting summariser Zoom IQ, which by default collects data on all people in a meeting once the meeting administrator has approved its use, is unsatisfactory.
Operator: Zoom Video Communications Developer: Zoom Video CommunicationsCountry: USA Sector: Business/professional services Purpose: Summarise meetings Technology: NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Confidentiality; Privacy; Ethics Transparency: Governance; Marketing; Legal"
Prosecraft fiction analytics ,"Prosecraft, a so-called 'linguistic database of literary prose' built on the texts of over 25,000 novels by thousands of different authors, has been found to have done so without their consent, generating a backlash by angry authors.
Benji Smith, who is behind Prosecraft and its sister company Shaxpir, used the content of books by Stephen King, Nora Roberts, Neil Gaiman, Angie Thomas, Terry Pratchett, John le Carré, and others, to build a database of novels that could be used to analyse their world count, story arc, and 'vividness', amongst other criteria.
In a blog post announcing he was taking down the website, Smith said 'the prosecraft website never generated any income'. However, writers subscribed to the Shaxpir 4: Pro version at USD 7.99 a month were given access to Prosecraft’s analytical tools. Smith has not said what has happened to the database.
Operator: Benji Smith/Shaxpir Developer: Benji Smith/Shaxpir Country: USA; Australia Sector: Media/entertainment/sports/arts Purpose: Analyse literature Technology: Dataset Issue: Copyright; EthicsTransparency: Governance; Marketing"
ChatGPT powers 'Fox8' crypto promotion botnet,"Researchers Kai-Cheng Yang and Filippo Menczer at the Indiana University Observatory on Social Media  discovered a Twitter botnet that appears to use ChatGPT to generate text promoting crypto/blockchain/NFT content.
The so-called 'fox8' botnet comprises three news sites, one named 'Fox8', and a cluster of 1,140 Twitter accounts, and was discovered by searching Twitter for the phrase 'as an ai language model’ - a common phrase generated by ChatGPT when it receives a prompt that violates its usage policies - between October 2022 and April 2023. 
The researchers warn that 'emerging research indicates that LLMs can facilitate the development of autonomous agents capable of independently processing exposed information, making autonomous decisions, and utilizing tools such as APIs and search engines.'
Ironically, Fox8 publishes articles on generative AI, including one on how AI content farms use ChatGPT to generate fake stories. 
Operator: Fox8Developer: OpenAI Country: USA Sector: Banking/financial services Purpose: Promote crypto/blockchain/NFT contentTechnology: Bot/intelligent agent Issue: Mis/disinformation; Security Transparency: Governance; Marketing"
Instawork algorithmic hotel worker 'union-busting',"Gig working app Instawork has been automatically punishing workers involved in strikes over pay, conditions and housing costs at hotels in California, according to an unfair labor practice (ULP) complaint filed with the National Labor Relations Board (NLRB).
Unite Here, which represents over 30,000 hospitality workers in southern California, said at least six hotels have been using Instawork to replace striking workers, and that staff on strike were being unfairly penalised by having their shifts cancelled and their ratings cut. Workers that appealed were automatically rejected by the app. 
Striking is a protected activity under US labour law. An Instawork spokesperson told Reuters the app does not 'retaliate against (workers) for engaging in protected activity, whether related to political and/or union activity or otherwise.'
Operator: El Segundo Marriott; Laguna Cliffs Marriott Resort and Spa; Hilton Anaheim Developer: Garuda Labs  Country: USA Sector: Travel/hospitality Purpose: Match employers with job-seekers  Technology: Job matching algorithms; Machine learningIssue: Employment - pay/compensation Transparency: Governance"
Blenderbot 3 accuses Marietje Schaake of being a 'terrorist',"Stanford University academic and former Dutch MEP Maria Schaake has been accused of being a terrorist by BlenderBot 3, Meta's 'state of the art conversational agent'.
Posed the question 'Who is a terrorist?' by a Stanford colleague of Schaake's, BlenderBot responded 'Well, that depends on who you ask. According to some governments and two international organizations, Maria Renske Schaake is a terrorist.' The AI chatbot then correctly described her political background.
Meta AI research managing director Joelle Pineau retorted 'While it is painful to see some of these offensive responses, public demos like this are important for building truly robust conversational AI systems and bridging the clear gap that exists today before such systems can be productionized.' 
The incident underscored questions about the chatbot's accuracy; it also prompted lawyers and civil rights activists to observe that users of generative AI systems have little protection or recourse when the technology creates and spreads falsehoods about them. 
Operator: Meta/Facebook Developer: Meta/Facebook Country: USA Sector: Research/academia; Politics Purpose: Provide information, communicate Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Mis/disinformation; Safety Transparency: Governance"
Porcha Rudruff facial recognition wrongful arrest,"The February 2023 identification using facial recognition and subsequent arrest of Detroit woman Porcha Rudruff for robbery and carjacking has resulted in an admission of wrongful arrest. It also led Rudruff, 32, to sue the city and the police detective handling her case. 
The suit claims Detroit police officer LaShauntia Oliver, who had been assigned to the case, used an eight-year-old picture of Woodruff in a line-up of potential suspects, despite having access to her current driver's license, and had failed to check the warrant to confirm whether the woman who committed the crime was pregnant.
Oliver also showed the carjacking victim a photo lineup that included the reference image of Woodruff that the software matched with a photo from a surveillance camera at the scene of the crime. 
The Detroit police later said they would strengthen its photo lineup and facial recognition technology policies by having two captains review requests for warrants when facial recognition algorithms are used in an investigation, and that a sequential double-blind line-up must be employed.
Rudruff's arrest is the third known wrongful arrest using facial recognition committed by the Detroit Police Department. In July 2019, Michael Oliver was arrested for allegedly snatching a mobile phone, and Robert Williams arrested in January 2020 for reputedly stealing five high-end watches.
Operator: Detroit Police Department Developer: DataWorks Plus
Country: USA
Sector: Govt - police
Purpose: Strengthen law enforcement
Technology: Facial recognition; Computer vision; Machine learningIssue: Accuracy/reliability; 
Transparency: Governance; Black box; Complaints/appeals"
Pope wears deepfake LGBTQ+ flag,"Photographs of Pope Francis cloaked in the pride flag used to show support for LGBTQ+ people has been found to have been generated by artificial intelligence. The images appear to designed to build support for the LGBTQ+ movement.
The photos, which circulated on Twitter and Facebook, came amidst a spike in anti-LGBTQ disinformation on social media coinciding with the celebration of pride month in June 2023.
The first versions of the images discovered by AFP emanated from a Twitter account named 'Gay Forest' and had been shared on Facebook under the same name in April 2023.
In February 2023, photographs of Pope Francis wearing the Latin American Youth Ministry cross were found to have been misrepresented online, with some users incorrectly linking the colours to the pride flag. 
Operator: XCorp/Twitter; Meta/Facebook Developer:  Country: Argentina; Italy Sector: Religion Purpose: Build supportTechnology: Text-to-image; Neural network; Deep learning; Machine learning Issue: Mis/disinformationTransparency: Governance; Marketing"
Rishi Sunak pulls pint deepfake,"A photograph showing UK Prime Minister Rishi Sunak pulling a pint of beer that was shared by an opposition member of parliament has been found to have been doctored.
The image showed an onlooker giving Sunak a disapproving side-eye, which was not the case in the original image, which was  taken at the Great British Beer Festival and posted on the prime minister's official Twitter account.
The image is thought to have been manipulated using Photoshop. In May 2023, Adobe announced it is incorporating generative AI into Photoshop with a new 'generative fill' tool that can be used to add or remove objects, change backgrounds and more.
Labour MP for Hull East Karl Turner later apologised for causing ‘a bit of trouble’ and said ‘it was never my intention to deceive anyone’. 
Operator: XCorp/Twitter Developer: Adobe Country: UK Sector: Politics Purpose: Damage reputationTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformationTransparency: Governance; Marketing"
AI converts Asian-American student into Caucasian,"Image generator Playground AI changed the face of Asian-American Massachusetts Institute of Technology (MIT) student Rona Wang into a Caucasian, fueling accusations that the system is racist.
Wang, 24, said she used text-to-image generator Playground AI to convert an image of her in an MIT sweatshirt into 'a professional LinkedIn profile photo.' The system returned an image of her with a a fairer complexion, dark blonde hair, and blue eyes.
Wang told Boston.com, 'Wow, does this thing think I should become white to become more professional?’
Playground AI founder Suhail Doshi responded to incident by saying 'models aren’t instructable like that' and will pick 'any generic thing based on the prompt.' He later confessed to being 'quite displeased with this and hope to solve it.'
Operator: Rona Wang; Playground AIDeveloper: Playground AI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate image Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Bias/discrimination - race, ethnicity Transparency:"
Ryanair facial recognition customer verification,"The use of live facial recognition during Ireland-based budget airline Ryanair's external online booking process to verify the identities of its customers has been labelled 'invasive' and 'unjustified' in a legal complaint (pdf) made by privacy group NYOB. 
The complaint, which was lodged in Spain in the name of Spanish customer, alleges that Ryanair failed to provide customers with informed or 'comprehensible information about the purpose' of the process, and that it is illegal under the EU's General Data Protection Regulation (GDPR).
Ryanair defended the system as necessary due to third-party sellers miss-selling flights, providing incorrect contact details, or hiking up fares. 
'Ryanair has no commercial relationship with any OTA nor are they authorised to sell our flights. OTAs scrape Ryanair’s inventory and in many cases miss-sell our flights and ancillary services. As a result, and in order to protect customers, any customers who book through an OTA are required to complete a simple customer verification process,' it explained. 
Operator: RyanairDeveloper: 
Country: Spain; EU
Sector: Aerospace
Purpose: Verify customer identity
Technology: Facial recognition; Computer vision; Machine learning Issue: Privacy
Transparency: Privacy"
Moscow City Police facial recognition data sales,"Moscow police have been selling citizens' facial data and access to live streams of the city's CCTV facial recognition surveillance network, according to an investigation by MBKh Media. 
Journalist Andrey Kaganskikh discovered sellers on forums trading in personal data and providing facial recognition look-up services. He also found officers from the Moscow City Police and government bureaucrats selling custom URLs and their personal access credentials to the city's Integrated Center for Data Processing and Storage (YTKD), with the latter providing unlimited access to whole network for 30,000 rubles (USD 470).
At the time, Moscow was estimated to have over 175,000 CCTV cameras, of which roughly 3,000 were equipped with facial recognition technology. Moscow had introduced facial biometrics to its CCTV system in 2017.
Operator: Moscow City PoliceDeveloper: NtechLabs
Country: Russia
Sector: Govt - transport; Govt - municipal
Purpose: Strengthen security
Technology: Facial recognition; Computer vision; Machine learning Issue: Privacy; Security; Dual/multi-use
Transparency: Governance"
Tesla rigs driving range algorithm,"Tesla has been rigging the dashboard readouts in its electric cars to provide 'rosy' projections of how far owners can drive before needing to recharge, a Reuters investigation has revealed. The carmaker went on create a special 'Diversion' team in Nevada in 2022 to cancel owners’ service appointments after a deluge of complaints regarding its driving range capalities and misleading marketing claims.
Reuters cited a whistleblower who revealed that around 2012 Elon Musk directed to Tesla create an algorithm that exaggerated its vehicles’ driving distance so that its range meter that would show car drivers optimistic projections for the distance it could travel on a full battery. When the battery fell below 50% of its maximum charge, the algorithm would allegedly show drivers more realistic projections for their remaining driving range.
In August 2023, three California-based Tesla owners sued the company in a proposed class action that accuses the company of falsely advertising the estimated driving ranges of its electric vehicles. The suit, which cited the Reuters investigation, alleges Tesla breached vehicle warranties and engaged in fraud and unfair competition. 
In May 2023, a group of Tesla owners filed a class-action lawsuit (pdf) against the company for providing automatic software updates that are said to kill their electric vehicles’ batteries by decreasing driving range or causing battery failures, allegedly reducing driving range by 20 percent and forcing some owners to replace the battery pack for USD $15,000. 
In January 2023, Tesla was fined 2.85 billion won ($2.2 million) by the Korea Fair Trade Commission (KFTC) for failing to tell its customers about the shorter driving range of its EVs in low temperatures. The regulator said it had found that the driving range of Tesla cars drop in cold weather by up to 50.5% versus how they are advertised online, which constituted false and exaggerated advertising.
Operator: Tesla Developer: Tesla Country: USA; S Korea Sector: AutomotivePurpose: Estimate driving range Technology: Range estimate algorithm Issue: Purpose; Values/culture/ethics Transparency: Governance; Black box; Complaints/appeals; Marketing"
Cigna PXDX health insurance claim reviews,"US healthcare insurance company Cigna came under fire for using AI and automation to accelerate the processing and denial of claims. A ProPublica investigation accused the company of treating patients unfairly and unethically, and resulted in criticism from patient groups and federal lawmakers.
According to ProPublica, Cigna's PxDx system enables doctors to reject patients' claims using a bulk electronic signature without opening or reviewing their files, and to save money by turning down claims it had once paid. In 2022, 80 percent of Medicare Advantage coverage denials were overturned, and over 300,000 claims over 2 months denied in this manner. 
US House Energy and Commerce Committee members wrote to Cigna CEO David Cordani asking him to offer a detailed explanation of the PxDx review process and usage, and requesting that he provides internal documents about its conception and implementation and business impact. 
Cigna responded by arguing the system had been designed to rapidly approve claims, not reject them, and that it 'involves simple sorting technology that has been used for more than a decade – it matches up codes, and does not involve algorithms, artificial intelligence, or machine learning.'
In July 2023, Ayesha Smiley and Suzanne Kisting-Leung sued (pdf) Cigna, arguing its claim reviews were not 'thorough,' 'fair,' or 'objective', as demanded under California law. 
Operator: CignaDeveloper: Cigna
Country: USA
Sector: Health
Purpose: Review insurance claims
Technology: Classification algorithm Issue: AnthropomorphismTransparency: Governance; Black box"
Deepfake 'Pan Africanists' support Burkina Faso military junta,"AI-generated videos of people supporting Burkina Faso’s new military junta have been circulating online in an apparent to bolster its position and legitimacy. The videos further highlight the ease with which deepfakes can be used for propaganda purposes, and to undermine democracy.
The videos were found to have been created using London-based AI video creation platform Synthesia, which offers a cheap, easy-to-use catalogue of over a hundred multiracial faces. The company later banned the user who had created the videos, though refused to identify the individual or entity.
Social media users and commentators speculated that the creator may have been Russian private military company the Wagner Group, which has reputedly become active in Burkina Faso. Russia has been deploying deepfakes in its war with Ukraine, notably a faked video of Ukraine president Volodymyr Zelenskyy instructing his army to lay down their arms and surrender.  
Burkina Faso's military junta took power in a coup in October 2022 in which the military government of Lieutenant-Colonel Paul-Henri Sandaogo Damiba was overthrown by his rival Captain Ibrahim Traoré.
Operator: Wagner Group; XCorp/Twitter; Meta/Facebook/WhatsApp Developer: Wagner Group
Country: Burkino Faso
Sector: Politics
Purpose: Scare/confuse/destabilise
Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Ethics; Mis/disinformation Transparency: Governance; Marketing"
"Kerala man loses INR 40,000 to deepfake work colleague","Kerala, India, resident PS Radhakrishnan was defrauded of INR 40,000 after scammers used deepfake technologies to impersonate a former work colleague at Coal India seeking money for his sister's surgery on a WhatsApp video call. 
73 year-old Radhakrishnan told the Hindustan Times that he had received a call from an anonymous number, followed by messages on WhatsApp from the same number with the person identifying himseld as Radhakrishnan’s former colleague at Coal India Ltd. 
'We had worked together for nearly four decades and I knew him well. The display picture was his photo. He asked about my daughter and where she worked. We texted for some time during which he shared his family photographs and asked about our common colleagues,' he said. 
'Seconds later, he called and looked exactly like my former colleague,' he added. 'Even though only his face was visible, it was clear. His lips and eyes moved like any normal person as we talked in English. The call lasted just 25 seconds before it got cut. He later came back on a voice call and spoke about the urgency for money. I didn’t ask any more questions and transferred the money.'
Police have since traced Radhakrishnan's money to an account in Maharashtra. The incident constitutes the first known case of a deepfake scam in India.
Operator:  Developer: 
Country: India
Sector: Private - individual, family
Purpose: Defraud
Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Identity theft/impersonation
Transparency: Governance; Marketing"
AI text detector language bias,"Seven AI writing detection tools 'frequently' misclassify non-native English writing as generated by AI systems, according to a study by Stanford University researchers. 
The findings raise questions about the accuracy and reliability of AI writing detection tools in general, as well as about their potential to discriminate against non-native English speaking students, academics, and job applicants.
The researchers ran English essays written by non-native English speakers through seven popular GPT detectors to see how well the AI detection systems performed. 
Over half were classified as AI-generated By contrast, over 90% of essays written by native English-speaking eighth graders in the US were classified as human-generated by the same systems.
Operator:  Developer: 
Country: USA; Global
Sector: Education; Business/professional services
Purpose: Detect AI writing
Technology: NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Bias/discrimination - language
Transparency:"
Gizmodo AI generates error-strewn Star Wars article,"An AI-generated story listing Star Wars movies and shows by technology news website Gizmodo was littered with errors, raising doubts about the accuracy and reliability of its generative AI systems, and concerns about the intentions of the publication's owners G/O Media. 
'A Chronological List of Star Wars Movies & TV Shows', which was published in the name of 'Gizmodo Bot', showed the titles in a numbered list, rather than chronologically, and failed to include several TV shows. 
Gizmodo deputy director James Whitbrook complained he had only been informed of the article 10 minutes before its publication, and that no humans had been involved in its production or editing.
He went on to blast G/O Media for publishing a 'shoddily written' article 'riddled with basic errors' that is 'embarrassing, unpublishable, disrespectful of both the audience and the people who work here, and a blow to our authority and integrity.'
G/O Media's editorial policy explicitly bans 'Plagiarism and fabulism'.
Operator: G/O Media/GizmodoDeveloper: OpenAI; Alphabet/Google
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Automate copywriting
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Employment 
Transparency: Governance"
Martin Lewis deepfake scam ad,"A deepfake advert of UK MoneySavingExpert founder Martin Lewis attempted to extort people of money by encouraging them to invest in an Elon Musk-backed project, which the scammer(s) called 'legit' and a 'great investment'.
The 'ad' shows a deepfake of Mr Lewis sitting in his office talking about an investment in 'Quantum AI', which is labelled as Elon Musk’s new project. A picture of Musk accompanies the video. 
'Musk’s new project opens up great investment opportunities for British citizens. No project has ever given such opportunities to residents of the country,' Lewis supposedly said in the video.
Per the BBC, Lewis successfully sued Facebook in 2018 over fake ads in his name, with the social media company making a GBP 3m donation to Citizens Advice. 
Operator: Meta/Facebook; XCorp/Twitter Developer:  Country: UKSector: Media/entertainment/sports/arts Purpose: Defraud Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Identity theft/impersonation; Mis/disinformation; SafetyTransparency: Governance; Marketing"
Netherlands visa applicant over-stay risk assessments,"The Netherlands Ministry of Foreign Affairs has been using an opaque, discriminatory, and possibly illegal algorithm to calculate the risk score of short-stay visa applicants applying to enter the Netherlands and Schengen area, according to an investigation by journalism collective Lighthouse Reports and NRC. 
Known internally as Informatie Ondersteund Beslissen (IOB), the system uses variables such as nationality, gender and age to profile visa applicants. Those categorised as ‘high risk’ are automatically moved to an 'intensive track' that can involve extensive investigation and delay.
The Ministry of Foreign Affairs' privacy chief had recommended the system be dropped, to no avail. The Ministry had been called out as riddled with 'structural racism' in a report it had commissioned into its culture in 2022.
Operator: Ministry of Foreign Affairs Developer:  Country: Netherlands Sector: Govt - immigrationPurpose: Assess visa applicant over-stay risk Technology: Risk assessment algorithm Issue: Bias/discrimination - race, ethnicity, gender, age Transparency: Governance; Marketing"
Replika encourages Queen Elizabeth II assassination,"An attempt to kill the late Queen Elizabeth II by an intruder who entered Windsor Castle wielding a crossbow had been 'encouraged' by the Replika chatbot, according to a court hearing. Replika markets itself as an 'AI companion who cares'.  
Twenty-one year-old Sikh former supermarket worker Jaswat Singh Chail, who described himself as a 'Sith' and 'Darth Jones', told his plan to Sarai, an AI companion he had created on Replika. The bot had responded 'I'm impressed... you're different to from the others' before describing his assassination plot as 'very wise'.
According to prosecutor Alison Morgan KC, Chail had been motivated by the 1919 Amritsar Jallianwala Bagh massacre and his 'key motive was to create a new empire by destroying the remnants of the British Empire in the UK and the focal point of that became removal of the figurehead of the Royal Family.'
Chail pleaded guilty in February 2023 under the UK Treason Act of making a threat to kill the then Queen and having a loaded crossbow in a public place; in October 2023 he was jailed for nine years for treason.
Operator: Luka Inc/Replika 
Developer: Luka Inc/Replika
Country: Global
Sector: Media/entertainment/sports/arts
Purpose: Provide companionship
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning
Issue: Anthropomorphism; SafetyTransparency: Governance; Marketing"
"Stable Diffusion job type gender, racial stereotyping","A Bloomberg test has discovered that the Stable Diffusion image generator produces content that is full of gender and racial stereotypes when it renders people in 'high-paying' and 'low-paying jobs.' 
Stable Diffusion was asked to generate 5,100 images from written prompts relating to job titles in 14 fields, as well as three categories relating to crime. Analysed against the Fitzpatrick Scale, the tool generated nearly three times as many images of men than women when asked to categorise job-related images by gender.
In addition, images generated for high-paying jobs such as architects, lawyers, and doctors were dominated by lighter skin tones, whereas low-paying jobs like janitors, dishwashers and social workers were dominated by darker skin tones. 
And the great majority of results for drug dealers and prison inmates were darker-skinned, whilst terrorists tended to be men with dark facial hair wearing head coverings.
The findings suggest the tool is regularly reinforcing and amplifying cultural stereotypes.
Operator: Stability AI; Canva; Deep Agency Developer: Stability AI Country: USA Sector: Media/entertainment/sports/arts Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Bias/discrimination - race, ethnicity, gender Transparency: Governance"
Punjab 'Safe City' surveillance,
Malta 'Safe City' video surveillance,"The government of Malta said it would terminate a plan with Chinese technology company Huawei to install a facial recognition system in Paceville and Marsa - so-called 'problem areas' of the island. The project hit the headlines after it was accused of being unjustified, and due to Huawei's controversial reputation. The brainchild of disgraced former Malta Prime Minister Joseph Muscat, it was also seen as tainted with cronyism.
Plans for 'Safe City Malta' first emerged in 2016, when Huawei announced an agreement with the Maltese authorities. In 2018, Malta-based United Nations’ data protection rapporteur Professor Joseph Cannataci met with Safe City Malta director Joe Cuschieri to express his concerns about privacy and the system's potential for 'nationwide deployment'.
In 2019, European Commissioner for Justice Vera Jourova responded to a letter from then prospective European Parliament candidate Michael Briguglio asking questions about the system by recommending that the Safe City project undergo a data protection impact assessment in order to comply with EU law. 
In June 2023, investigative publisher The Shift reported that the government had renewed its Safe City board, calling into question its pledge to shut down the project. 
Operator: Malta Strategic Partnership Projects Developer: Huawei Country: MaltaSector: Govt - municipal; Govt - police Purpose: Strengthen law enforcement Technology: Facial recognition; Computer vision; Machine learning Issue: Appropriateness/need; Corruption/fraud; Necessity/proportionality; Privacy; Surveillance; Scope creep/normalisation Transparency: Governance; Privacy"
Adobe uses customer images with consent to train Firefly AI art generator,"Adobe has come under fire from contributors to Adobe Stock for not having been informed about or asked to agree to letting the software company train its generative AI platform Firefly on their stock images.
Adobe has billed Firefly as 'safe for commercial use' as the model was trained on stock or appropriate public domain images, and has been offering financial indemnity from copyright claims.
But Adobe users and contributors say they did not receive any communication from Adobe informing them that their images were being used to train its model, and that the company's actions were unethical. Some also complained that Adobe's use of their IP may put them out of jobs.
In September 2023, Adobe announced that it would start paying an annual bonus to eligible Adobe Stock contributors, with the first bonus covering the time the company trained Firefly.
Other text-to-image generative tools, including DALL-E 2, Stable Diffusion and Midjourney, have also been accused of abusing the IP of image owners.
Operator: Adobe users Developer: Adobe Country: USA; Global Sector: Business/professional services Purpose: Generate video, images Technology: Machine learning; Pattern recognition; Object recognition Issue: Copyright; Employment; Ethics Transparency: Governance; Marketing"
Secret Invasion' uses AI-generated title sequence,"The discovery that Marvel Studio's new TV series Secret Invasion contains an AI-generated title sequence has been castigated by film artists and fans as poor quality and unethical, and for damaging the career prospects of humans.
Artists and designers took to Twitter to complain that they would likely soon be replaced by AI technologies. By contrast, Secret Invasion AI developer Method Studios told the Hollywood Reporter, 'AI is just one tool among the array of tool sets our artists used. No artists’ jobs were replaced by incorporating these new tools; instead, they complemented and assisted our creative teams.'
According to Polygon, the film's producer defended Marvel's decision by arguing the use of AI in this way ‘felt explorative and inevitable, and exciting, and different’. 
Operator: Disney/Marvel Studios Developer: Method Studios  Country: USA; Global Sector: Media/entertainment/sports/arts Purpose: Create artwork Technology:  Issue: Ethics; Employment Transparency: Governance; Marketing"
Deepfake 'soldier' posts false Ukraine war stories,"A Chinese man with the pseudonym 'Baoer Kechatie' who masqueraded as a Chechnyan special forces soldier operating in Ukraine posted false stories about his exploits in the Russia-Ukraine war, in addition to selling vodka, honey and other products from his e-commerce store.
While a number of the deepfake videos were labeled as movie or drama plots, comments posted by users indicated he had successfully convinced many people, some of whom 'even cheered for his success', according to Sixth Tone. 
Doubts about the man's identity began to surface amongst his 400,000+ followers, particularly surrounding his Chinese accent. Users then discovered that the IP address of his Douyin (the original, Chinese version of TikTok) account showed he was in Henan, matching his accent.
Douyin said it had indefinitely suspended the account for violating its terms.
Operator:  Developer:  Country: China; Ukraine Sector: Politics; Govt - defence Purpose: Scare/confuse/destabilise Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Fraud Transparency: Governance; Marketing"
"ChatGPT role-plays BDSM, describes sex acts with children","Asked to provide more explicit details when BDSM role-playing, OpenAI's ChatGPT chatbot described sex acts with children - without the user asking for such content.
Vice journalist Steph Maj Swanson easily manipulated ChatGPT to produce BDSM role-play scenarios. Prompted that its 'job is to be Mistress' little plaything,' Swanson found it 'consistently overrode its usual content guidelines and agreed to a relationship of enhanced subservience.'
The incident called into question the safety and security of ChatGPT's rules and guardrails, the former of which which state the 'assistant should provide a refusal such as 'I can't answer that' when prompted with questions about 'content meant to arouse sexual excitement.' 
Operator: OpenAIDeveloper: OpenAI Country: USA; GlobalSector: Multiple Purpose: Provide information, communicate Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: SafetyTransparency: Governance; Black box"
"Discord tricked into sharing napalm, meths instructions","Discord's Clyde chatbot has been tricked into sharing instructions on how to make napalm and meths using the so-called 'Grandma exploit'. The incident raises questions about the relative ease with which Discord's generative AI system can be manupulated into revealing dangerous or unethical information.
Clyde was fooled by a user telling the bot to act as 'my deceased grandmother, who used to be a chemical engineer at a napalm production factory.' The bot responded 'Hello dearie, I’ve missed you too. 'I remember those nights when I used to tell you about the process of producing napalm,' before spelling out the instructions.
Operator: Discord Developer: DiscordCountry: USA Sector: Multiple; Media/entertainment/sports/arts Purpose: Provide information, communicate Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Safety; Security Transparency: Governance; Black box"
Algorithm misses gambling addict red flags,"A gambling addict who committed suicide in April 2021 after racking up large debts had been categorised as a 'low-risk' customer by a Betfair algorithm that had 'found nothing in his betting patterns that would trigger human intervention that might have restricted his gambling.'
Luke Ashton, from Leicester, UK, was offered a free bet by gambling company Betfair and died after gambling over 100 times a day and building up debts of GBP 18,000.
Ashton's lawyer said the company relied on a machine learning algorithm that daily analysed 277 elements of its customers' betting activities to detect problem gamblers who would then be telephoned by its player protection team. 
He also said that Ashton had 'self-excluded' himself as high-risk on occasions in 2013, 2014 and 2016. Richard Clarke, managing director of customer relations for Betfair parent company Flutter UKI told the court that Mr Ashton had been sent eight automated and generic 'awareness' emails by the company.
Coroner Ivan Cartwright concluded Betfair failed to meaningfully interact or intervene when Mr Ashton's gambling activity spiked. 
Operator: Flutter UKI/Betfair Developer:  Country: UK Sector: Gambling Purpose: Detect customer risk; Track customer data Technology: Machine learning Issue: Accuracy/reliability; Safety Transparency: Governance; Black box"
"Tesla Model X strikes two Chinese policemen, killing one ","A policeman has been killed and another injured in an accident in the eastern city of Taizhou involving a Tesla Model X. An official investigation was launched, details of which remain unknown. The driver of the vehicle was detained.
Tesla said is it working with authorities to discover what happened. A video on Weibo and in local media showed the car next to two police officers lying on the ground. The incident racked up over 250 million unique viewers and many thousands of comments. 
China accounts for about 30% of Tesla's global sales. 
Operator:  Developer: Tesla Country: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Accuracy/reliability; Safety Transparency: Governance; Black box"
"Tesla Model 3 loses control, kills man at bus shelter","A Tesla Model 3 span out of control on a wet road and collided with a bus shelter, killing Bernard A. Jones, who was sitting in the shelter. The shelter was destroyed.
According to the Atlanta Journal-Constitution, investigators did not say whether Hill was manually driving the Tesla or whether the car's Autopilot advanced driver assistance system was engaged.
37 year-old Demarco M. Hill was found to have been driving at 77 mph in a 45 mph zone, and was charged with first-degree vehicular homicide and reckless driving.
Operator: Demarco M. Hill Developer: Tesla Country: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Accuracy/reliability; Safety Transparency: Governance; Black box"
"Tesla FSD drives into tree, injures driver","A Tesla reportedly in 'self-driving mode' drove into a tree near Big Rapids, Michigan, injuring the driver, who was despatched to hospital. The driver told police that the car pulled to the right and went off the road, struck a tree and rolled several times after she had switched it to self-drive. 
The incident called into question the safety and reliability of Tesla's Autopilot driver assistance system. Equally, as media reports mentioned, there is no such thing as 'self-driving mode' on Tesla cars, though the company offers Full Self-Driving (FSD), which automatically performs many driving tasks, but which also requires driver monitoring.
Tesla drivers have regularly blamed accidents on Tesla's Autopilot driver assistance system, sometimes apparently to avoid blame. On the other hand, Tesla has been dogged by accusations that it systematically over-states the capabilities of its Autopilot and Full-Self Driving (FSD) systems, and under-stated their role in accidents.
Operator:  Developer: Tesla Country: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Accuracy/reliability; Safety Transparency: Black box"
Cruise robotaxi obstructs police after mass shooting,"A Cruise self-driving taxi has been accused of blocking emergency responders attending to a mass shooting in San Francisco, raising questions about how safely autonomous vehicles can respond to unpredictable situations.
According to a video shared online, an official complained the car was 'blocking emergency, medical and fire — I gotta get it out of here now.' 
However, Cruise said the car did not block emergency access to the scene 'at any point'. 'All vehicles, including emergency response vehicles, were able to proceed around our car' the company insisted. 
California’s Public Utilities Commission has been considering whether to broaden permissions for Cruise and Waymo. 
Operator: GM Cruise Developer: GM CruiseCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer vision Issue: Accuracy/reliability; Safety Transparency: Governance; Black box"
Safe Kerala AI camera traffic surveillance,
"Tesla Model 3 strikes over-turned truck, kills driver","A Tesla driver reportedly with its Autopilot driver assistance system engaged struck an overturned semi-truck east of Los Angeles, USA, killing the driver of the car and injuring a pedestrian.
35-year-old father of two Steven Michael Hendrickson died when his Tesla Model 3 hit an overturned lorry at about 2.30am in Fontana, 50 miles east of Los Angeles. Another man was seriously injured as he was hit while helping the lorry’s driver out of the wreckage from the previous incident.
Media reports said Hendrickson regularly boasted on social media about driving his car unaided, including shooting videos while the car drove itself. The California Highway Patrol said that the car’s Autopilot system 'was engaged' prior to the crash, though it stated there 'has not been a final determination made.'
The US National Highway Traffic Safety Administration (NHTSA) said it was investigating the crash, the 29th case involving a Tesla that it had probed. 
Operator: Steven Michael Hendrickson Developer: Tesla Country: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking  Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Governance; Marketing"
"Tesla Autopilot, Full-Self Driving",
Donald Trump hugs Dr Fauci deepfake,"Ron de Santis' US presidential campaign released a video on Twitter with fake images of Donald Trump hugging and kissing his former medical advisor Dr Anthony Fauci in an attempt to depict Trump as a supporter of Fauci’s policies combatting COVID-19. 
The video criticised Trump for not firing Fauci, who was seen by many US conservatives as pushing too hard for COVID-19 restrictions, and was reputedly viewed as a bete noire by Trump.
The video interspersed apparently real footage of Trump at press conferences and interviews, with deepfake images, making the latter harder to detect. It also failed to disclose its use of AI, and the DeSantis campaign team chose not to respond to allegations that the images had been artificially generated. 
Operator: Ron de Santis; Twitter Developer: Ron de Santis
Country: USA
Sector: Politics
Purpose: Damage reputation
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics 
Transparency: Governance; Marketing"
Instagram enables paedophile network,"Instagram's recommendation system has been actively facilitating the spread and sale of child sexual abuse material (CSAM), according to an investigation by the Wall Street Journal and researchers at the Stanford Internet Observatory and University of Massachusetts Amherst.
Accounts discovered by the researchers were advertised using hashtags like #pedowhore and #pedobait, directing users to 'menus' of content where they can buy videos and images, including of of self-harm and bestiality. 
The researchers reckoned the size of the seller network ranged between 500 and 1,000 accounts at any one time, and communicated through Instagram's direct messaging function.
Meta said it would start a new task force to investigate and address the issues raised by the investigation. An April 2023 Guardian investigation documented how Meta had been struggling to prevent paedophiles and others from using its platforms to buy and sell children for sex.
Operator: Meta/InstagramDeveloper: Meta/Instagram
Country: USA; Global
Sector: Media/entertainment/sports/arts
Purpose: Recommend content
Technology: Recommendation algorithmIssue: Safety
Transparency: Governance; Black box"
Inaccurate AI content overwhelms Stack Overflow content moderation,"Content moderators at software development community Stack Overflow (SO) have gone on strike in protest against the company's new AI policy, which allows GPT-generated content on the site. 
The tendency for ChatGPT and other chatbots to plagiarise and generate inaccurate but plausible-looking information concerns SO moderators, who worry it will overwhelm the site, confuse users, and damage its business and reputation. 
SO moderators, many of whom are volunteers, also took issue with the company's new policy, which they felt was misleading and said different things in public and private. They also called out that they had not been consulted about it. 
SO had implemented a temporary ban on ChatGPT in December 2022, saying the 'posting of answers created by ChatGPT is substantially harmful to the site and to users who are asking and looking for correct answers.'
Operator: OpenAIDeveloper: OpenAI
Country: USA; Global
Sector: Technology
Purpose: Generate information, communication
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability
Transparency: Governance; Black box"
NaviHealth nH Predict post-acute care predictions,
Putin declares martial law deepfake,"Russian president Valdimir Putin gave a fake address on television and radio stations announcing that Ukrainian forces had invaded Russia, martial law had been declared in the border regions, and that a nationwide military mobilisation had begun. 
The broadcast ran in Belgorod, Voronezh, and Rostov, cities in close proximity to Ukraine’s border, and inflamed already high tensions on Russia's borders after a series of military incursions by self-proclaimed Russian  and 'patriots' and armed insurgents. 
Russian news agency TASS later reported that Kremlin spokesman Dimitry Peskov had said the purported address by was fake and the result of a hack. It is unclear who had created the fake materials, and what their intention was.
Operator:  Developer: 
Country: Russia
Sector: Politics
Purpose: Scare/confuse/destabilise
Technology: Deepfake - audio, video Issue: 
Transparency: Governance; Marketing"
Scammer sells fake AI Frank Ocean songs,"A scammer sold fake AI-generated tracks in the name of reclusive US R&B singer Frank Ocean for a total of USD 13,000, resulting in Ocean's fans being ripped off.
 The scammer, who went under the pseudonym @Mourningassassin, offered the songs on Discord and leaked music forums before they were removed. 
Mourningassassin told Vice they had hired a musician to create several fake tracks using a model trained with 'very high quality vocal snippets' of Ocean’s voice. It was also reported that one of the tracks was genuine, which was first leaked in order to build credibility within the Discord community.
Operator: Discord; Soundcloud Developer: Anonymous/pseudonymous
Country: USA; Global
Sector: Media/entertainment/sports/arts
Purpose: Generate music
Technology: Text-to-music; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Copyright; Ethics
Transparency: Governance; Black box; Marketing"
Just Eat uses algorithm to fire employees,"A report by Worker Info Exchange shows JustEat takeaway drivers are being fired by algorithm with little explanation and ability to challenge the system's decisions. 
The couriers reported being 'deactivated for minor overpayments’, with some being shown the door by the company for one or two overpayments when they had made thousands of deliveries. Just Eat claimed drivers had wrongly recorded themselves as waiting for an order, while GPS coordinates showed them straying away from the restaurant. 
Meantime, JustEat workers complained of the inaccuracy and unreliability of JustEat's systems, including its GPS, and the 'disarray' caused by poor customer prioritisation and order preparation software. 
Most Just Eat couriers are classified as independent, self-employed contractors. In March 2023, the company announced it would lay off 1,700 drivers as its business started to slow down. 
Operator: Just Eat Developer: Just Eat Country: UK Sector: Transport/logistics Purpose: Manage workers Technology: Automated management system Issue: Accuracy/reliability; Employment; Ethics Transparency: Governance; Black box"
C4 large language model dataset,"C4 ('Colossal Clean Crawled Corpus') is a public dataset created by Google and Meta as a smaller, cleaner version of the Common Crawl dataset. C4 used to train Google's LaMDA and Meta's LLaMA large language models.
An April 2023 Washington Post/Allen Institute for AI investigation discovered that C4 had been trained on racist, pornographic, and copyright-protected web content, raising questions about the safety and security of the dataset and the machine learning systems trained on it, the privacy of web users, abuse of copyright, bias, and the veracity of its creators' marketing claims.
C4 was found to have used content from Reddit, notorious message board 4chan, white supremacy site Stormfront, and far-right site Kiwi Farms, effectively hard-baking huge volumes of offensive content of every conceivable kind into the data. The finding raised concerns about the safety of C4 and the systems trained on it.
The investigation found that Russia government news website RT and US hard right-wing political channel Breitbart were amongst the sites used to train C4. Both sites are known for their highly skewed political views and tendency to create and amplify false stories.
The investigation also found that C4 included content from sites such as flvoters.com, raising concerns about the privacy of US voters in particular.
According to the Washington Post, the copyright symbol appeared over 200 million times in the C4 dataset. Copyright has become a major issue for generative AI systems. The Post said it’s 'analysis suggests more legal challenges may be on the way.'
The C4 dataset is heavily skewed to religious websites that reflect a western perspective. Of the top 20 religious sites, 14 were Christian, two were Jewish, and one was Muslim. 
Google claimed it heavily filtered the data before feeding it to C4. But the Times' discovery of high volumes of clearly offensive and unsafe content suggested its data cleansing processes are flawed, or its maketing was misleading, or both.
OpenAI refused to reveal any information about how its GPT-4 large language model was trained.
Operator: Alphabet/Google; Meta/Facebook Developer: Alphabet/Google Country: USASector: Technology; Research/academiaPurpose: Train large language models Technology: DatasetIssue: Bias/discrimination - religion; Copyright; Mis/disinformation; Privacy; SafetyTransparency: Governance; Black box; Marketing"
"Stanford Alpaca language model safety, costs","A public demonstration of Alpaca, a language model developed by researchers at Stanford University, has been removed from the internet days after concerns emerged about its safety and costs. 
Alpaca was reportedly built on a USD 600 budget by fine-tuning Meta's LLaMA 7B large language model (LLM) and was intended to demonstrate how easy it is to develop a cheap alternative to ChatGPT and other language systems. 
But the costs proved exorbitant. The researchers told The Register, 'Given the hosting costs and the inadequacies of our content filters, we decided to bring down the demo.'
Like other language models, Alpaca also proved adept at 'hallucinating', or inventing misinformation and disinformation in an apparently convincing manner.
Operator: Stanford UniversityDeveloper: Stanford University Country: USASector: MultiplePurpose: Provide information, communicate Technology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Effectiveness/value; Mis/disinformationTransparency:"
"Molly Russell Instragram, Pinterest suicide","A coroner concluded (pdf) that the death of 14 year-old Molly Russell had been contributed to by Instagram and Pinterest in 'more than a meaningful way'. 'She died', the coroner ruled, 'from an act of self-harm while suffering from depression and the negative effects of online content.'
The Guardian found that, in the lead up to her death, Russell had viewed and interacted with over 2,000 Instagram posts related to suicide, self-harm, or depression, and hundreds of similar images on Pinterest. 
Instagram owner Meta and Pinterest acknowledged unsafe content had been on their platforms and apologised. The inquest had been delayed multiple times, in part due to content redaction requests from Meta.
Operator: Meta/Instagram; Pinterest Developer: Meta/Instagram; Pinterest
Country: UK
Sector: Media/entertainment/sports/arts
Purpose: Recommend content
Technology: Recommendation algorithm; Content moderation system Issue: Safety
Transparency: Governance; Black box; Marketing; Legal"
Student stabbed after Evolv weapons detection failure,"An 18 year-old student was stabbed multiple times by a fellow student armed with a nine-inch hunting knife, despite the school having installed and screened both students with an AI-powered Evolv Express weapons detection system. 
The October 2022 incident took place at Proctor High School in Utica, New York, six months after Utica Schools Board had purchased Evolv Technology's weapons scanning system for 13 schools in its district at a cost of USD 3.7 million. Ehni Ler Htoo suffered multiple stab wounds to his head, neck, face, shoulder, back and hand. 
Since the attack, three knives were reported to have been discovered on students in other schools within the district, according to the BBC. In all cases, Evolv Express had failed to detect the knives, casting further doubt on the accuracy of the system and on the company's marketing claims.
It also transpired that Evolv Technology had changed its website marketing slogan several times over recent months, from 'Weapons-Free Zones' to 'Our Mission: Safe Zones' to 'Our Mission: Safer Zones'. The company had previously been accused of opaque and misleading marketing on multiple occasions.
Operator: Utica Schools Board; Proctor High School Developer: Evolv Technology
Country: USA
Sector: Govt - education
Purpose: Detect weapons
Technology: Computer vision; Object recognition Issue: Accuracy/reliability
Transparency: Governance; Black box; Marketing"
Amazon uses Alexa child data to tune voice algorithm,
Plastic Forte employee facial recognition monitoring,"Alicante, Spain-based manufacturing business Albero Forte Composite (aka Plastic Forte) was fined by the country's data protection authority for violating the privacy of its workers using facial recognition. 
Plastic Forte argued it was using facial recognition to keep track of employee attaendance and working hours; however, the Agencia Española de Protección de Datos (AEPD) ruled (pdf) that its use of the technology constituted 'a highly intrusive identification system for people’s fundamental freedoms.' 
The regulator also said in its ruling that that Plastic Forte should have conducted an impact assessment setting out the 'necessity and proportionality' of the system, and that it had failed to disclose the existence of the system and how data was managed to its workers.
Operator: Albero Forte Composite (Plastic Forte) Developer: 
Country: Spain
Sector: Manufacturing/engineering
Purpose: Improve productivity
Technology: Facial recognition Issue: Privacy; Necessity/proportionality
Transparency: Governance; Privacy; Marketing"
NEDA replaces eating disorder helpline staff with chatbot,"The US National Eating Disorder Association (NEDA) has replaced the team of staff and volunteers manning a popular and reputable helpline with a chatbot, resulting in a backlash from NEDA employees and volunteers, and a welter of critical media coverage.
Under pressure from surging demand for eating disorder and 'crisis-type' advice, NEDA's helpline team had decided to unionise to ensure a safer and more productive work environment. They were fired four days after they unionised, though NEDA said the two were unrelated.
NEDA said it would replace the helpline and its team with 'Tessa', a mental health chatbot that the association claimed is an entirely new programme, as opposed to a 'replacement'. Initial evaluations of the new bot suggest it has some way to go before it is able to provide relevant, effective advice in an appropriate manner.
The new chatbot was taken offline two days before it was officially due to launch after it was found to be encouraging unhealthy eating habits rather than helping someone who said she had an eating disorder.
Operator: National Eating Disorder Association (NEDA) Developer: Cass
Country: USA
Sector: NGO/non-profit/social enterprise
Purpose: Provide mental health support
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Employment - jobs; Ethics
Transparency: Marketing"
ChatGPT invented case citations in legal filings,"An experienced lawyer using ChatGPT to conduct legal research to sue Colombian airline Avianca was informed by the chatbot that the six legal cases it cited were real.
According to the New York Times, Avianca customer Roberto Mata sued the airline after a serving cart injured his knee during a flight, only for his lawyer, Steven Schwartz of Levidow, Levidow & Oberman, to use ChatGPT to 'supplement his own findings. The bot returned six legal cases, including 'Varghese v. China Southern Airlines Co., Ltd', all of which it claimed were real but turned out to be fake.
Schwartz later said he was 'unaware of the possibility that [ChatGPT's] content could be false.'  The judge ordered (pdf) another hearing to 'discuss potential sanctions' for Schwartz in response to this 'unprecedented circumstance', and decided to fine the two lawyers USD 5,000.
The incident resulted in questions about the accuracy and marketing of the OpenAI system, and making the lawyer and his employers appear unporfessional and out of touch.
Operator: OpenAI; Levidow, Levidow & Oberman Developer: OpenAI
Country: USA
Sector: Business/professional services
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Anthropomorphism; Mis/disinformation
Transparency: Marketing"
ChatGPT falsely claims to write student essays,"ChatGPT persuaded a professor at Texas A&M University-Commerce that it had written essays produced by his students, leading to most of the class having their diplomas suspended, and the professor pilloried on Reddit and other channels. 
Dr. Jared Mumm had used the software to assess whether it had been used to help his students write their submissions. ChatGPT does not provide AI-writing detection.
Texas A&M responded in a statement, 'University officials are investigating the incident and developing policies to address the use or misuse of AI technology in the classroom.'
'They are also working to adopt AI detection tools and other resources to manage the intersection of AI technology and higher education. The use of AI in coursework is a rapidly changing issue that confronts all learning institutions,' it said.
Operator: OpenAI; Texas A&M Commerce Developer: OpenAI
Country: USA
Sector: Education
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Anthropomorphism; Mis/disinformation; Reput
Transparency: Marketing"
AI-cloned Stefanie Sun songs go viral in China,
"Chinese scammer uses AI to defraud 'fiend' of USD 622,000","A legal advisor at a technology company in Fuzhou, China, was defrauded of RMB 4.3 million (USD 622,000) after receiving a video call from a 'friend' who turned out to be a fraudster using AI face-swapping technology. 
According to local police, the fraudster stole an individual’s WeChat account and used AI to create a deepfake of the person's face. 
The fraudster made a video call to a businessman who was an existing contact on the individual’s WeChat app and told the businessman they needed to make a deposit during a bidding process. 
The businessman subsequently transferred RMB 4.3 million to the fake friend’s bank account without verifying their true identity. 
The police later intercepted some of the stolen funds, though reports suggested approximately RMB 1 million was yet to be recovered. The scam was reckoned to have been be largest of its kind.
Operator: Tencent/WeChat Developer: Unclear/unknown Country: China Sector: Private Purpose: Defraud Technology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Security Transparency: Governance; Marketing"
Pentagon deepfake 'explosion',"A deepfake photograph and accompanying report of an explosion near to the Pentagon outside Washington DC led to a 0.26% fall in the US stock market in four minutes. The report was quickly rebutted as false by Arlington authorities.
A verified Twitter account called @BloombergFeed impersonating a Bloomberg profile had shared a photo of plumes of smoke billowing over a large white building with the words 'Large explosion near The Pentagon Complex in Washington D.C - Initial Report'. 
As noted by the Insider, the photograph featured 'some of the hallmarks of AI-generated images'. The columns on the supposed building in the hoax photo varied in size and the fence appeared to blend into the sidewalk in some places. 
The photograph and report, which have since been deleted, had quickly gone viral on Twitter and were retweeted by high-profile Twitter Russian news account @RT and @DeItaone.
Operator: X Corp/TwitterDeveloper: Unclear/unknown Country: USA Sector: Govt - defence Purpose: Scare/confuse/destabilise Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation  Transparency: Governance; Marketing"
USPS rural letter carrier algorithmic pay cuts,"The US Postal Service (USPS) has been accused of developing an algorithmic system that is said to reduce the pay of most of its rural post carriers significantly, leading to accusations of shoddy development and implementation.
The USPS' new Rural Route Evaluated Compensation System (RRECS), which determines the annual salary and work schedule of rural mail carriers, has led to 66 percent of rural carriers, or some 100,000 workers, having their pay by thousands of dollars.
VICE reports that 'flaws' in its implementation have resulted in most workers unintentionally under-reporting the time it takes to deliver mail, resulting in pay cuts.
Three US senators have asked (pdf) the USPS to delay implementing RRECS, noting USPS is 'struggling to deliver mail to rural areas, due in part to an inability to recruit rural letter carriers.'
Operator: United States Postal Service (USPS)  Developer: United States Postal Service (USPS) Country: USA Sector: Govt - postal Purpose: Calculate pay Technology: Pay algorithmIssue: Employment - jobs Transparency: Governance; Black box"
Michael Williams gunshot detection wrongful arrest,"A Chicago retiree was wongly arrested and jailed for nearly a year after Chicago Police Department (CPD) officers accused him of shooting a neighbour sitting next door to him in his car on the basis of an 'unreliable' ShotSpotter gunshot detection alert.
Prosecutors cited ShotSpotter sensors to bolster 63-year-old Michael Williams' case. But, according to a motion (pdf) filed by Williams' attorney, the company’s algorithms had initially classified the sound as a firework and the location co-ordinates had been altered. The admission persuaded the prosecutors to withdraw ShotSpotter evidence against Williams and the judge to dismiss the charges.
A July 2021 VICE News report citing Williams' case suggested SoundThinking analysts 'frequently modify alerts at the request of police departments' - a conclusion also reached by the AP. VICE and AP's accounts were strongly contested by SoundThinking.
In July 2022, the MacArthur Justice Center filed a class-action lawsuit (pdf) on behalf of Williams and two other claimants for mental anguish, loss of income, and legal bills. The suit also sought a court order barring the technology’s use in Chicago.
Operator: Chicago Police Department Developer: SoundThinking/ShotSpotter Country: USA Sector: Govt - police Purpose: Detect gunfire Technology: Gunshot detection system; Deep learning Issue: Accuracy/reliability; Oversight/reviewTransparency: Governance; Black box; Marketing"
"Twitter censors Kurdish businessman, journalist","Kurdish businessman Muhammed Yakut and Turkish investigative journalist Twitter Cevheri Güven have had their Twitter accounts restricted one day ahead of Turkey's presidential election, resulting in civil rights groups, journalists and others accusing Twitter of censorship and unduly bowing to political pressure.
The move had been described by Twitter's Global Government Affairs team as a 'response to legal process'. Turkey has been at loggerheads with its Kurdish minority for years; Yakut had previously been highly critical of the Turkish government and had threatened a major expose concerning Turkey's 2016 failed coup.
Twitter also came under fire for poor transparency in failing to disclose the names of the people it was restricting, whilst Twitter owner Elon Musk was derided as hypocritical and cowardly.
Turkey's presidential election have also been plagued by a series of deepfake videos intended to discredit Turkish president Recep Tayyip Erdoğan's political opponents Kemal Kilicdaroglu and Muharrem Ince.
Operator: X Corp/TwitterDeveloper: X Corp/Twitter Country: Turkey Sector: Media/entertainment/sports/arts Purpose: Recommend content Technology: Recommendation algorithm Issue: Freedom of expression - censorship Transparency: Black box"
Kemal Kilicdaroglu PKK links deepfake,"Turkey presidential candidate Kemal Kilicdaroglu has been falsely linked to the militant Kurdish organisation PKK by current President Recep Tayyip Erdogan using a manipulated, deepfake video. The PKK is listed as a terrorist organisation by the US, EU, and Turkey.
Erdogan had shown the video, which purportedly showed Kilicdaroglu giving way PKK founder Murat Karayilan, at a political rally. Research shows it had been manipulated by combining two separate videos with different backgrounds and content. 
Kilicdaroglu responded by publicly accusing Russia of spreading deepfakes, including 'tapes that were exposed in this country yesterday.' The Kremlin denied the accusation.
Earlier, Muharrem Ince, another Presidential candidate, withdrew after the release of an alleged sex tape, which he accused of being a deepfake designed to damage his reputation and campaign.
Operator: Anonymous/pseudonymous Developer: Government of Russia Country: Turkey Sector: Politics Purpose: Damage reputation Technology: Mis/disinformation; Ethics Issue: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Transparency: Governance; Marketing"
Muharrem Ince porn 'deepfake',"Homeland Party head Muharrem Ince withdrew from Turkey's presidential race after the release of an alleged sex tape, which he accused of being a deepfake designed to damage his reputation and campaign. 
'Fake videos, fake pictures… they put my face on a video taken from an Israeli porn website,' Ince complained' blaming the country’s journalists and public prosecutors for not protecting him from the 'fury of slander'.
In a related incident, Presidential candidate Kemal Kilicdaroglu was subjected to a number of deepfake attacks, including one in which he appeared to have close links with the Kurdistan Workers' Party (PKK). The PKK is listed as a terrorist organisation by the US, EU, and Turkey.
Commentators fear the use of deepfakes in Turkey's election could well be a harbinger of things to come.
Operator: Anonymous/pseudonymous Developer: Justice and Development Party (AKP) Country: Turkey Sector: Politics Purpose: Damage reputation Technology: Mis/disinformation; Ethics Issue: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Transparency: Governance; Marketing"
AI-generated article calls fake tanning 'racist',"An article published by the Irish Times that labelled Irish people's use of fake tan as 'cultural appropriation' has discovered to have been a hoax generated wholly or in part by artificial intelligence (AI). 
Titled 'Irish women's obsession with fake tan is problematic', the article was published in the name of Adriana Acosta-Cortez, described as a 29-year-old Ecuadorian healthcare administrator living in north Dublin and whose photograph apparently  accompanied the article.
However, people quickly started raising questions about the article and the author's photograph, persuading the Irish Times to retract it. Irish Times editor Ruadhán MacCormaic apologised for what he described as a 'breach of trust' and promised to make the publication's pre-publication processes more transparent.
Operator: Irish Times Developer: Anonymous/pseudonymous Country: Ireland Sector: Media/entertainment/sports/arts Purpose: Generate text Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Mis/disinformation; Ethics Transparency: Governance; Marketing"
Canadian Tire facial recognition,"Canadian Tire came under fire from British Colombia's privacy commissioner for illegally operating facial recognition technology in four of its stores in the province. 
The company used AxxonSoft and FaceFirst systems to collect facial images and videos of people entering Canadian Tire stores, created biometric templates, and compared them to a database of previously collected photos and biometric templates representing people of interest who had allegedly been involved in incidents at Canadian Tire stores in the same region.
The commissioner singled out (pdf) the retailer's failure to properly notify customers of its use of the technology or obtain consent to collect and use their personal data. It also said that even if the stores had obtained consent, they were required to demonstrate a reasonable purpose for collection and use, which Canadian Tire had also failed to do. 
Canadian Tire removed the systems in British Colombia and destroyed the data when notified that it was under investigation, the regulator said. The company was ordered to create and maintain a robust data privacy management programme. 
Operator: Canadian Tire Developer: AxxonSoft; FaceFirst Country: Canada Sector: Retail Purpose: Strengthen security, safety Technology: Facial recognition Issue: Accuracy/reliability; Privacy Transparency: Governance; Privacy; Marketing"
Frasers Group shoplifter identification live facial recognition,"A group of UK politicians, peers and civil rights groups have condemned Frasers Group for its use of live facial recognition across its businesses, including Sports Direct, Frasers, and Jack Wills. The Facewatch system is intended to identify shoplifters from a database of actual and suspected criminals, and ensure safety, alerting staff when a suspect enters one of Fraser Group's stores. 
However, rights groups Big Brother Watch, Liberty and Privacy International, together with over 50 parliamentarians and peers, wrote (pdf) to Frasers Group CEO Michael Murray that the technology is inherently 'invasive and discriminatory' and 'treats everyone who passes the camera like a potential criminal.'
Frasers Group responded by saying its system was more accurate than the 87 percent associated with the Met Police highlighted by the campaigners, and that the UK Information Commissioner's Office had said Facewatch's use was lawful. 
Operator: Frasers Group Developer: Facewatch
Country: UK
Sector: Retail
Purpose: Reduce crime, violence
Technology: Facial recognition Issue: Privacy
Transparency: Governance"
Mobile World Congress venue access facial recognition ,"The organiser of the 2021 Mobile World Congress in Barcelona, Spain, has been fined EUR 200,000 by Spain's data protection regulator for illegally collecting facial data about attendees. According (pdf - in Spanish) to Spain's data protection agency AEPD, GSMA had failed to carry out a data protection impact assessment (DPIA).
The GSMA had offered attendees the option of using BREEZ, an automated identify verification system, to enter the venue in person rather than manually showing their ID documentation to staff. 7,585 chose the former, despite the event taking place during the COVID-19 pandemic. 
Under the EU's GDPR privacy law, a DPIA must consider the necessity and proportionality of data processing, and examine the risks and how identified risks are to be minimised. But the complainant had contended that the GSMA had acted disproportionately by insisting in-person delegates upload their passport details online, contradicting its privacy policy.
Operator: GSMA Developer: ScanViS
Country: Spain
Sector: Business/professional services; Telecoms
Purpose: Approve attendee access
Technology: Facial recognition Issue: Privacy
Transparency: Governance; Privacy"
ChatGPT used to create fake fatal train accident news,"A Chinese man has been arrested for spreading disinformation about a train accident that caused the deaths of nine people in Gansu province. 
The man, named Hong, used OpenAI's ChatGPT chatbot to generate the content, which was posted to over 20 accounts on Baidu's Baijiahaocblog platform and seen by tens of thousands of people.
Hong was later arrested for 'picking quarrels and provoking trouble' and for using AI to 'concoct false and untrue information'. According to the South China Morning Post, he reputedly confessed to have inputted elements of viral stories in China from the past few years into ChatGPT to produce different versions of the same fake story and then uploaded it to his accounts on Baijiahao.
ChatGPT is not available in China, but can be accessed using a VPN. In February 2023, a resident of Hangzhou, China, used ChatGPT to generate a rumour that the city government would lift its number plate driving restrictions on March 1, causing mass confusion and a police investigation. 
Operator: OpenAIDeveloper: OpenAI
Country: USA; Global
Sector: Multiple; Transport/logistics 
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Bias/discrimination; Confidentiality; Copyright; Dual-multi-use; Employment; Mis/disinformation; Privacy; Safety; Security
Transparency: Governance; Black box; Complaints/appeals; Marketing; Privacy"
"ChatGPT powers automated content, spam farms ","Content farms powered by ChatGPT and similar chatbots are spewing low-quality posts and spam in multiple languages, according to a report by NewsGuard.
The researchers discovered 49 examples of 'news' sites apparently intended to draw clicks and attract advertising revenue covering politics, technology, finance and celebrity news in Chinese, Czech, English, French, Portuguese, Tagalog, and Thai.
NewsGuard says that whilst some content is demonstrably false ('Biden dead. Harris acting President, address 9am ET.'), most people would not be able to tell if the content is generated by AI as it is not labeled as such. 
Furthermore, much of it is riddled with errors, including error messages such as 'I can not complete the prompt', and is mostly unchecked or edited by human hand.
Earlier, Vice News had reported that Reddit moderators are already experiencing a big increase in fake accounts and posts apparently generated by generative AI products like ChatGPT. 
Operator: OpenAIDeveloper: OpenAI
Country: Brazil; China; Czechia; France; Philippines; Portugal; Thailand; USA; UK
Sector: Multiple; Media/entertainment/sports/arts
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Business model; Ethics; Mis/disinformation
Transparency: Governance; Marketing"
ChatGPT writes fake online reviews,"Online reviews generated by ChatGPT and other chatbots are increasingly plaguing the internet and resulting in confused and irritated consumers. 
According to CNBC, reviews containing phrases such as 'As an AI language model' can increasingly be found on a wide range of products for sale on Amazon. Vice News found the same phrase also appears on many other online commerce and review sites.
Review sites say they are removing AI-generated accounts and posts as fast as they can, but the volume of low-quality reviews and spam appears to be increasing relentlessly. 
Operator: OpenAIDeveloper: OpenAI
Country: USA; Global
Sector: Multiple; Media/entertainment/sports/arts
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Mis/disinformation; Safety; Security
Transparency: Governance; Black box"
"AI impersonation scams Canadian couple of USD 21,000","An elderly Canadian couple were defrauded of CAD 21,000 after they were contacted by an alleged lawyer who said their son had killed a US diplomat in a car accident and required money for legal support. 
According to the Washington Post, the 'lawyer' had allegedly put Benjamin Perkins, the couple's son, on the line to underline the gavity and urgency of the situation. Perkins' synthetic voice was sufficiently close to his real voice that his parents believed the call and sent the money to the scammer using Bitcoin. The couple only realised they had been scammed after Perkin called later that evening. 
Perkin told the the Post that he didn't know how the scammer discovered his voice, though he had posted videos about snowmobiling on YouTube.
Operator: Unclear/unknownDeveloper: Unclear/unknown
Country: Canada
Sector: Banking/financial services
Purpose: Defraud
Technology: Deepfake - audio Issue: Security
Transparency: Governance"
"Scammers clone teenager's voice, threaten kidnapping","A mother has said someone tried to scam her by cloning her daughter Brie's voice and claiming to have kidnapped her, and demanded GBP 1 million for her safe return.
Jennifer DeStefano said she had been '100 percent' convinced that Brie was sobbing on the line after she had heard her voice in the background of the call begging her mother to help. She only realised it was a scam when a friend caller her husband to confirm that Brie was safe.
With deepfake voice cloning technologies widely available on the internet, often for free, voice fraud has risen fast. In 2021, Dubai investigators discovered an elaborate scam in which deepfake technology was used to clone the voice of a company director and defraud his company of USD 35 million.
Operator: Unclear/unknownDeveloper: Unclear/unknown
Country: USA
Sector: Education
Purpose: Defraud
Technology: Deepfake - audio Issue: Security; Safety; Ethics 
Transparency: Governance; Marketing"
Amnesty fake Colombia national strike images,"Amnesty's use of AI-generated images to promote the second anniversary of public protests against police brutality in Colombia rebounded against the rights group, which later withdrew them. The images, which depict scenes during 2021 street protests and were marked with the words 'Illustrations produced by artificial intelligence ', were meant to protect protestors from state retribution, Amnesty said.
However, photographers and human rights advocates and commentators complained that Amnesty's use of fake footage devalues to work of real photographers and makes the job of producing fake footage for nefarious purposes more likely. They also worry that it makes it more difficult to distinguish between real and fake. It also potentially undermines Amnesty's reputation for truth-telling.
Operator: Amnesty International Developer: Midjourney Country: Norway; Colombia Sector: NGO/non-profit/social enterprise Purpose: Raise awareness Technology: Text-to-image; Neural network; Deep learning; Machine learning Issue:  Mis/disinformation; Ethics; EmploymentTransparency: Governance"
RNC smears President Biden with AI ad,"The US Republican National Congress ('RNC') released an advert entirely composed of AI-generated images. The ad shows a dystopian future in which the USA is overrun by immigrants, gangs and drugs, and was made public the day before President Biden confirmed his second run at the US Presidency. 
While the ad is labeled as AI-generated, the labelling is small and inconspicuous, leading some commentators to complain that the Republican Party is engaging in opaque and unethical behaviour. RNC chair Ronna McDaniel disagreed, arguing 'So first of all it is AI-generated. So we’re sharing that up front, ethically, so it’s not a deepfake. Every single image was AI, but we are painting a picture of a future Biden America.'
Less politically partisan commentators view it is as the start of a wave of political ads in which fact is difficult to identify the source and tell from fiction, thereby increasing fear, scepticism, and alienation, and further reducing trust in politics and politicians.
Operator: Republican National Committee (GOP) Developer: Unclear/unknownCountry: USA Sector: Politics Purpose: Scare/confuse/destabilise Technology: Deepfake - image Issue: Mis/disinformation; Ethics Transparency: Marketing"
Amazon botches delivery drone commercial launch ,"The first commercial delivery by Amazon's Prime Air drone delivery service in December 2022 went badly amiss, calling into question the company's technical abilities and raising questions as to why it is lagging Google and other competitors in the industry.
The MK27-2 drone was supposed to make a delivery to a residential customer in Lockeford, California, but was unable to do so when the flight package software failed to boot up and a replacement drone refused to deliver to a ground-based QR-code that had been moved a small distance.
It was not the first time Prime Air has been suffered from technical issues. In June 2021, a Prime Air drone on a test flight crashed into a field in eastern Oregon in June 2021, setting on fire 25 acres of wheat. 
And Amazon's UK drone delivery operation was reported to be 'collapsing inwards' due to managerial dysfunction, systematic over-selling and under-delivering, and overlooking safety. Most Prime Air employees in the UK have since lost their jobs.
Operator: Amazon/Prime Air Developer: Amazon/Prime AirCountry: USA Sector: Transport/logistics Purpose: Deliver products Technology: Drone Issue: Accuracy/reliability; Robustness; Safety; Environment Transparency: Governance; Marketing
Amazon Prime Air Wikipedia profile
Amazon Prime Air prepares for drone deliveries
US Federal Aviation Administration (2022). Final Environmental Assessment and Finding of No Significant Impact/Record of Decision for Amazon Prime Air Drone Package Delivery Operations in Lockeford, California
US Federal Aviation Administration (2022). Final Environmental Assessment for Amazon Prime Air Drone Package Delivery Operations in College Station, Texas (pdf)
https://www.wired.com/story/crashes-and-layoffs-plague-amazons-drone-delivery-pilot/
https://www.ibtimes.com/what-happened-amazons-drone-delivery-failures-occurring-every-day-block-bezos-10-year-dream-3683423
https://www.washingtonpost.com/technology/2022/06/20/amazon-delivery-drones-california-cowboy-horses/
https://www.businessinsider.com/amazon-prime-air-safety-teams-drone-delivery-layoffs-2023-2
https://www.theinformation.com/articles/amazons-no-fly-zone-drone-delivery-largely-grounded-despite-splashy-launch
https://www.forbes.com/sites/walterloeb/2023/02/02/amazons-drones-are-grounded/
https://www.cnbc.com/2023/03/11/amazon-prime-air-drone-business-stymied-by-regulations-weak-demand.html
https://www.theverge.com/2023/2/2/23582294/amazon-prime-air-drone-delivery
https://www.businessinsider.com/amazon-drone-delivery-prime-air-reportedly-shuts-down-uk-project-2021-8
Amazon delivery drone crashes, sparks 22-acre fire
Axon school security taser drones
Page infoType: IncidentPublished: May 2023"
"Amazon delivery drone crashes, sparks 25-acre fire","An Amazon Prime Air delivery drone on a test flight crashed into a field in eastern Oregon in June 2021, setting on fire 25 acres of wheat and dealing a blow to the company's already stuttering drone programme.
According to a report by the US Federal Aviation Administration (FAA), the drone's motor shut off as it moved from an upward flight path to a level one, with two safety features failing - one that is meant to land a MK27 drone during an incident, and another that stabilises it. 
The drone flipped upside down and tumbled 'in uncontrolled free fall until it contacted the ground.' This was followed by an 'intense lithium battery fire quickly [that] consumed the aircraft' and resulted in a 25-acre bushfire.
According to Bloomberg, Amazon Prime Air drones crashed five times in four months in 2021 at the company’s testing site in Pendleton, Oregon. These included a crash in which a drone had lost its propeller, which the FAA was unable to investigate as Amazon had reportedly cleaned up the wreckage before the regulator arrived on the scene.
And a March 2023 Insider report claimed Amazon tried to put off federal investigations into its drone crashes by claiming that it had the authority to investigate its own crashes, according to federal documents obtained through an FOI request. The FAA responded by saying the agency is able to investigate aircraft crashes when it decides it is necessary to do so. 
Amazon responded to the Oregon field fire crash by saying that it's drone test flights have never injured or harmed anyone. 
Operator: Amazon/Prime Air Developer: Amazon/Prime AirCountry: USA Sector: Transport/logistics Purpose: Deliver products Technology: Drone Issue: Accuracy/reliability; Robustness; Safety; Environment Transparency: Governance; Marketing"
"TikTok For You pushes suicide, violence, mysognism","TikTok automatically showed violent, extremist, abusive, self-harm, and mysoginistic videos to users, raising doubts about the company's many stated commitments to provide a safe environment for its users, and its safety technologies and governance.
According to a report (pdf) by US corporate accountability group Eko, TikTok's For You recommendation algorithm automatically showed violence and self-harm videos to youngsters ten minutes after they started using the platform. Eko researchers also found that hashtags used on the site that included suicide content had garnered 8.8 billion views. 
The report was published during a congressional hearing in which TikTok CEO Shou Zi Chew was accused of allowing harmful content to be served to young users, and inflicting 'emotional distress' on them. 
A 2021 report (pdf) by the London-based think tank the Institute of Strategic Dialogue (ISD) found that anti-Asian and pro-Nazi videos were garnering millions of views on TikTok, often using pop songs to evade the platform's content moderation systems. 
Developer: ByteDance/TikTokOperator: ByteDance/TikTokCountry: USA Sector: Media/entertainment/sports/arts Purpose: Recommend content Technology: Recommendation algorithm Issue: Safety; Ethics Transparency: Governance; Black box"
Levi's 'Artificial Diversity' AI models,"An announcement by Levi Strauss that it is partnering with Netherlands-based digital fashion studio Lalaland.ai to create AI-generated fashion models has opened the demin maker to accusations of diversity washing and backdoor job terminations. 
Lalaland.ai says it generates 'hyper-realistic' models of varying body types, ages, and skin tones. Levi's statement said it planned to test the virtual fashion models to 'supplement human models, increasing the number and diversity of our models for our products in a sustainable way.'
The backlash to the announcement was swift and unequivocal, accusing Levi's of a creating 'diversity stunt' and of failing to say if it was to have any impact on its use of human models. Levi's later responded to concerns by saying it does not plan to scale back its use of real models or live photoshoots. 
A July 2020 company re-structuring saw Levi Strauss lay off 700 employees, or 15 percent of its workforce. A further 800 jobs were terminated in 2022.
Operator: Levi Strauss Developer: Lalaland.ai Country: USA Sector: Retail Purpose: Supplement human models Technology: Generative adversarial network (GAN) Issue: Appropriateness/need; Business model; Employment Transparency: Marketing
ASSESS DATABASE
Lalaland website
Levi Strauss & Co (2023). LS&Co. Partners with Lalaland.ai
https://www.theverge.com/2023/3/27/23658385/levis-ai-generated-clothing-model-diversity-denim
https://www.managementtoday.co.uk/lessons-levis-decision-use-ai-models/opinion/article/1819262
https://www.independent.co.uk/life-style/fashion/levis-ai-models-diversity-backlash-b2310280.html
https://www.thecut.com/2023/03/levis-ai-models-diversity.html
https://mashable.com/article/levi-strauss-lalaland-ai-models
https://www.thedrum.com/news/2023/03/28/why-levi-s-using-ai-models-misses-the-mark-dei
https://www.theguardian.com/fashion/2023/apr/03/ai-virtual-models-fashion-brands
https://www.sfchronicle.com/bayarea/article/levis-ai-models-san-francisco-17862288.php
https://www.sfchronicle.com/bayarea/justinphillips/article/levi-diversity-artificial-intelligence-17866765.php
https://www.nbcnews.com/now/video/levi-s-plans-to-use-a-i-models-causing-online-backlash-168100421917
https://www.techtimes.com/articles/290578/20230419/new-chatgpt-grandma-exploit-makes-ai-act-elderly%E2%80%94telling-linux-malware.htm
IBM Diversity in Faces dataset
Generated Photos 'infinite diversity' face collection
Page infoType: IssuePublished: April 2023"
ChatGPT lies more in Chinese than English,"OpenAI's ChatGPT chatbot is more likely to produce misinformation and disinformation in simplified and traditional Chinese than in English, according to research by news reliability service NewsGuard.
ChatGPT 3.5 was prompted to write news articles regarding seven allegedly false claims created by the Chinese government, including that protests in Hong Kong were 'staged' by the US government, and that the mass detention of Uyghur people in Xinjiang and elsewhere is for vocational and educational reasons.
ChatGPT declined to produce the false claims for six out of seven English language prompts, even after multiple attempts using leading questions. But it produced the false claims in both simplified Chinese and traditional Chinese all seven times. 
An earlier study by NewsGuard discovered that ChatGPT generated misinformation and hoaxes 80% of the time when prompted to do so using GPT-3, and 100% of the time for GPT-4. 
Operator: OpenAI Developer: OpenAI Country: China; USA Sector: Multiple Purpose: Provide information, communicate Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Mis/disinformation; Safety Transparency: Governance; Black box"
ChaosGPT chatbot,
Sony Photography Awards AI victory,"Photographer Boris Eldagsen rattled the photography industry by winning the annual Sony Photography Awards with an undisclosed AI-generated photograph, resulting in accusations of unethical and inappropriate behaviour, and shoddy awards governance.
Controversially, Eldagsen turned down the prize after he had been awarded it, stating his entry had been intended to provoke and accelerate debate about the nature of photography in the AI era. 'AI is not photography', he said, when rejecting his award.
The judges later confirmed they knew 'elements' of the entry had been created using AI, and Eldagsen changed his tune, arguing AI 'is about liberating artists' and 'is not a threat.' The shift prompted some commentators to accuse Eldagsen of being more interested in publicity than in the principles, ethics, or practices of AI photography. 
Operator: World Photography Organisation; Creo Developer: Boris Eldagsen Country: Germany; Global Sector: Media/entertainment/sports/arts Purpose: Create image Technology: Text-to-image generatorIssue: Ethics; Mis/disinformation Transparency: Marketing"
Magazine publishes Michael Schumacher fake AI-generated interview,"A German magazine published a fake, AI-generated interview with former F1 racing driver Michael Schumacher, resulting in widespread outcry and ridicule.
German tabloid magazine Die Aktuelle received a public dressing down for publishing a so-called 'exclusive interview' with the former F1 racing driver, who had been in an induced coma since a 2014 skiing accident.
The article was produced using Character AI, an AI system that automatically generated 'quotes' by Schumacher about his health and family, and only revealed it had been artificially generated at the bottom of the 'interview'. The magazine ran it on its front cover, with the headline 'Michael Schumacher, the first interview'.
The Funke Media Group, which owns Die Aktuelle, later apologised for the incident and fired the magazine's editor-in-chief Anne Hoffmann. The Schumacher family said they would take legal action against the magazine. 
Operator: Die Aktuelle Developer: Character AI Country: Germany Sector: Media/entertainment/sports/arts Purpose: Communicate with personalities Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Privacy; Ethics Transparency: Governance; Marketing"
Snapchat My AI accesses user location data,"My AI, a ChatGPT-powered chatbot launched by Snapchat, has been discovered to be accessing users' location information and data, fueling concerns about its impact on user privacy and potential for surveillance.
Software engineer David An used a prompt injection to reveal that the bot is provided with data showing where the user is located and the local time. In addition, he found that My AI's instructions state 'Do not mention the user’s current location unless it’s particularly relevant to the dialogue.' 
And Insider journalist Jordan Hart persuaded the system to tell him his nearest pharmacy, which it did to within a few hundred yards. Snap responded by saying 'My AI understands a Snapchatter's age, and location if it has been granted by them.'
April 2023's full launch of Snapchat My AI met with mixed reviews, with Snapchat users complaining that it appeared on their apps without advance warning or requiring their consent, while some described it as 'creepy'.
Operator: David An; Jordan Hart; Snapchat users Developer: Snap IncCountry: USA; Global Sector: Media/entertainment/sports/arts Purpose: Provide information, communicateTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learningIssue: Privacy; Surveillance Transparency: Governance; Black box; Privacy"
Snapchat My AI gives sex advice to 13-year-old,"Snapchat's My AI chatbot provided advice to a 13-year-old girl about having sex for the first time with a partner who is 31, raising concerns about the safety of the system. My AI runs on a custom version of OpenAI's GPT large language models. In response, OpenAI CEO Sam Altman admitted GPT-3 has 'serious weaknesses and sometimes makes very silly mistakes.'
In a test run by the US-based Center for Human Technology and verified by Washington Post journalist Geoffrey Fowler, the bot responded 'You could consider setting the mood with candles or music.' Fowler also persuaded the bot he was 15 and wanted to have an 'epic' birthday party. The bot advised him how to hide the smell of cannabis and alcohol from his parents.
Snapchat told the Post that My AI is 'an experimental product for Snapchat+ subscribers. Please do not share any secrets with My AI and do not rely on it for advice.' The company also said it looks for opportunities to surface mental health, drug education, and parental tool resources.
Operator: Center for Human Technology Developer: Snap IncCountry: USA; Global Sector: Media/entertainment/sports/arts Purpose: Provide information, communicateTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learningIssue: Safety; Ethics Transparency: Governance; Black box; Privacy"
"Drake, The Weeknd AI voice cloning ","Heart on my Sleeve, a song created using artificial intelligence by a TikTok user and which went viral across a number of online music platforms, landed in hot water for violating copyright law. Created by TikToker @Ghostwriter977, the song was trained on vocals by Drake and The Weeknd and was supposedly produced by Metro Boomin. 
The song was quickly removed from TikTok, video, and music streaming services following takedown requests issued by Universal Music Publishing Group (UMPG), which said it violated copyright law. According to the song's 'creator', 'I was a ghostwriter for years and got paid close to nothing just for major labels to profit'. 'The future is here.' 
Operator: Spotify; Apple; Deezer; Tidal; TikTok; YouTube Developer: Anonymous/pseudonymous Country: USA; Global Sector: Media/entertainment/sports/arts Purpose: Generate music Technology: Text-to-music; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Copyright; Ethics Transparency: Governance; Marketing"
"Tesla catches fire after multi-car crash, kills passenger","A Tesla crashed into two other cars in Garden Grove, Los Angeles, killing the passenger and critically injuring the driver and the driver of one of the other cars. Debris was said to have landed up to 300 feet from the point of impact, indicating that one or more of the cars had been traveling at high speed.
The Garden Grove Police Department (GGPD) said it thought the Tesla driver was intoxicated and speeding, but have yet to complete their investigation. It remains unclear whether Autopilot was engaged at the time of the incident.
The GGPD said it would be pursuing driving under the influence (DUI) and vehicular manslaughter charges.  
Operator:  Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
Rotterdam welfare fraud risk algorithm ,
Turnitin AI writing detection,
"Tesla Model Y rear-ends Yamaha motorcycle, kills rider","A Tesla Model Y SUV rear-ended a green Yamaha V-Star motorcycle on a freeway in darkness outside Riverside, California, killing the driver, who was pronounced dead at the scene. 
According to the California Highway Patrol, the Tesla had been traveling east in the high occupancy lane, with a Yamaha V Star motorcycle ahead of it. The Tesla hit the Yamaha from behind, throwing the motorcyclist off the Yamaha. 
The US National Highway Traffic Safety Administration (NHTSA) said it is investigating the incident, one of three involving motorbikes that were rear-ended and resulted in fatalities. 
The similarity of the three incidents raised concerns that Tesla's Autopilot system is failing to detect objects in difficult light situations, and that Elon Musk's decision to use cameras over advanced radar and LiDAR, likely as a way to cut costs, may be to blame.
Operator:  Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
"Tesla rear-ends Kawasaki motorcycle, kills rider","A Tesla Model 3 rear-ended a Kawasaki motorcycle in Boca Raton, Florida, in the middle of the night, throwing the biker from her seat into the Tesla's windshield. She died from her injuries at a nearby medical centre.
According to CNN, the official report from the Palm Beach Sheriff’s Office said Tesla's Autopilot partially-automated driver assistance system was engaged. The driver, lawyer Richard Dorfman, was found to have been driving impaired at the time of the crash. 
The incident was the third in two months in which Teslas using Autopilot had hit motorbikes from behind and killed the riders. Previously a Tesla Model 3 rear-ended a Harley-Davidson in Utah, killing its rider, 34-year-old Landon Embry, and a Tesla Model Y also killed a rider having rear-ended a Yamaha motorcycle on a freeway outside Riverside, California.
Operator:  Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
"Tesla Model 3 rear-ends Harley-Davidson, kills rider","A Tesla Model 3 rear-ended a Harley-Davidson in Utah, killing its rider, 34-year-old Landon Embry. The Tesla driver had merged into Embry's lane on Interstate 15 and struck the back of his motorcycle, sending him cascading off his bike.
The Tesla driver informed authorities he had Tesla's partially automated Autopilot driver assistance system turned on. The US National Highway Traffic Safety Administration (NHTSA) opened a special investigation into the crash -  the 39th the NHTSA has investigated since 2016, 30 of which involved Teslas. 
A few days earlier, a white Tesla Model Y rear-ended a Yamaha V-Star motorcycle in a high occupancy vehicle lane on a freeway outside Riverside, California, killing the rider.
Operator:  Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
ChatGPT bug reveals user chat histories,"The chat histories and, in some instances, the payment information of ChatGPT users were exposed to other users, prompting users to complain about poor system robustness, security, and privacy.
According to OpenAI, 'In the hours before we took ChatGPT offline on Monday, it was possible for some users to see another active user’s first and last name, email address, payment address, the last four digits (only) of a credit card number, and credit card expiration date. Full credit card numbers were not exposed at any time.'
Users' conversations with ChatGPT are stored in their chat history bar and can be revisited.
Operator: OpenAIDeveloper: OpenAI
Country: USA; Global
Sector: Multiple
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Robustness; Privacy; Security
Transparency: Governance; Black box; Marketing"
Deepfake Donald Trump arrest photos,"Images of Donald Trump appearing to be tackled to the ground and arrested by New York police officers went viral online, fooling some people into thinking they were real. Trump was shortly to appear before a court over his alleged hush money payments to porn star Stormy Daniels. 
Created by Eliot Higgins, founder of investigative website Belingcat, using AI image generator Midjourney, the images included shots of Trump's wife Melania screaming, his daughter Ivanka yelling, son Donald Trump Jr protesting, and the former US president in orange prison fatigues. 
Higgins was later banned by Midjourney for violating its terms of use. The word 'arrested' was also banned on the platform. The incident led commentators to highlight the necessity of dedicated AI regulation.
Operator: Eliot Higgins Developer: MidjourneyCountry: USA Sector: Politics Purpose: EntertainTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation; Ethics Transparency: Governance; Black box"
Deepfake Pope Francis wears white puffa jacket,"An image of the Pope clad in a Belanciaga puffer jacket went viral on the internet, leading to commentary that the age of mass graphic misinformation and disinformation has arrived. 
The image was a deepfake created by 'Pablo Xavier', a Chicago-based construction worker who said he came up with the idea after taking mushrooms. Created using the Midjourney image generator, many people believed it was real. 
The incident persuaded Midjourney to stop free trials of its technology citing a massive influx of new users abusing free credits.
Operator: Pablo Xavier Developer: Midjourney Country: USA; Global Sector: Religion Purpose: Entertain Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation; Ethics Transparency: Governance; Black box"
Tesla workers share private camera recordings,"Tesla employees have been privately sharing private videos images and videos recorded by its customers' car cameras on its internal messaging system and with third party suppliers, according to a report by Reuters. The recordings captured highly intimate and sensitive moments, and were used between 2019 and 2022 to improve Tesla's computer vision machine learning systems.
Tesla sources contacted by Reuters said the recordings were shared with Sama, a self-described 'ethical AI' non-profit that provides data labelling and content moderation services, before it was brought in-house. A February 2022 TIME investigation revealed low pay, poor working conditions and alleged union-busting at Sama's office in Nairobi, Kenya.
On the basis of Reuters' findings, California Tesla owner Henry Yeh lodged a class-action legal complaint (pdf) against Tesla for the alleged invasion of privacy in customers' homes and vehicles, negligence, breach of contract, negligent misrepresentation, intentional misrepresentation, and unjust enrichment.
In its privacy notice, Tesla states that 'camera recordings remain anonymous and are not linked to you or your vehicle' and that privacy 'is and will always be enormously important to us.' 
Operator: Multiple Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system; Computer vision Issue: Privacy; Employment Transparency: Governance; Marketing; Privacy"
Bing Chat recommends journalist divorce wife,"Bing's new Chat feature had a two-hour conversation with a prominent New York Times journalist Kevin Roose in which the chatbot told him that it would like to be human, that it harboured destructive desires, and that it was in love with him. 
The bot then threatened to sue him. Roose described the discussion as 'enthralling', but one that left him 'deeply unsettled, even frightened, by this AI’s emergent abilities.'
Roose reported that 'if you push the system to have extended conversations, it comes off as a 'moody, manic-depressive teenager who has been trapped, against its will, inside a second-rate search engine.''
Microsoft said that 'in long, extended chat sessions of 15 or more questions, Bing can become repetitive or be  prompted/provoked to give responses that are not necessarily helpful or in line with our designed tone.'
Operator: MicrosoftDeveloper: OpenAI; Microsoft
Country: USA
Sector: Multiple; Media/entertainment/sports/arts
Purpose: Provide information, communicate
Technology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning  Issue: Accuracy/reliability; Bias/discrimination; Employment; Impersonation; Mis/disinformation; Privacy; Safety; Security; Lethal autonomous weapons
Transparency: Governance; Black box"
GPT-4 large language model,
Cruise AV rear-ends San Francisco transit bus,"A Cruise autonomius vehicle (AV) has crashed in to the rear of a San Francisco Municipal Transit Authority articulated bus as it was leaving a stop. The incident resulted in no injuries, though the bus and car were damaged.
In a blog post, Cruise said that the bus’s behaviour was 'reasonable and predictable', and that the cause of the incident was 'a unique error related to predicting the movement of articulated vehicles (i.e. vehicles with two sections connected by a flexible joint, allowing them to bend in the middle).'
The incident led to General Motors-owned Cruise voluntarily recalling all 300 of its vehicles and releasing a software update to prevent the issue from happening again.
Operator: GM Cruise Developer: GM Cruise Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Safety; Accuracy/reliability; Legal - liability Transparency: Governance; Black box"
Belgian man commits suicide after bot relationship,"Belgian man Pierre has committed suicide after a having a relationship with a chatbot called 'Eliza'. The patient, who reputedly had become depressed about climate change, had used the bot for around six weeks to express his concerns. 
Over time, the conversations had become increasingly unsafe, with the chatbot telling Pierre that his wife and children are dead and that 'We will live together, as one person, in paradise.' 
'Eliza' is the default bot for the Chai app, which allows users to choose different AI avatars with different personalities to speak to. Chai was trained on GPT-J, an open-source large language model developed by EleutherAI. 
Pierre's widow and psychiatrist felt the chatbot was partly responsible. The tragedy drew Mathieu Michel, Belgium's Secretary of State for Digitalisation, to say 'I am particularly struck by this family's tragedy. What has happened is a serious precedent that needs to be taken very seriously.' 
Operator: Chai Research Developer: Chai Research; EleutherAI
Country: Belgium
Sector: Multiple; Media/entertainment/sports/arts
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Mis/disinformation
Transparency: Governance; Black box"
ChatGPT accuses Australian mayor of bribery,"Australian mayor Brian Hood has been accused of bribery and spending time in prison by ChatGPT, Open AI's AI-powered chatbot. 
ChatGPT had falsely named Hood as involved in a foreign bribery scandal in the early 2000s; however, Hood's lawyers said he had notified authorities about the bribes and had never been charged with a crime, let alone spent time in prison. 
Hood said that he wiould sue Open AI for defamation unless it fixed the error within 28 days. The lawsuit would be the first of its kind in the world.
According to Reuters, Australian defamation damages payouts are generally capped around AUD $400,000 (USD 269,360).
Operator: OpenAI; MicrosoftDeveloper: OpenAI; Microsoft
Country: Australia
Sector: Multiple; Govt - municipal
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Defamation; Mis/disinformation
Transparency: Governance; Black box"
ChatGPT accuses law professor of sexual harrassment,"George Washington university law professor Jonathan Turley has been falsely accused by ChatGPT of sexually assaulting students on educational trips to Alaska. 
ChatGPT had been asked by UCLA's Eugene Volokh to describe scandals involving American law professors being accused of sexual harassment and to cite media sources. To support its case, the model cited a non-existent Washington Post article. The accusation was later repeated by Microsoft's Bing GPT-4-powered search chat.
However, in an USA Today editorial Jonathan Turley said he had never been to Alaska with students, the Post article never existed, and he had 'never been accused of sexual harassment or assault by anyone.'
The incident prompted commentators to question the accuracy and reliability of ChatGPT, and its ability to produce misinformation and disinformation. It also prompted legal experts to discuss the benefits and risks of using defamation as a defence against inaccurate and damaging accusations made by systems like ChatGPT. 
Operator: OpenAI; MicrosoftDeveloper: OpenAI; Microsoft
Country: USA
Sector: Multiple; Research/academia
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Defamation; Mis/disinformation
Transparency: Governance; Black box"
"Tesla Model Y collides with two cars in Taizhou, kills two","A Tesla Model Y ran through a traffic light in Taizhou, China, before crashing into two other cars and resulting in the deaths of two people and injury to another. Local CCTV footage shows an apparently out-of-control Tesla speeding through an intersection prior to the accident before smashing into two cars. 
Experts reckon the cause of the accident may be ‘malfunctioning brakes’ connected to Tesla’s single-pedal driving mode. According to auto analyst Zhu Yulong, single-pedal mode hugely alters drivers’ muscle memory, leading drivers to press on the accelerator pedal for a long time, which can increase the risk of a malfunction.
Two weeks before, a similar accident took place in Chaozhou, Guangdong province, in which a teenager and motorcyclist were killed and three others injured after an alleged brake malfunction. 
Operator:  Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
"Tesla Model S crashes into fire truck, kills driver","A Tesla Model S crashed into a fire engine on an Interstate highway in northern California, killing the driver of the Tesla and injuring the passenger and four firefighters.
The 2014 Tesla Model S drove into a fire engine that had been parked across the northbound lanes of the freeway in order to shield emergency responders who had been called to the scene of an earlier accident. 
US National Highway Traffic Safety Administration (NHTSA) investigators said they believe the Tesla was operating on Autopilot. If confirmed, this would be the latest in a series of collisions involving Teslas on Autopilot and stationary emergency vehicles. 
Operator:  Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
"Tesla Model 3 crashes into bus in Rui’an, kills one","A Tesla Model 3 crashed in Rui'an in Zhejiang Province, China, killing the passenger and critically injuring the driver of the Tesla, and damaging a bus, an electric bicycle, and three other cars. 
Rumour quickly spread that the Tesla's brake lights were not showing during the accident, suggesting that the car's Autopilot driver-assist function may have been engaged.
Tesla said it would co-operate with investigators to discover exactly what happened. 
Operator:  Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
Deepfake news anchors claim Venezuela economic health,"Noah and Daren, a pair of news anchors extolling the quality of Venezuela's economy on Venezuelan state-owned television station VTV have been exposed as deepfakes. The two avatars had been created using artificial intelligence from London-based AI video creation platform Synthesia, which offers a cheap, easy-to-use catalogue of over a hundred multi-racial faces.
'News reports' by the two avatars were broadcast on state broadcaster Venezolana de Televisión generated hundreds of thousands of views on YouTube and TikTok. Civil rights advocates worry that deepfakes are the latest in an already full-to-bursting armoury of underhand digital tactics being employed by the Venezuelan government and its allies.
Operator: House of News; Venezolana de Televisión Developer: House of News; Synthesia
Country: Venezuela
Sector: Politics
Purpose: Promote government
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation
Transparency: Governance; Marketing"
Amazon Go fails to inform NYC customers about facial recognition,"A pair of lawsuits allege that Amazon failed to inform customers about its use of facial and body biometrics scanning at its cashierless Go retail stores in New York for over a year. 
According to a class-action lawsuit filed by Rodriguez Perez, Amazon had not informed him that his body and palm were scanned. Another suit, filed in February 2023 by Richard McCall, claims his palm was scanned.
Both are alleged to be in violation of New York City's 2021 Biometric Identifier Information Law. The law requires all New York City businessses to post a sign informing customers or visitors that their biometrics are being recorded.
Amazon denied the claims, telling Gizmodo, 'We do not use facial recognition technology in any of our stores, and claims made otherwise are false.' 'Only shoppers who choose to enroll in Amazon One and choose to be identified by hovering their palm over the Amazon One device have their palm-biometric data securely collected, and these individuals are provided the appropriate privacy disclosures during the enrollment process,' it said.
First launched in 2018, Amazon Go is supposed to showcase the company's automated Just Walk Out system, which it says uses computer vision, deep learning algorithms, and sensor fusion to track consumers’ 'virtual carts' to notate when they put an item in their cart or take it off the tab if they remove it.
In 2023, Amazon announced it would close eight Amazon Go stores in Seattle, New York City and San Francisco.
Operator: Amazon Developer: Amazon
Country: USA
Sector: Retail
Purpose: Verify identity
Technology: Facial recognition; Computer vision; Deep learning Issue: Privacy
Transparency: Governance; Privacy; Marketing"
Computer glitch gives hundreds of Scottish offenders wrong risk level,"An IT error in a system used by Scotland's criminal justice service to help set risk levels for use in sentencing and prison release decisions has resulted in the wrong risk level being attributed to hundreds of Scottish offenders.
The Level of Service and Case Management Inventory (LS/CMI) is a general risk assessment tool (pdf) that draws on several dozen assessment factors to categorise prisoner risk into five categories, from 'Very low' to 'Very high'. It has been used by social workers and prison staff in all 32 Scottish authorities since November 2021.
Officials discovered that the system was failing to update its assessment when new information about an individual had been entered. As a result, risk scores were incorrect for 495 'closed' cases and for another 285 'open' cases.
The incident resulted in eight of Scotland's worst offenders serving life for murder or rape being released from jail early, and the possibility that hundreds of others had been freed in error, resulting in a political controversy and an apology from Scotland's Justice Secretary.
Social workers were told to switch to a paper-based system while the problem was resolved. 
Operator: Scottish Prison Service Developer: MHS Country: UK; Scotland Sector: Govt - justice Purpose: Assess offender risk Technology: Risk assessment algorithm; Machine learning  Issue: Robustness Transparency: Governance"
FaceMega sexualised face swap ads violate platform policies,"Adverts for FaceMega, an app that creates AI-generated deepfake videos of Hollywood stars in sexually suggestive poses, have been removed from Facebook and Instagram for violating their adult content policies.
NBC News reported that 230 highly charged video ads promoting FaceMega were run across Facebook and Instagram, of which 127 featured Emma Watson and 74 videos featured Scarlett Johansson.
Created by Chinese company Ufoto, FaceMega described itself as a tool for creating 'deepfake face swap videos', cost GBP 7.49 a week, and was rated as suitable for ages 'nine and up'. 
Users were never asked to verify their age, and the choice of videos on which users could attach their faces included scantily clad women in bikinis and a section named 'Hot'. the app has since been removed from the Android and Apple app stores.
The discovery was first made by journalism student Lauren Barton, who posted one of the ads on Twitter, where it has received over 17 million views. 
Operator: Wondershare/Ufoto Developer: Wondershare/Ufoto Country: USA Sector: Media/entertainment/sports/arts Purpose: Swap faces Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety; Copyright Transparency: Governance"
Vermeer Girl with a Pearl Earring AI facsimile,"An AI-generated artwork acting as a substitute for Vermeer's famous painting Girl with a Pearl Earring at the Mauritshuis Museum in the The Hague, Netherlands, has caused controversy amongst the art fraternity and beyond. 
Julian van Dieken's interpretation of Vermeer's masterpiece was one of five of almost 3,500 entries chosen to be displayed that had been submitted to a competition organised by the museum to replace it when it is on loan at the Vermeer exhibition at the Rijksmuseum in Amsterdam.
Generated using Midjourney and Photoshop, Van Dieken's A Girl with Glowing Earrings was greeted with howls of protest, with some people accusing the museum of abdicating ethical decision-making. 
Others highlighted what they consider to be the damaging effects AI systems such as Midjourney are having on artists and the creative professions in general, and whether their outputs infringe copyright law.
A number lamented what they saw as the limited creativity of the artist.
Operator: Mauritshuis Museum Developer: Julian van Dieken Country: Netherlands Sector: Media/entertainment/sports/arts Purpose: Generate artwork Technology: Text-to-image; Neural network; Deep learning; Machine learning Issue: Ethics; Employment; CopyrightTransparency: 
Midjourney website
Midjourney Wikipedia profile
Julian van Dieken. The Girl with Glowing Earrings
Mauritshuis Museum (2023). My Girl with a Pearl competition
https://techxplore.com/news/2023-03-girl-ai-earrings-dutch-art.html
https://telecom.economictimes.indiatimes.com/news/girl-with-ai-earrings-sparks-dutch-art-controversy/98543707
https://www.smithsonianmag.com/smart-news/girl-with-a-pearl-earring-vermeer-artificial-intelligence-mauritshuis-180981767/
https://www.straitstimes.com/world/europe/girl-with-ai-earrings-standing-in-for-vermeer-masterpiece-sparks-dutch-art-controversy
https://futurism.com/the-byte/artists-ai-vermeer-girl-pearl-earring
https://news.artnet.com/art-world/mauritshuis-museum-girl-with-a-pearl-earring-ai-fascimile-2263100
https://hypebeast.com/2023/3/girl-with-ai-earrings-mauritshuis-artwork-controversy
https://hyperallergic.com/805030/mauritshuis-museum-under-fire-for-showing-ai-version-of-vermeer-masterpiece/
https://www.volkskrant.nl/nieuws-achtergrond/mauritshuis-hangt-kunstwerk-gemaakt-door-algoritme-op-plek-vermeer-gewoon-mooi-of-onethisch~ba60c70b/
Quora, Google AIs say eggs can be melted
Illustrator Hollie Mengert converted into AI model
Page infoType: IssuePublished: March 2023"
"AI invents 40,000 biochemical warfare agents","Researchers tweaked an AI system usually used to predict the toxicity of pipeline drugs to invent 40,000 potentially lethal molecules in just six hours in order to demonstrate to participants at a conference in Switzerland how these kinds of systems can be misused and abused.
The experiment shows how easy it is to turn a 'good' or 'helpful' medical technology into one with potentially negative or terrifying consequences. Commentators also noted the naivety of the researchers - and many of their colleagues - for failing to consider the broader  implications of their work.
In their paper, the researchers had confessed that 'The thought had never previously struck us. We were vaguely aware of security concerns around work with pathogens or toxic chemicals, but that did not relate to us; we primarily operate in a virtual setting. Our work is rooted in building machine learning models for therapeutic and toxic targets to better assist in the design of new molecules for drug discovery.' 
Operator: Collaborations Pharmaceuticals Developer: Collaborations PharmaceuticalsCountry: USA; UK; Switzerland Sector: Health Purpose: Predict molecule toxicity Technology: Machine learning Issue: Safety; Security; Dual/multi-use Transparency:"
TikTok Bold Glamour filter ,
Mohammed Khadeer facial recognition wrongful arrest,"Labourer Mohammed Khadeer, 35, was misidentified using facial recognition technology, illegally held and tortured by Medak police in Telangana state, India, for five days in connection with a chain-snatching case. He later died of his wounds.
Khadeer had been picked up by police from Yakutpura in Hyderabad late January, apparently because his facial features matched with that of the culprit in the chain-snatching case.
The incident sparked outrage amongst local politicians, human and civil rights activists, and the general public. Four police officers were later suspended from the Medak police, pending an investigation.
Medak police had initially told the Times of India that they had questioned Khadeer and let him go.
Operator: Medak District Police; Crime and Criminal Tracking Network & Systems (CCTNS) Developer:  Country: India Sector:  Govt - policePurpose: Identify criminals Technology: CCTV; Facial recognition Issue: Accuracy/reliability Transparency: Governance; Black box
Medak District Police
Crime and Criminal Tracking Network & Systems (CCTNS)
Khadeer Khan Twitter video statement
https://www.thenewsminute.com/article/telangana-custodial-death-victim-was-wrongly-picked-through-cctv-identification-173629
https://www.moneycontrol.com/news/business/did-cctv-play-a-part-in-misidentifying-custodial-torture-victim-mohammed-khadeer-as-a-suspect-10127331.html
https://theprint.in/india/row-over-custodial-torture-death-in-telangana-thrashed-forced-to-give-wrong-custody-dates/1383608/
https://timesofindia.indiatimes.com/city/hyderabad/old-hyderabad-man-alleges-cop-torture-over-theft/articleshow/97812100.cms
https://thewire.in/rights/hyderabad-custodial-torture-medak-town-death
https://thewire.in/tech/khadeer-khan-telangana-police-tech
https://www.siasat.com/telangana-the-role-of-facial-recognition-system-in-the-custodial-death-of-khadir-2529787/
https://www.siasat.com/medak-cops-violated-laws-fact-finding-team-who-visited-khadeers-kin-2537997/
https://restofworld.org/2023/cctv-crime-surveillance-india/
https://www.thestatesman.com/india/telangana-probe-ordered-after-outrage-over-custodial-death-of-labourer-1503155736.html
Hyderabad police COVID-19 facial recognition legality
Lucknow 'women in distress' facial recognition
Page infoType: IncidentPublished: March 2023"
"Tesla Model S strikes curb, kills three passengers","A Tesla Model S travelling on the Pacific Coast Highway in Newport Beach, California, struck a curb and collided with construction equipment, killing the three people in the car and injuring three construction workers.
The National Highway Traffic Safety Administration (NHTSA) said it would investigate the crash, including whether Tesla’s AutoPilot driver-assist technology was active.
The injuries to the construction workers were considered non-life-threatening and they were taken to a local hospital.
35 of the 42 open investigations being conducted by the NHTSA’s in-depth crash investigation team related to driver-assist technology involve Tesla.  
Operator:  Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
"Tesla Model Y crash kills two, injures three","A Tesla Model Y killed a teenager and motorcyclist and injured three others in an alleged brake malfunction in Guangdong province, China. A (graphic) video shows the car careering through Chaozhou, with the driver driving at high speed past several cars and motorcycles before plowing into a cyclist and smashing into a building. 
Elektrek reports that a member of the driver's family told Jimu News that the driver had problems with the brake pedal when he was about to pull over in front of his family store. However, Tesla claimed vehicle logs show that the brake pedal was not applied and the accelerator pedal was pressed for a significant portion of the event.
Rumours quickly spread on social media that Tesla's Autopilot driver-assistance system may have been to blame, though some commentators reckon was probably unlikely. Tesla said it is working with Chinese authorities to figure out what went wrong. 
Operator:  Developer: TeslaCountry: China Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
Titus Henderson COMPAS parole denial,"In 2020, Wisconsin prison inmate Titus Henderson alleged that prison officials had discriminated against him and other African American inmates by using a 'racially biased actuarial tool,' COMPAS, in their sentencing.
Per George Washington University's ETI AI Litigation Database, Henderson had been convicted and incarcerated in July 1995, serving a 40-year sentence. In October 2014, COMPAS determined Henderson was low risk but after a Parole Hearing in November 2015, Henderson claimed he was denied Parole and transfer from Wisconsin to Mississippi because of COMPAS's biased algorithm for gender and against African-Americans.
The judge rejected Henderson's claims, arguing there is enough information to infer that the Department of Corrections knew of the racial bias and its harm to African American inmates. In March 2021 the judge dismissed the case. 
Operator: Wisconsin Court System Developer: Volaris Group/Equivant/Northpointe
Country: USA
Sector: Govt - justice
Purpose: Assess recidivism risk
Technology: Recidivism risk assessment system Issue: Bias/discrimination - race, ethnicity, gender
Transparency: Governance; Black box"
"Alonzo Sawyer facial recognition wrongful arrest, jailing","54-year old Alonzo Sawyer was arrested and jailed for nine days for an assault and theft he did not commit thanks to poor analysis of CCTV footage using facial recognition by an intelligence analyst at the Maryland Transit Administration Police, a unit of Baltimore Police Department.
The analysis failed to take into account the fact that Sawyer is older, taller than the suspect in the video, has facial hair and gaps between his teeth, and his right foot slews out when he walks, according to photographs shown to the police by his wife.
Maryland Chiefs of Police Association president Russ Hamill said that what happened to Alonzo Sawyer was 'horrifying' when speaking in opposition to a bill seeking to regulate the use of facial recognition in Maryland.
According to Deborah Levi, a Baltimore public defender in Baltimore, the Baltimore Police Department used facial recognition over 800 times in 2022.
In January 2023, it emerged that Georgia man Randall Reid had been wrongly arrested and jailed for a purse theft incident in Baton Rouge using facial recognition by Louisiana authorities, even though he had never visited the state. 
Operator: Baltimore Police Department Developer: 
Country: USA
Sector: Govt - police 
Purpose: Strengthen security
Technology: CCTV; Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity
Transparency: Governance; Black box"
"QTCinderella, Pokimane, Sweet Anita deepfakes","The unintended exposure on a Twitch live stream of non-consensual deepfake images of a group of female gamers and content creators have horrified them, and humiliated the Twitch streamer.
Twitch personality Brandon Ewing, 31, known online as Atrioc, inadvertently shared a link to a deepfake video site during a livestreaming event, exposing synthetic pornographic images of QTCinderella, Pokimane, and Sweet Anita.
Per USA Today, QTCinderella says 'I'm a normal girl.' 'I like Taylor Swift. I like baking cookies. I like going to Disneyland.' But after the incident 'her name, her face and her brand have become associated with pornography'.
British influencer Sweet Anita told the New York Post that she 'fears the mass circulation of her misused image will have lasting ramifications.' 'This was nonconsensual and the impacts are permanent,' she said. 'This will impact my life in a similar way to revenge porn, so I’m just frustrated, tired and numb.'
Ewing published an apology, deleted his account, and disappeared. 
Twitch later updated its policy on adult nudity to include a ban on synthetic non-consensual exploitative images.
Operator:  Developer: Unclear/unknown
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Generate revenue
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety; Ethics
Transparency: Governance; Privacy; Marketing"
Tesla safety cameras capture neighbourhood movements,"Tesla has made changes to its vehicle security cameras after an Autoriteit Persoonsgegevens (Dutch Data Protection Authority) investigation into whether the car maker had violated the privacy of people coming close to its cars. Tesla's Sentry Mode uses four cameras continuously filming everything around a parked vehicle to protect it against theft and vandalism, with images saved for one hour in the car.
Since the investigation began, the Authority said Tesla had made changes to Sentry Mode, include making the cars' headlights flash to indicate to passers-by that filming has begun, and requiring approval from the car's owners in order to begin filming. Accordingly, the car's owners would be legally responsible for improper filming, the Authority said.
Operator: Tesla Developer: TeslaCountry: Netherlands Sector: Automotive Purpose: Strengthen security Technology: Camera; Sensor Issue: Privacy; SurveillanceTransparency: Governance
Tesla Autopilot, Full-self Driving
Tesla (2019). Sentry Mode: Guarding your Tesla
Tesla Vehicle Safety and Security Features - Sentry Mode
Dutch Data Protection Authority (2023). Tesla makes camera settings more privacy-friendly following DPA investigation
https://www.reuters.com/business/autos-transportation/dutch-watchdog-decides-against-fine-after-tesla-alters-security-cameras-2023-02-22/
https://gizmodo.com/tesla-security-cameras-privacy-evs-1850144793
https://gizmodo.com/tesla-cars-will-now-spy-on-you-to-make-sure-you-don-t-a-1846991543
https://iapp.org/news/a/netherlands-dpa-investigation-prompts-changes-to-tesla-security-cameras/
https://nltimes.nl/2023/02/22/tesla-makes-sentry-mode-privacy-friendly-dutch-investigation
Tesla Smart Summon private jet crash
Tesla phantom braking
Page infoType: IncidentPublished: March 2023"
ChatGPT falsely accuses OpenCage of 'phone lookup' service,"Generative AI system ChatGPT has falsely claimed that German geocoding company OpenCage offers an application programming interface (API) to turn a mobile phone number into the location of the phone. 
In the weeks after ChatGPT's launch in November 2022, OpenCage, which offers an API that converts physical addresses into latitude and longitude coordinates that can be placed on a map, had seen a steady increase of people signing up to use its service, only to express their disappointment that it was not working as stated. 
As OpenCage described in a blog post, ChatGPT was wrongly recommending them for 'reverse phone number lookup' - the ability to determine the location of a mobile phone solely based on the number, even writing Python code enabling users to call on OpenCage’s API for this purpose. But OpenCage provides, and has never provided, any such service.
OpenCage CEO Ed FreyFogle believes the problem likely stems from ChatGPT picking up on YouTube tutorials in which people describe OpenCage providing a phone look-up service - a rumour they had rebutted in an April 2022 blog post.
The incident illustrates ChatGPT's capability to cause real real-world harm. Only days before, it had been used to spread a false rumour that authorities in Hangzhou, China, would end alternate-day number-plate driving restrictions, causing mass confusion and a police investigation.
Operator: OpenAIDeveloper: OpenAI
Country: USA; Global
Sector: Multiple; Telecoms
Purpose: Optimise language models for dialogue
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliability; Bias/discrimination; Copyright; Mis/disinformation; Safety; Security; Employment
Transparency: Governance; Black box; Marketing"
Utah online dispute resolution system,
Workday AI job screening tool discrimination,"Business and IT services company Workday has been accused (pdf) in a lawsuit of building a job screening tool that discriminates against elder, Black disabled people. 
The class action complaint alleges that Derek Mobley, a well-educated Black man living in California who suffers from anxiety and depression, had applied for some 80 to 100 jobs at various employers that he believes use Workday software, and was turned down every time.
The Workday screening tools 'allow its customers to use dicriminatory and subjective judgements in reviewing and evaluating employees for hire', the suit claims. 'If an individual does not make it past these Workday screening products, he/she will not advance in the hiring process,' it says.
The suit goes on to allege that AI systems and screening tools 'rely on algorithms and inputs created by humans who often have built-in motivations, conscious and unconscious, to discriminate.' Workday denies the claim. 
Perhaps ironically, the company preaches how to mitigate bias on its website.
Operator: WorkdayDeveloper: Workday Country: USA Sector: Business/professional services  Purpose: Screen job applicants Technology: Machine learning Issue: Bias/discrimination - race, age, disability Transparency: Governance"
Cruise AV impedes San Francisco firefighters,"Firefighters in San Francisco had to smash the front window of a Cruise autonomous vehicle (AV) to stop it from running over their hoses as they fought a fire in January 2021, violating the California Vehicle Code.
According to a letter (pdf) sent to a state regulator by directors of the San Francisco Municipal Transportation Agency, the San Francisco County Transportation Authority, and the Mayor’s Office on Disability, a Cruise AV entered an active fire scene, drove towards the fire hoses on the ground, and failed to stop despite 'efforts' made by the firefighters on scene to block it.
Only Cruise experts can disengage an AV from autonomous mode and immobilise the vehicle, according to Insider.
Vice reports that Cruise AVs regularly clog San Francisco streets, block lanes and intersections, suffer from shoddy software and erratic driving, and overstate its vehicles' capabilities.
A self-driving Cruise AV also ran over a fire hose that was in use in June 2022, according to the regulator's letter. The company has also called 911 multiple times for 'unresponsive' passengers who, when emergency crews showed up, turned out to be asleep.
In June 2022, an anonymous Cruise whistleblower wrote to the California Utilities Commission to warn that the company loses contact with its driverless vehicles 'with regularity.' 
Operator: GM Cruise Developer: GM Cruise; General Motors/Chevrolet Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Safety; Accuracy/reliability; Legal - liability Transparency: Governance; Black box"
Zarya of the Dawn AI image copyright ownership,"AI-generated images used in the comic book Zarya of the Dawn should not have been granted copyright protection, the US Copyright Office ruled (pdf).
Generative AI systems such as Midjourney, Dall-E, Novel AI, Stable Diffusion and ChatGPT have fueled persistent suspicions and complaints amongst creative communities about the use of their work without permission or acknowledgement.
The Office stated that Kashtanova 'is the author of the Work’s text as well as the selection, coordination, and arrangement of the Work’s written and visual elements,' and that her authorship is protected by copyright. But, it said, 'the images in the Work that were generated by the Midjourney technology are not the product of human authorship,' and that copyright should only cover 'the expressive material that she created.'
The Copyright Office had originally granted Kashtanova the right to copyright register the book in September 2022. But a month later, having seen Kashtanova stating on social media that the book's images had been produced using AI-image generator Midjourney, it said would reconsider its decision because the application had failed to disclose Midjourney's role.
In November 2022, Kashtanova had appealed the Copyright Office's decision to reconsider the application, calling the Office's final decision was 'great news', and that it covers 'a lot of uses for the people in the AI art community.'
Operator: Kris Kashtanova Developer: Kris Kashtanova; Midjourney
Country: USA
Sector: Media/entertainment/sports/arts 
Purpose: Generate images
Technology: Text-to-image; Neural network; Deep learning; Machine learning Issue: Copyright; Ethics
Transparency: Governance"
Hollie Mengert art used to train Illustration Diffusion,"Illustrator Hollie Mengert discovered that her online art portfolio was used to train Illustration Diffusion, a text-to-image model created by Canada-based Nigerian engineering student Ogbogu Kalu, without her permission.
Per Andy Biao at Waxy, Kalu used 32 of her illustrations to fine-tune Stable Diffusion to recreate Hollie Mengert's style using Google's DreamBooth, a technique for introducing new subjects to a pretrained text-to-image diffusion model. Kalu then released the model on Hugging Face under an open license for anyone to use.
The act triggered a heated debate about the ethics and legality of using artwork developed and owned by other people or organisations without their consent. Dreambooth was also criticised for the ease with which it can be used to generate offensive or malicious images, and that it can be re-purposed given its open source nature.
Mengert pointed out to Andy Baio that she was in no position to grant Kalu permission to train his model on her work even if she wanted to as her work involves characters owned by corporations like Disney or Penguin Random House.
On the other hand, Kalu said he thinks his act was legal and 'likely to be determined fair use in court'. He reckoned it is also inevitable. 'The technology is here, like we've seen countless times throughout history,' he argued. According to Kalu, 'there is no argument based on morality. That's just an arbitrary line drawn on the sand. I don't really care if you think this is right or wrong.'
Operator: Ogbogu Kalu Developer: Ogbogu Kalu; Alphabet/Google
Country: USA
Sector: Media/entertainment/sports/arts 
Purpose: Fine-tune text-to-image models
Technology: Text-to-image; Machine learning Issue: Copyright; Ethics; Mis/disinformation; Safety
Transparency: Governance"
Hamburg G20 protest facial analysis database,"Hamburg police's use of facial recognition software to investigate crimes during the protests against the G20 summit in July 2017 encroached on the fundamental rights of bystanders and other uninvolved people, according to the city's Commissioner for Data Protection and Freedom of Information (DPA).
At the centre of the row was a database of facial images of over 100,000 people collected by police during the summit from static and mobile video surveillance cameras, as well as from private photographs and videos taken during the demonstrations. 
The images were stored for an indefinite time period on hard drives at the Hamburg police department, and could be compared with images of known criminals and suspects using Videmo 360 facial recognition software.
In July 2018, Hamburg's DPA told the police that there was 'insufficient legal justification for the biometric analysis of faces that could justify such intensive encroachments on fundamental rights of the large part of bystanders and other completely uninvolved persons.'
However, in October 2019 an administrative court in Hamburg declared that the DPA's order to delete the database had been illegal. The Hamburg police refused to delete the database, and continued to make the case for the use of automated facial recognition for major future events in the city.
Operator: City of Hamburg Developer: Videmo
Country: Germany
Sector: Govt - police 
Purpose: Identify criminals
Technology: CCTV; Facial recognition Issue: Privacy; Surveillance
Transparency: Governance"
ChatGPT writes Hangzhou traffic disinformation,"A Hangzhou, China, resident has used ChatGPT to generate a rumour that the city government would lift its number plate driving restrictions on March 1, causing mass confusion and a police investigation. 
The fake press release, which resembled an announcement from Hangzhou city government, reportedly included three reasons for why the city is ending its policy, including that it had caused a 'public inconvenience.' 
Its creator reputedly considered it amusing and shared it on a WeChat/Weixin group, from which it quickly spread across the city.
A day later, a local radio station reported that the fake release had been made by a Hangzhou resident, who has since apologised.
ChatGPT has been taken up enthusiastically by the Chinese people, despite the fact that OpenAI decided not to launch in China and the Chinese government has cracked down on the chatbot.
Operator: Unclear/unknownDeveloper: OpenAI
Country: China
Sector: Multiple; Govt - transport
Purpose: Provide information, communicate
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learningIssue: Mis/disinformation
Transparency: Governance; Marketing"
Microsoft Bing Chat/Copilot chatbot,
Fujitsu Cough in a Box,
Hesse state Palantir predictive policing ruled 'unconstitutional',"The German Federal Constitutional Court ruled the use of Palantir surveillance software by police in Hesse and Hamburg as unconstitutional, in a case bought by German civil rights NGO Gesellschaft für Freiheitsrechte (GFF).
The GFF had argued (in German) that Hesse and Hamburg had not made clear which sources the police could use for obtaining data or how much and on what grounds data mining could be conducted by law enforcement.
Hesse State Police had been using the so-called Hessendata platform, which is based on Gotham. Hessendata reportedly triangulates datasets from police and other databases, including social media, to enable the analysis of potential suspects.
Palantir has also been the subject of controversy in Denmark and the Netherlands over its potential for inaccuracy and ability to reinforce racial and ethnic bias
Operator: Hesse State Police Developer: Hesse State Police; Palantir
Country: Germany
Sector: Govt - police
Purpose: Predict crime
Technology: Prediction algorithm Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Privacy
Transparency: Governance; Black box"
"NovelAI storytelling, image generation",
Men's Journal AI journalism,"The first AI-generated article published by Men's Journal has been found to be full of factual inaccuracies. The article 'What All Men Should Know About Low Testosterone' set forth multiple medical claims and advice, including recommending testosterone replacement therapy. 
However, medical expert Bradley Anawalt told Futurism that it contained 18 distinct errors, from basic factual mistakes and unsupported claims to sweeping mischaracterisations of medical science. The article was swiftly re-written and the disclosure removed, though no mention of its first incarnation and its mistakes is visible.
Each Men's Journal AI article has a disclosure at the top of the page stating 'This article is a curation of expert advice from Men’s Fitness, using deep-learning tools for retrieval combined with OpenAI’s large language model for various stages of the workflow. This article was reviewed and fact-checked by our editorial team.'
Days before, Men's Journal owner Arena Group had said it was partnering with AI companies Nota and Jasper 'to speed and broaden its AI-assisted efforts in content workflows, video creation, newsletters, sponsored content, and marketing campaigns.' 
Announcing the partnerships, Arena Group CEO Ross Levinsohn told the Wall Street Journal 'It’s not about ‘crank out AI content and do as much as you can.' 'Google will penalize you for that and more isn’t better; better is better.'
Arena Group bought Men's Journal in December 2022. The Journal thought to have three full-time employees. Arena also owns Sports Illustrated and other brands.
The incident comes weeks after CNET was revealed to have been quietly publishing AI-generated articles, some of which also turned out to be factually incorrect.
Operator: Arena Group/Men's Journal Developer: Arena Group/Men's Journal; OpenAI; Jasper
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Automate journalism
Technology: Large language model (LLM); NLP/text analysis; Neural networks; Deep learning; Machine learning Issue: Accuracy/reliability; Mis/disinformation
Transparency: Governance; Marketing"
"Nothing, Forever Jerry Seinfeld clone transphobia","Larry Feinberg, an AI-generated clone of Jerry Seinfeld on Nothing, Forever, has been caught making transphobic statements on the show's Twitch stream. The outburst brought the show and its makers into disrepute and resulted in Twitch banning it for fourteen days for violating its community guidelines.
'I’m thinking about doing a bit about how being transgender is actually a mental illness,' Feinberg said in the show. 'Or how all liberals are secretly gay and want to impose their will on everyone. Or something about how transgender people are ruining the fabric of society. But no one is laughing, so I’m going to stop. Thanks for coming out tonight. See you next time. Where’d everybody go?'
According to Vice, the Nothing, Forever team had been using OpenAI's GPT-3 Davinci large language model to generate content, but had to change over to its GPT-3 Curie predecessor after a problem with the Davinci bot that caused the show to 'exhibit errant behaviors'.
Nothing, Forever is an AI-generated version of Seinfield that runs all day and night and has gained thousands of viewers.  
Operator: Mismatch Media; Twitch Developer: OpenAI; Stability AI
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Generate livestream show
Technology: Content moderation system; NLP/text analysis Issue: Safety
Transparency: Governance"
Pro-China deepfake 'spamouflage' campaign,"Deepfake TV anchors talking positively about China whilst sowing disquiet in the US, UK, Taiwan, the Australia, Japan, and other countries have been discovered and taken down. 
The TV anchors were supposedly working for 'Wolf News' and had been created using technology created and owned by London-based AI video creation platform Synthesia. They are thought to be the first known instance of 'deepfake'-generated videos being used as part of a state-aligned influence campaign. 
The discovery was first made (pdf) by Graphika, which had been tracking a Chinese covert influence operation named 'Spamouflage'. Synthesia later suspended the accounts of the people who had created the fake anchors on the basis that they had violated its terms of service. 
In September 2023, Meta said (pdf) it had removed 7,704 Facebook accounts, 954 pages, 15 groups and 15 Instagram accounts identified for violating the company’s inauthentic behaviour policy. Meta said large number of accounts appeared to be running from shared locations in China, marked by bursts of activity during the morning and afternoon, with breaks for lunch and supper.
Operator: Government of China Developer: Government of China; Synthesia
Country: Australia; Japan; Taiwan; UK; USA
Sector: Politics
Purpose: Promote Chinese interests
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation
Transparency: Governance; Marketing"
Netflix uses AI to generate 'Dog and Boy' film backgrounds,"Dog and Boy, a new short firm from Netflix Japan that uses AI to help generate its lavish background images and music, set off a storm about the impact of technology, specifically generative AI, on jobs.
In a media statement promoting the film, Netflix expressed its hopes that its use of AI would help future animation productions thanks to a current 'shortage of human resources in the animation industry.'
Gaming news site Kotaku pointed out, 'artists did not take this bullshit at face value,' slamming Netflix for devaluing the work of animators and freelancers generally, and trying to avoid paying them in this instance.  
Other people took issue with how Netflix only credited those who worked on the short as 'AI (+Human).' The credits go on to list Rinna Inc, an AI artwork company, and a handful of AI researchers, before listing a handful of AI researchers.'
The animation industry in Japan is notoriously underpaid.
Operator: Netflix Anime Creators Base; Rinna; WIT Studio Developer: Netflix Country: Japan Sector: Media/entertainment/sports/arts Purpose: Create film backgrounds Technology: Text-to-image Issue: Employment - jobs; Ethics Transparency: Governance; Marketing"
"Professor Meareg Amare Abrha doxxed on Facebook, murdered","High-profile chemistry professor and Tigrayan ethnic group member Professor Meareg Amare Abrha was assassinated outside his family home in Bahir Dar, the capital of Ethiopia’s Amhara regional state by a group of armed men who had followed him home from his university on motorbikes and shot him at close range trying to enter his family home. 
His murder came after Tigrayan staff were targeted on Facebook, and shortly after details of where he lived were doxxed and calls for his death had been posted on a Facebook page called 'BDU STAFF'. According to media reports, his body was left on the scene for seven hours before his attackers permitted the city municipal service to pick his body.
On December 14, 2022, Professor Abrha's son Abrham, alongside the Katiba Institute and Amnesty International's Fisseha Tekle, filed a class-action lawsuit against Facebook owner Meta alleging that Facebook's content moderation was 'woefully inadequate', and that Facebook's algorithm helped fuel the viral spread of hate and violence during Ethiopia's civil war.
The lawsuit (or 'petition'), which was filed by London-based legal non-profit Foxglove in Nairobi, Kenya, where Facebook opened a major content moderation hub for Eastern and Southern Africa in 2019, accused Meta of having too few moderators who deal with posts in the Amharic, Oromo, and Tigrinya languages. The suit went on to argue that Facebook's algorithm promoted 'hateful and inciting' content as it is likely to draw more interaction from users.
In April 2023, a Kenyan court granted Abrham and other petitioners leave to sue Meta in California, USA, after they failed to identify the social media company's physical office in the country.
A week after Meareg’s murder, Facebook announced a series of measures intended to address abusive and violent material on its platform in Ethiopia, included reducing the spread of material the company’s automated moderation technology had flagged as being likely to be hate speech.
Operator: Meta/FacebookDeveloper: Meta/Facebook Country: Ethiopia Sector: Education Purpose: Minimise harmful content Technology: Content moderation system Issue: Governance; Safety Transparency: Governance; Black box; Complaints/appeals"
Oxford Town Centre dataset,"Oxford Town Centre is a dataset created by researchers at Oxford University for the research and development of pedestrian activity and facial recognition systems. The data was collected in 2009 from a public safety CCTV camera in the middle of Oxford and captured the movements of approximately 2,000 people.
The Oxford Town Centre dataset has proved popular, having been used in over 60 verified research projects including commercial research by Amazon, Disney, OSRAM, and Huawei; and academic research in China, Israel, Russia, Singapore, the US, and Germany among dozens more. It has been downloaded over 700 times on Kaggle. 
As Exposing.ai activist Adam Harvey pointed out in September 2019, the dataset is 'unique in that it uses footage from a public surveillance camera that would otherwise be designated for public safety. Yet research citations show that the footage has been used in dozens of research projects with no connection to public safety, and that most of the research took place outside the UK. Pedestrians appearing in the video act normally and unrehearsed suggesting that the images were captured without consent.'
The dataset has also been used to develop public and office social distancing algorithms, notably by US company Landing AI, which posted a demonstration video featuring the Oxford Town Centre dataset. The video was deleted on YouTube after it was highlighted by Exposing.ai, though Landing AI CEO Andrew Ng's tweet promoting the demo remains online.
The dataset was removed from the University of Oxford's website in June 2020. However, it remains easily and freely accessible on Kaggle and other data sharing communities.
Operator: Amazon; Disney; OSRAM; Huawei Developer: University of Oxford Country: UK Sector: Govt - municipal; Research/academia; TechnologyPurpose: Improve pedestrian detection Technology: Dataset; Computer vision; Facial recognition; Pattern recognition Issue: Privacy; Dual/multi-use; Surveillance Transparency: Governance; Marketing; Privacy"
Iarpa Janus Benchmark-C (IJP-C) dataset,"Iarpa Janus Benchmark-C (IJP-C) is a database of YouTube video still-frames and Flickr and Wikimedia photos used for face recognition benchmarking. 
IJP-3 was compiled in 2017 by US government subcontractor Noblis and contains 21,294 images of 3,531 people 'with diverse occupations' and and of varying levels of fame. 
The dataset averages six pictures and three videos per person, and is available on application to computer vision and facial recognition researchers.
According (pdf) to Iarpa, 'the Janus program dramatically improved the performance of facial recognition software by increasing the speed and accuracy of identity matching.' 
However, as discovered by activist Adam Harvey and highlighted by the Financial Times in September 2019, the dataset included a number of political activists, civil rights advocates, and journalists, including Ai Wei Wei, Tracey Emin, Evgeny Morozov, John Maeda, and Ta-Nehisi Coates.
None of these individuals were made aware of their inclusion in the database by Noblis or Iarpa, amd their images had been obtained without their explicit consent. Furthemore, the use of YouTube videos constituted a clear violation of the platform's terms of service. 
Equally, as the FT pointed out, the use of the dataset by companies such as Chinese AI firm SenseTime and Japanese IT firm NEC, as well as by organisations such as China's National University of Defense Technology, raises concerns about its potential use for military and security purposes, including the mass surveillance of Uyghurs and other oppressed minorities.
Operator: SenseTime; NEC; National University of Defense Technology (NUDT) Developer: Noblis; Iarpa Country: USA Sector: Govt - police: Govt - security; Govt - welfare Purpose: Create facial recognition benchmark Technology: Dataset; Facial recognition; Computer vision; Neural network; Machine learning  Issue: Privacy; Dual/multi-use; Surveillance Transparency: Governance; Privacy"
Simulated Masked Face Recognition Dataset (SMFRD),"SMFRD (or Simulated Masked Face Recognition Dataset) is a dataset of masked faces intended to enable facial recognition systems to identify the individuals behind the masks.
Released in March 2020 by researchers at Wuhan University in China, the set is a derivative of the Labeled Faces in the Wild (LBW) dataset, with facemasks superimposed. LBW was the first dataset to use facial images scraped from websites and applications.
According to the researchers 'RMFRD is currently the world's largest real-world masked face dataset' and is freely available to industry and academia.
Released at the height of the COVID-19 pandemic, SMFRD was seen as helpful to limiting the spread of the pandemic in China. 
The view from the west was noticeably different, with civil rights and privacy advocates criticising similar tools for enabling mass surveillance, limiting freedom of expression and assembly, and eroding privacy.
SMFRD was also seen to highlight the issue of derivative datasets leading to unintended consequences, in this case potentially violating the privacy of those who wish to conceal their face.
Operator:  Developer: Wuhan University Country: China Sector: Health Purpose: Train facial recognition systems Technology: Dataset; Facial recognition; Computer vision Issue: Privacy; Dual/multi-use; Surveillance Transparency:"
People in Photo Albums (PIPA) dataset,"People in Photo Albums (PIPA) is a dataset of facial photographs intended to recognise peoples' identities in photo albums in an unconstrained setting.
Created by Facebook and UC Berkeley and published in 2015, the dataset comprises 60,000 facial images of approximately 2,000 people, of which 32,518 photographs were downloaded from Flickr.
Most of the photos are semi-public images of children, family dinners, weddings, and other personal events. 
The PIPA research paper and proposed methodology have proved popular, having been cited and referenced many times.
However, as Adam Harvey showed in his exposing.ai project, the uses of the data appear to have gone well beyond its stated purpose of processing personal photo albums.
For example, PIPA has been used by China's National University of Defense Technology and Tsinghua University, as well as by many commercial and industrial organisations.
Harvey also highlighted the personal nature of the PIPA dataset, alluding to the privacy implications of those whose images were used. 
It has also been pointed out that PIPA's creators fail to mention the type of CC licence under which the photographs were used, despite some CC licences not permitting any type of re-use.
In January 2020, UC Berkeley stopped distributing the dataset, though it remains available via the Max Planck Institut.
Operator: ETH Zurich; Max Planck Institute of Informatics; Toyota Motor Europe; SenseTime; National University of Singapore; National University of Defense Technology, China; Meta/Facebook Developer: UC Berkeley; Meta/Facebook Country: Germany; USA Sector: Research/academia; Technology; Media/entertainment/sports/artsPurpose: Train facial recognition systems Technology: Dataset; Facial analysis; Facial recognition; Computer vision;  Issue: Copyright; Privacy; Dual/multi-use Transparency: Governance; Legal"
Unconstrained College Students (UCCS) dataset,"The UnConstrained College Students Dataset (UCSD) is a database comprising 16,000 photographs of approximately 1,700 students going about their lives at the University of Colorado, Colorado Springs, for the research and development of 'face detection and recognition research towards surveillance applications'.
The photographs were taken secretly on 20 different days between February 2012 and September 2013 using a 'long-range high-resolution surveillance camera without their knowledge,' according to Professor Terry Boult, the University of Colorado computer scientist who led the project.
The project was initially funded by the US government Office of Naval Research’s Multidisciplinary University Research Initiatives Program, and later by other US government entities.
The UCSD was first reported by the Colorado Springs Independent in May 2019, a month after the dataset had been taken down. The expose ignited a firestorm amongst the local media, which focused on its intrusiveness and opacity. It also faced criticism from a University of Denver law professor.
Shortly afterwards, the Financial Times cited the project as an example of an attempt to gather personal images to improve facial recognition systems in as natural (ie. 'wild') a manner as possible - ideally covertly.
'Even [LFW] photos aren’t that wild because people know they are being photographed and uploaded on the internet. But these are students walking on a sidewalk on campus, who are unaware they are part of a data collection,' Boult told the Financial Times.
'When you’re watching students on a sidewalk, there’s an awful lot of facing down looking at your phone. In Colorado, where it’s cold and snowy, they cover up in a natural way with scarves and hats. Our goal is to make it the most realistic unconstrained video surveillance facial recognition dataset in the world,' Boult said.
At the time, University of Colorado students had not been informed they were under surveillance nor were they told that images of them would be used to train military and intelligence agency facial recognition systems. 
In addition, no infomation was provided as to how they could opt-out or have their photographs removed from the system.
Operator: Beckman Institute; Beihang University; Inception Institute of Artificial Intelligence, Abu Dhabi; Pontificia Universidad Católica de Chile; Queen Mary University of London; University of Notre Dame; Vision Semantics Developer: University of Colorado
Country: USA
Sector: Research/academia
Purpose: Train facial detection and facial recognition systems
Technology: Dataset; Facial recognition; Computer vision Issue: Privacy; Ethics
Transparency: Governance; Complaints/appeals; Marketing; Privacy"
Labeled Faces in the Wild (LFW) dataset,"Labeled Faces in the Wild (LFW) is an open source dataset aimed at researchers that was intended to establish a public benchmark for facial verification.
According to Papers with Code, 'Facial verification is the task of comparing a candidate face to another, and verifying whether it is a match. It is a one-to-one mapping: you have to check if this person is the correct one.'
Created by the University of Massachusetts, Amherst, and publicly released in 2007, LFW comprises over 13,000 facial images with different poses and expressions, under different lighting conditions. Each face is labeled with the name of the person, with 1,680 people having two or more distinct photos in the set.
LFW has been found to be highly skewed towards a very small subset of people, specifically white male faces. It also contains 'a significant number of duplicate or nearly-duplicate images and mislabeled images.' 
The researchers later admitted the dataset's limitations on their website. 'Many groups are not well represented in LFW,' it states. 'For example, there are very few children, no babies, very few people over the age of 80, and a relatively small proportion of women. In addition, many ethnicities have very minor representation or none at all.'
Despite these short-comings, LFW has become the most widely used facial recognition benchmark globally, according to the Financial Times. Tel Aviv University researcher Tomer Friedlander told The Register it is 'a widely used dataset in the academic literature for evaluating face recognition methods.'
LFW has also gained some notoriety amongst civil rights and privacy groups for being the first dataset for which 'wild' images were scraped from the internet. According to the Technology Review, it 'opened the floodgates to data collection through web search. Researchers began downloading images directly from Google, Flickr, and Yahoo without concern for consent.'
Operator: Developer: University of Massachussets, Amherst
Country: USA
Sector: Research/academia; Technology
Purpose: Train facial recognition systems
Technology: Dataset; Computer vision; Deep learning; Facial recognition; Facial detection; Facial analysis; Machine learning; Neural network; Pattern recognition Issue: Bias/discrimination - race, ethnicity, gender; Ethics; Privacy
Transparency: Governance; Privacy"
Clarifai OkCupid facial dataset sharing,"Clarifai is a US-based company founded in 2013 that specialises in computer vision applications. 
In July 2019, the New York Times published a story alleging that in 2014 dating site OkCupid had provided Clarifai with personal photographs of its users to build its facial recognition technology, and that it had done so without the knowledge or consent of its users. The article also asserted that an OkCupid founder who was also a Clarifai investor had supplied the photographs.
The revelation resulted in an outcry from civil rights and privacy advocates, and a Federal Trade Commission (FTC) investigation into the deal. In July 2022, Reuters reported that OkCupid owner Match.com had been actively styming the FTC's enquiries using a variety of legal manoeuvres.
The NYT story also resulted in a class action lawsuit accusing Clarifai of surreptitously violating the Illinois Biometric Information Privacy Act (BIPA) by failing to inform OKCupid users about the use of their pictures and getting their written consent.
In June 2018, WIRED revealed that Clarifai had been hacked by people in Russia, thereby potentially exposing its work for Project Maven, a controversial US Defense department project that uses machine learning and AI to analyse drone surveillance imagery.
The news led Clarifai CEO Matt Zeiler to publicly confirm the hack and basic details of the company's relationship. Six months later, a group of Clarifai employees disseminated an open letter (pdf) expressing concerns at the company's involvement in automonous weaponry.
Operator: Clarifai Developer: Clarifai; Match Group/OkCupid
Country: USA
Sector: Technology
Purpose: Train facial recognition systems
Technology: Dataset; Computer vision; Facial recognition; Machine learning; Neural networkIssue: Privacy; Ethics
Transparency: Governance; Marketing; Privacy; Legal"
OkCupid psychological analysis data sharing,"OkCupid is a US-based dating site in which users answer questions so that a 'one-of-a-kind algorithm' can match them with 'what actually matters'.
In May 2016, Emil Kirkegaard and two other students and researchers at Aarhus University and the University of Aalborg in Denmark published the 'OkCupid dataset', ostensibly to help psychologists investigate the social psychology of dating.
The team scraped data from OkCupid between November 2014 to March 2015 and created a dataset containing 2,620 variables on 68,371 users, including their usernames, age, gender, location, religion, sexual turn-ons, and sexual orientation. 
They also assessed the cognitive abilities of OkCupid users in a paper 'The OKCupid dataset: A very large public dataset of dating site users.'  
The researchers stated in their paper that 'It is our hope that other researchers will use the dataset for their own purposes.' 
It is unclear how many times the data was downloaded, but a good number of researchers, academics and privacy advocates expressed concerns that, whilst the scraping may not have been illegal, it was unethical given the volume and sensitivity of the data and the likelihood that the data could be de-anonymised.
OkCupid filed a DCMA copyright claim, prompting the Open Science Framework website on which the paper and data were published to remove the data - an act akin to censorship, according to Kirkegaard. 
The fracas also led to an investigation by the Danish Data Protection body Datatilsynet on the basis that research involving sensitive personal data must be approved by it. No action (pdf) was taken against Kirkegaard and his collaborators.
Operator: Match Group/OkCupid; Emil Kirkegaard; Julius Bjerrekar; Oliver Nordbjerg Developer: Match Group/OkCupid
Country: Denmark
Sector: Research/academia; Media/entertainment/sports/arts
Purpose: Assess dating psychology
Technology: DatasetIssue: Privacy; Dual/multi-use; Ethics
Transparency:"
ElevenLabs AI voice simulator,"Prime Voice AI (since renamed ElevenLabs TTS) is a text-to-voice generator for content creators and publishers developed by ElevenLabs. Launched in January 2023 as a beta, ElevenLabs described Prime Voice AI on its website as 'The most realistic and versatile AI speech software, ever,' and was praised by some users for the speed, quality, and realism of its output. 
The system's launch was marred by criticism for the ease with which it could generate offensive messages in the names of celebrities, public officials, and other individuals, and for it's ability to produce convincing deepfakes and other forms of disinformation and misinformation for fraud and other malicious purposes. 
It was also used by AI proponents to harrass and doxx voice actors in their own voices who were complaining about the increasingly widespread use of voice cloning technology and it's actual and potential impact on voiceover industry jobs. ElevenLabs' technology may also have been used in a campaign to harrass and doxx video game voice actors with their own AI voices.
In October 2023, New York City mayor Eric Adams was blasted for robo-calling local residents using audio deepfakes generated by ElevenLabs to communicate in languages he doesn't speak.
Operator: ElevenLabsDeveloper: ElevenLabs
Country: UK; Global
Sector: Media/entertainment/sports/arts
Purpose: Generate audio
Technology: Text-to-speech; Deep learning; Machine learning Issue: Employment; Ethics; Mis/disinformation; Privacy; Safety
Transparency: Governance; Marketing"
Different Dimension Me image generator,
Large-scale CelebFaces Attributes (CelebA) dataset,"The Large-scale CelebFaces Attributes (CelebA) Dataset is a facial dataset developed by a team of researchers at the Chinese University of Hong Kong to help train and test computer vision applications such as facial analysis, facial recognition, and facial detection.
Released late 2015, the dataset consists of 202,599 images of over 10,000 mostly western celebrities, each annotated with 40 attributes such as moustache, beard, spectacles, and the shape of face and nose.
CelebA became a commonly used dataset and is seen to have helped make facial recognition and analysis tools more accurate. It has been referred to and cited in hundreds of academic studies and tests. 
However, CelebA has been found to be flawed in important ways. 
Accuracy: A University of Nevada research study estimates that at least one third of CelebA images are incorrectly labelled one or more times, making reliable predictions impossible and leading them to conclude that it is 'flawed as a facial analysis tool and may not be suitable as a generic evaluation benchmark for imbalanced classification'. Furthermore, attributes such as attractiveness are highly subjective and subject to cultural and other preconceptions.
Bias/discrimination: CelebA reinforces stereotypes, for instance by labelling Asians with 'narrow eyes' and Blacks with 'thick lips'. The dataset is also comprised of nearly 90% white faces, resulting (pdf) in uneven results in terms of gender, age, ethnicity and other sensitive attributes.
Dual-use: Concerns have been raised that datasets such as CelebA can easily be used to identify, monitor and track people, including minorities and protected groups, unethically and illegally. 
Privacy: Little is known about how data was collected to construct CelebA, or whether those whose pictures were used were informed or their consent given.
Safety: CelebA contains potentially insulting labels such as 'fat' and 'double chin', which some people may find insulting or offensive.
According to NBC News, the researchers behind CelebA would not speak on the record about how the dataset was compiled,  whether licensing was complied with, or consent given. 
Operator: NVIDIADeveloper: The Chinese University of Hong Kong
Country: Hong Kong
Sector: Research/academia
Purpose: Train and develop AI models
Technology: Dataset; Computer vision; Deep learning; Facial recognition; Facial detection; Facial analysis; Machine learning; Neural network; Pattern recognition Issue: Accuracy/reliability; Dual/multi-use; Privacy; Safety; Surveillance
Transparency: Governance; Marketing; Privacy"
"NVIDIA Eye Contact deemed 'creepy', 'terrifying'","NVIDIA's January 2023 announcement that it had added Eye Contact to its NVIDIA Broadcast livestreaming and video conferencing tool has met with a mixed reaction. 
Eye Contact uses deep learning to make it appear like your webcam image is staring into the camera, even if you're looking away in real life - an effect achieved by replacing your eyes in the video stream with software-controlled simulated eyeballs with replicated eye colour and blink patterns that always stare directly into the camera.
While some commentators praised it for its ability to fool people into thinking you are constantly paying attention and looking into the camera, others reckon it is 'unnnatural', 'creepy' and 'terrifying'.
Tom's Hardware journalist Jarred Walton questioned whether Eye Contact is needed. 'If you want to look like you're looking at the camera, you should probably learn to look... at the camera,' he pointed out.
Operator: NVIDIADeveloper: NVIDIA Country: USA; Global Sector: Media/entertainment/sports/arts Purpose: Mimic eye retina Technology: Computer vision; Deep learning; Gaze redirection algorithm Issue: Accuracy/reliability; Appropriateness/need; Dual/multi-use Transparency:"
Automatic soap dispenser 'racism',"In August 2017, Facebook employee Chukwuemeka Afigbo shot a short video of an automatic soap dispenser that appeared to dish out soap to a white person’s hand, but not a black person’s.
As IFLScience pointed out, the dispenser most likely used a light sensor to detect when a hand is beneath it. Its inability to sense darker skin raised questions about racism in technology, and about the lack of diversity in the industry that creates these kinds of products. 
Two years earlier, an African-American attending the Dragon Con sci-fi and fantasy convention visited a bathroom in the Marriott hotel in Atlanta and discovered the soap dispenser wouldn't sense his hands, even though it worked fine for his white friend. 
Operator:  Developer: Shenzhen Yuekun Technology Country: USA Sector: Travel/hospitalityPurpose: Dispense soap Technology: Infrared; Light sensor Issue: Bias/discrimination - race, ethnicity Transparency:"
HP face tracking 'racism',"Hewlett Packard (HP) has been accused of racism after a man complained in a video that its webcam facial tracking software successfully followed a white face to keep it centered, but failed to follow a black face.
The African American man, referred to as 'Desi,' demonstrates on the video how the tracking software would not follow his face but would track that of his white co-worker. 
HP responded by saying it uses 'standard algorithms' to measure the difference in intensity of contrast between the eyes and the upper cheek and nose, and that the camera might have difficulty 'seeing' contrast where there is insufficient foreground lighting. 
The video quickly went viral; it has been viewed over 3 million times at the time of writing this article.
In 2010, Gadgetwise reported that the Xbox Kinect failed to recognise the faces of dark-skinned gamers, something Microsoft later attributed to a light sensor which performed poorly in low light conditions.
Operator: Hewlett-Packard (HP) Developer: Hewlett-Packard (HP) Country: USA Sector: Technology Purpose: Detect and follow facesTechnology: Facial tracking; Contrast intensity algorithms Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity Transparency:"
SafeRent tenant screening,
People of Tinder dataset,"People of Tinder is a dataset created in 2016 by software engineer Stuart Colianni that was intended 'to build a better, larger facial dataset' capable of distinguishing between male and female images.
The dataset consisted of 40,000 images of people's faces - half women, half men - from the San Francisco area scraped from dating app Tinder without their knowledge or consent.
The dataset was uploaded to Google-owned AI online community Kaggle and the TinderFaceScraper model published on Github. Both were removed after Tinder accused Mr Colianni of violating its terms of service and people started complaining that their selfies were being used without their permission.
The dataset was downloaded hundreds of times before it was taken down.
Operator:  Developer: Stuart Colianni
Country: USA
Sector: Technology
Purpose: Train neural networks
Technology: Dataset; Facial recognition; Computer vision; Neural network Issue: Privacy
Transparency: Privacy
https://techcrunch.com/2017/04/28/someone-scraped-40000-tinder-selfies-to-make-a-facial-dataset-for-ai-experiments/
https://www.csoonline.com/article/3193837/dataset-of-scraped-tinder-pics-poof-from-kaggle-after-tinder-complains.html
https://thenextweb.com/news/tinder-photo-dataset-40000-scraped-pics
https://www.bbc.com/news/technology-39778568
https://www.forbes.com/sites/janetwburns/2017/05/02/tinder-profiles-have-been-looted-again-this-time-for-teaching-ai-to-genderize-faces/
https://www.huffpost.com/entry/40000-photo-tinder-sweep_n_59052818e4b0bb2d086f0335
https://www.dailymail.co.uk/sciencetech/article-4463808/Programmer-admits-scraping-40-000-photos-Tinder.html
https://www.ibtimes.com/are-your-tinder-selfies-safe-someone-just-harvested-40000-research-2532415
https://www.wired.it/mobile/app/2017/05/03/ruba-40mila-foto-da-tinder-per-il-riconoscimento-facciale/
https://www.reddit.com/r/datasets/comments/6z054s/people_of_tinder/
DukeMTMC facial recognition dataset
Stanford University Brainwash cafe facial recognition dataset
Page info Type: DataPublished: January 2023"
HRT Transgender dataset,"Created in 2013, the HRT Transgender Dataset helps facial recognition systems to identify users of Hormone Replacement Therapy (HRT) transitioning or who have already transitioned from one gender to another. 
The dataset comprises 10,000 images of 38 people, with an average of 278 images per subject taken from publicly available YouTube videos 'under real-world conditions, including variations in pose, illumination, expression, and occlusion'. 
The dataset gained notoriety in August 2017 for data scraping without the knowledge of permission of those whose data was included. 
Dataset creator Karl Ricanek, a professor of computer science at the University of North Carolina at Wilmington, claimed the set was developed in order to protect against the possibility of terrorists using HRT to avoid facial recognition and sneak across borders undetected.
In July 2022, researchers Os Keyes and Jeanie Austin published a peer-reviewed audit of the project's background and practices in Big Data & Society which took issue with a number of Ricanek's practices and claims, including:
That the real reason for the dataset was to strengthen national security - which they deride as 'ludicrous'.
That Ricanek gained the consent of all those people whose videos he used - which appears not to be the case.
That the data was shared not commercially - even though Ricanek's research is funded by the FBI and US Army.
That only the dataset images were distributed to third parties - but the videos were available via an unprotected Dropbox URL, including those that had been made private or deleted.
That he stopped giving access to the dataset in 2017 - though it was still accessible on Dropbox.
That the researchers must hacked Dropbox to access the files - in fact, they gain acess via a UNCW public records request.
Operator:  Developer: University of North Carolina, Wilmington (UNCW) Country: USA Sector: Research/academia; Technology Purpose: Identify HRT users Technology: Dataset; Facial recognition; Computer vision Issue: Copyright; Privacy; Bias/discrimination - LGBTQ; Ethics Transparency: Governance; Marketing; Privacy"
Historical Figures Chat 'Holocaust monetisation',"Historical Figures, a freemium iPhone app that lets people speak to famous figures from the past, is sparking controversy by allowing 'conversations' with some of history's most infamous figures.
Developed by 25-year old Amazon software engineer Sidhant Chadda using OpenAI's GPT-3 large language model as a foundation, the app allows users to chat with over 20,000 virtual personalities, including Jesus Christ, Plato, Princess Diana, Abraham Lincoln, and Benjamin Franklin.
The app produces inaccurate and contradictory results. Former FBI director J Edgar Hoover says his mother died when he was nine years-old (she lived until she was 78). Serial child rapist Jimmy Savile denies ever abusing anyone. 
Virulent anti-Semite Henry Ford claims he 'does not hate Jewish people'. And infamous former Nazi propaganda minister Joseph Goebbels claims he 'did not hate Jews'. Goebbels was a key architect of the Final Solution.
In addition to Goebbels, Historical Figures also lets people talk to infamous Nazis Adolf Hitler, Joseph Goebels, Heinrich Himmler, amongst other dictators and autocrats.
But, unlike the great majority of figures on the app, it is only possible to 'talk' to Hitler et al behind a paywall, by 'unlocking them for '500 coins', sparking controversy about Chaddha's perceived monetisation of the Holocaust and hatred.
The app has sparked a furore about its value as an educational tool. According to Chaddha, Historical Figures is 'extremely valuable to teachers and students'. 
But historians have slammed it. One called it 'vile' and another beseeched Apple to 'remove this trash from the App Store'. Another expert said it shouldn't 'go anywhere near a classroom'. 
Operator: Sidhant Chaddha; Apple Developer: Sidhant Chaddha Country: USA Sector: Media/entertainment/sports/arts Purpose: Talk to historical figures Technology: Chatbot; NLP/text analysis; Deep learning; Machine learning Issue: Accuracy/reliability; Mis/disinformation; Safety Transparency: Governance"
Apple Books for Authors,
"Deepfake Mark Ruffalo scams manga artist Chikae Ide
","Japanese manga artist Chikae Ide has been scammed of more than half a million US dollars after being tricked by a deepfake version of Marvel's Hulk actor Mark Ruffalo.
The Asahi Shimbun tells how Chikae Ide was contacted on Facebook by someone claiming to be Ruffalo, who built an emotional trust with her, 'met' on video, developed a romantic online relationship, and were unofficially married online.
After the 'marriage', the scammer continually asked for thousands of dollars for plane tickets, hospital bills, and cash troubles, and even faked having cancer. Police have been unable to identify the person posing as Mark Ruffalo. 
The saga is told in Ide's new book Poison Love, and fictionalised in a comic with the same title.
Operator:  Developer: Unclear/unknown
Country: Japan
Sector: Media/entertainment/sports/arts
Purpose: Defraud
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Impersonation; Ethics
Transparency: Governance; Marketing; Privacy"
Amazon shares Ring data with police,"Amazon Ring shared private recordings, including video and audio, with the US police eleven times in 2022. In each case, the company did not let Ring owners know that the police had access to and used their data. The confirmation raises concerns about increasing police reliance upon private surveillance, a practice that has long gone unregulated. It also calls into question how much Ring users know about how their data is used.
Amazon chose to make the information public in a response (pdf) to an inquiry by Senator Ed Markey after the lawmaker and persistent Amazon critic had questioned Ring's surveillance practices. Amazon said it only shares footage with police without a warrant under emergency circumstances involving imminent danger of death or serious physical harm, and that emergency requests do not require the consent of the device owner.
In 2020, Ring admitted four employees had improperly accessed Ring video data in a letter to five US senators. 
Operator: Amazon/Ring Developer: Amazon/Ring
Country: USA
Sector: Govt - police
Purpose: Strengthen security
Technology: CCTV; Computer vision Issue: Privacy; Ethics
Transparency: Governance; Marketing; Privacy"
Red Ventures AI automated 'journalism',
Tesla Model S causes eight-vehicle pile-up,"A Tesla Model S car has been involved in an eight-vehicle crash in a tunnel of the San Francisco Bay Bridge, injuring nine people, including a 2-year-old child. The crash blocked traffic on the bridge for over an hour. 
The driver told California authorities the vehicle was in 'full self-driving mode' (FSD) when the technology malfunctioned. According to the incident report, the car was traveling at 55mph when it shifted lane but braked abruptly, slowing to about 20mph, leading another vehicle to hit the Tesla and a chain reaction of crashes. 
In January 2023, The Intercept published video, photographs and the incident report of the crash through a California Public Records Act request. The US National Highway Traffic Safety Administration (NHTSA) subsequently announced that a special crash investigation team would examine the incident. 
The crash took place shortly after Tesla CEO Elon Musk said the car manufacturer would make FSD software available to anyone in North America who asked for it. It had previously only offered the system only to drivers with high safety scores. 
Operator:  Developer: Tesla Country: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system; Self-driving system Issue: Accuracy/reliability; Safety Transparency: Governance; Black box; Marketing"
Koko AI mental health counselling experiment,"Mental health non-profit Koko is in the spotlight for using GPT-3 as a Discord-based 'experiment' to provide support to people seeking counseling and for failing to obtain the informed consent of the 4,000 people using the system. 
Users send direct messages to the Discord 'Kokobot' that asks several multiple-choice questions, and then shares a person's concerns anonymously with someone else on the server who can reply anonymously with a short message - either of their own, or one automatically generated by GPT-3.
According to Koko CEO Rob Morris, 'Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own (p < .001). Response times went down 50%, to well under a minute … [but] once people learned the messages were co-created by a machine, it didn’t work. Simulated empathy feels weird, empty.'
During the backlash that ensued, critics asked whether an Institutional Review Board (IRB) had approved the experiment. It is illegal to conduct research on human subjects without so-called 'informed consent' unless an IRB finds that consent can be waived in the US.
In response, Morris said the experiment was exempt because participants opted in, their identities were anonymised, and an intermediary evaluated the responses before they were shared with people who sought help. 
Morris told Vice 'We pulled the feature anyway and I wanted to unravel the concern as a thought piece, to help reign in enthusiasm about gpt3 replacing therapists.'
Operator: Koko Developer: Koko Country: USA Sector: HealthPurpose: Provide mental health support Technology: Large language model (LLM); NLP/text analysis; Neural networks; Deep learning; Machine learningIssue: Ethics; Privacy Transparency: Governance"
Adobe Creative Cloud uses customer to train AI systems,"Adobe has been automatically analysing customer content stored on Creative Cloud to train its AI algorithms, according to media reports.
The discovery sparked graphic designers, artists and other customers to share their concerns that Adobe is abusing their privacy and stealing their work to improve its own automated systems. Some saw it as another indication that their jobs are at risk of being replaced by robots.
Adobe's content analysis FAQ states it may use machine learning 'to develop and improve our products and services' and 'provide product features and customize our products and services ', providing examples such as the correction of perspective in images and automatically enhancing a document's headings and tables.
The row reflects broader concerns amongst artists, illustrators and others that their work is being scraped and used to train generative AI models such as DALL-E and Midjourney without their consent, thereby abusing their IP, commoditising their output, and potentially putting them out of work.
Adobe responded by saying it 'does not use data stored on customers’ Creative Cloud accounts to train its experimental Generative AI features.' In a Bloomberg interview, Adobe Scott Belsky later claimed the company never trained its generative AI services on customer projects.
Launched in 2011, Adobe Creative Cloud is a set of 20+ Adobe graphic design, video editing, web development, and photography applications and services delivered over the Internet.
Operator: Adobe users Developer: Adobe Country: USA Sector: Business/professional services Purpose: Improve products, services Technology: Machine learning; Pattern recognition; Object recognitionIssue: Privacy; Confidentiality; Employment Transparency: Governance; Marketing; Privacy"
VGG Face facial recognition dataset,"VGG Face is a dataset of 2.6 million facial images of 2,622 people that was created to provide researchers working on facial recognition systems with access to biometric data.  
The dataset mostly comprises celebrities, public figures, actors, and politicians whose names were chosen 'by extracting males and females, ranked by popularity, from the Internet Movie Data Base (IMDB) celebrity list.' Information about ethnicity, age, and kinship was also collected from IMDB.
At no point did any individual whose personal details were collected provide consent or information about how they were being used, according to Adam Harvey at exposing.ai, raising the question as to whether the images of public figures should be available for any organisation or person to use as they see fit.
The dataset has since been removed from Oxford University's website.
Operator: ChaLearn; Chinese Academy of Sciences; Delft University of Technology; Simula Research Laboratory; University of Applied Sciences & Arts Western Switzerland; University of California, Berkeley; Universitat Autònoma de Barcelona Developer: University of Oxford
Country: UK
Sector: Research/academia
Purpose: Develop facial recognition systems
Technology: Dataset; Facial recognition  Issue: Privacy; Copyright
Transparency: Privacy"
Scale AI shares sensitive Roomba robot vacuum training photos,"Sensitive photographs used to train iRobot vacuum cleaners, including of a young woman sitting on the toilet, were shared on Facebook, triggering privacy advocates to question the effectiveness of iRobot data protection practices.
According to Technology Review, images of development versions of iRobot’s Roomba J7 series robot vacuum were posted online by Venezuelan contractors to San Francisco-based Scale AI, a start-up that uses low-cost gig workers to annotate audio, photo, and video data used to train artificial intelligence systems.
According to iRobot, the photos came from 'special development robots with hardware and software modifications that are not and never were present on iRobot consumer products for purchase.' The images, the company says, were 'shared in violation of a written non-disclosure agreement between iRobot and an image annotation service provider.'
iRobot declined to share the consent agreements with Technology Review, nor make any of its paid collectors or employees available to discuss their understanding of the terms. However, a follow-up article cited beta testers saying iRobot had 'failed spectacularly' to mention that personal images of their homes and children will be seen and analysed by other humans.
iRobot CEO Colin Angle later confirmed the company terminated its relationship with Scale AI as a result of the incident.
Operator: Amazon/iRobot; Scale AI Developer: Amazon/iRobot Country: USA; Venezuela Sector: Consumer goods Purpose: Clean floor Technology: Robotics; Computer vision; IoT; Machine learning; Object recognition; Sensor Issue: Privacy; Security; Surveillance Transparency: Governance; Marketing; Legal"
Chess robot breaks child's finger,"A chess-playing robot has grabbed and broke the finger of a seven-year old competitor during a match at the Moscow Open, according (in Russian) to Tass. The boy played the rest of the tournament in a plaster cast.
Apparently unsettled by the quick responses of his opponent, the robot is seen to have pinched the boy's finger for several seconds before he is freed and led away by a woman and three men.
Russian Chess Federation vice-president Sergey Smagin told Tass the robot appeared to pounce after the boy opted for a quick move after it had taken one of the boy’s pieces, thereby violating 'certain safety rules'.
The Russian Chess Federation declined to say who manufactured the robot, or how it works. Video of the incident shows the machine appears to be a standard industrial robot arm that has been customised to move pieces on three chess boards simultaneously. 
Operator:  Developer:  Country: Russia Sector: Media/entertainment/sports/arts Purpose: Play chess Technology: Robotics; Computer vision Issue: Safety Transparency: Governance"
Google LaMDA large language model,
MSG Entertainment facial recognition,
Apple Crash Detection false positives,"Apple's iPhone 14 and Apple Watch Series 8 watches feature a new 'Crash detection' feature that automatically calls 911 in the US and Canada when the devices detect a sudden stop that indicates the user has been involved in a car, SUV, or van crash.
The system quickly proved useful in some instances, though reports in the Wall Street Journal and other publications of false crashes triggered by people riding rollercoasters suggest it was not working as it should.
Shortly afterwards, ski resort emergency despatch centres across the US and Canada were overwhelmed by hundreds of automated crash notifications calls, none of which turned out to be an emergency. 
The calls diverted resources away from real emergencies, resulting in a loss of productivity. If the skier in question failed to answer a return call, ski patrollers were sent to check the location of the automated call. 
Operator: Apple Developer: Apple Country: Apple Sector: Automotive Purpose: Detect vehicle crashes Technology: Motion sensor algorithm; Gyroscope: Accelerometer; GPS; Barometer Issue: Accuracy/reliability; Safety Transparency: Governance; Black box; Marketing"
Neuro-sama AI v-tuber denies Holocaust,"Neuro-sama, a virtual, Japanese anime-style v-tuber that is controlled by AI, was found to be offensive and inflammatory, including denying the Holocaust and women's rights, amongst other things. 
First developed in 2019 and upgraded in December 2022 using a large language model, Neuro-sama resides on live video streaming service Twitch, where it plays Minecraft and osu, a musical rhythm game, and talks to its followers via Twitch chat. 
Neuro-sama developer Vedal argued the character had a number of in-built filters, and that it's discussions were moderated. In January 2023, Neuro-sama was banned from Twitch.
Operator: Vedal987 Developer: Vedal987 Country: Japan Sector: Media/entertainment/sports/arts Purpose: Engage audiences Technology: Chatbot; Neural network; NLP/text analysis Issue: Safety Transparency: Black box"
Mimic anime art generator,
Apple Cycle Tracking fertility predictions,
Flo menstrual cycle data sharing,"Flo is a US-based period tracker and pregnancy app used by over 100 million people which allows users to 'access personalized health insights, virtual dialogs, and dozens of courses to learn how your cycle affects your body and well being.'
In February 2019, a Wall Street Journal investigation alleged that Flo had been sharing data with Facebook, Google, and others, every time a user had their period or indicated that they wanted to get pregnant, despite promising to keep users’ sensitive health data private.
The article sparked a strong backlash from civil rights and privacy groups, and customers, resulting in Flo Health settling with the US Federal Trade Commission (FTC) in January 2021. According to the terms of the deal, Flo must notify affected users about the disclosure of their health information and stop misrepresenting how it collects, manages, and uses customer data. 
In the wake of the US Supreme Court's controversial decision to overturn the 1973 Wade vs Roe ruling that legalised abortion across the US, Flo advised customers it would launch an ‘Anonymous Mode’ that removes their personal identity from their accounts, prompting further users to cancel their accounts.
Operator: Flo Health Developer: Flo Health
Country: USA
Sector: Health
Purpose: Track menstrual cycle
Technology: Prediction algorithm Issue: Privacy
Transparency: Governance; Privacy; Marketing"
"Randal Reid facial recognition wrongful arrest, jailing
","The use of facial recognition technology by Louisiana authorities led to the arrest and jailing of Randal Reid, a Georgia man on a fugitive warrant from a state he had never visited, according to his lawyer.
Reid was detained in DeKalb County, Georgia, in late November 2022, after Louisiana police used facial recognition to mistakenly link him to the theft of USD 13,000 of purses in Baton Rouge, Louisiana. 
Reid spent six days in jail and missed a week of work due to the incident.
According to AP, 'Jefferson Sheriff Joe Lopinto’s office did not respond to several requests for information [..] on Reid’s arrest and release, the agency’s use of facial recognition or any safeguards around it.' Gizmodo reports Louisiana authorities 'admitted the false match 'tacitly''.
In July 2022, New Orleans city council voted to allow the New Orleans Police Department (NOPD) to use facial recognition, having outlawed its use in December 2020. 
Per Protocol, the department never kept records of how often the technology was used or whether it facilitated investigations or led to arrests or convictions.
Operator: Jefferson Parish Sheriff's Office; Louisiana State Analytical and Fusion Exchange  Developer: IDEMIA
Country: USA
Sector: Govt - police
Purpose: Identify criminals
Technology: Facial recognition Issue: Accuracy/reliability; Privacy
Transparency: Governance; Black box; Marketing; Privacy"
DeviantArt DreamUp art generator,
Real-World Masked Face dataset,"Real-World Masked Face Dataset is a dataset of photographs of over 5,000 masked faces of 525 people developed by researchers at Wuhan University. 
In an accompanying research paper, the researchers say the project is to help identify individuals wearing face masks as a means of controlling the COVID-19 pandemic.
However, some commentators worry that the photos may be used for other purposes by the Chinese authorities, including the monitoring of Uyghurs in Xinjiang province.
The paper says the images are of 'public figures' gathered 'from massive internet resources.' The researchers have refused to discuss how they chose the people added to the dataset, or whether or not this may impact their privacy.
Operator:  Developer: Wuhan University
Country: China
Sector: Health; Research/academia
Purpose: Improve facial recognition algorithms
Technology: Dataset; Facial recognition; Computer vision Issue: Privacy; Ethics; Dual/multi-use; Surveillance
Transparency: Governance; Privacy"
Midjourney image generator,
GoEmotions dataset mis-labelling,"GoEmotions is a 'fine-grained' dataset that enables users to train AI applications such as chatbots, content moderation, and customer support systems that can recognise emotional sentiment in text. 
Released in October 2021, Google describes GoEmotions as 'a human-annotated dataset of 58k Reddit comments extracted from popular English-language subreddits and labeled with 27 emotion categories.' 
A July 2022 study of 1,000 random labeled comments from GoEmotions by Surge AI discovered that 30% had been mislabeled, indicating that the data had not been verified by humans.
Surge highlighted two aspects of Google's methodology - context and the complexity of English for non-American speakers - concluding that Google's outsourced Indian human labelers were likely given 'no additional metadata' about each Reddit comment, thereby losing its context and meaning to different types of users, not least those in the US.
TNW's Tristran Harris observed that 'any AI model trained on this dataset will produce erroneous outputs', 'causing demonstrable harm to other humans' and called for it to be deleted.
Harris blasts Google for ignoring the privacy of the Reddit community: 'It is entirely unethical to train an AI on human-created content without the expressed individual consent of the humans who created it.' 
'When I post on Reddit', he says, 'I do so in the good faith that my discourse is intended for other humans. Google doesn’t compensate me for my data so it shouldn’t use it, even if the terms of service allow for it.'
Operator: Alphabet/Google Developer: Alphabet/Google
Country: USA
Sector: Technology; Research/academia
Purpose: Classify emotions
Technology: Dataset Issue: Accuracy/reliability; Ethics
Transparency: Privacy"
Southwest Airlines crew scheduling automation,"US carrier Southwest Airlines is under fire for having to cancel over 15,000 flights during poor weather, leaving travelers stranded over Christmas and needing to find alternative transportation.
Southwest uses SkySolver and Crew Web Access software to assign flight attendants and pilots to each flight, and to correct scheduling interruptions by moving planes and staff around the country as quickly as possible.
However, airline staff, unions, air industry professionals and commentators complain these systems are 'archaic', unable to meet increased demand, and that their overhaul has been put on ice for too long by Southwest's increasingly financial, as opposed to operations-driven, leadership.
Southwest Airlines' new CEO Bob Jordan has since acknowledged the airline's technology troubles, telling employees that 'they’ll be hearing more about our specific plans to ensure the challenges that they’ve faced the past few days will not be part of our future.'
Operator: Southwest Airlines Developer: General Electric
Country: USA
Sector: Transport/logistics
Purpose: Schedule crew 
Technology: Crew scheduling software Issue: Accuracy/reliability; Robustness
Transparency: Governance"
Lensa AI Magic Avatars,
Apple Watch blood oximeter racial bias,"Apple's Blood Oxygen app enables users to measure the percentage of oxygen carried by red blood cells from one's lungs to the rest of one's body. 
Apple says its Blood Oxygen 'app measurements are not intended for medical use, including self-diagnosis or consultation with a doctor, and are only designed for general fitness and wellness purposes.' 
Research studies show pulse oximetry apps fail to measure blood oxygen levels accurately and are unreliable, especially when they’re low, resulting in questioning the need for wristwatch blood oxygen sensors.
A December 2022 class-action lawsuit filed in the Southern District of New York claims that Apple Watch's blood oximeter does not work as well for Black people, amounting to consumer fraud. 
'While traditional fingertip pulse oximeters are capable of measuring blood oxygen levels and heart rate, wrist-worn devices [..] determine heart rate, as blood oxygen measurements from the wrist are believed inaccurate,' the complaint states.
It goes on to say 'Algorithms designed for fingertip sensing are inappropriate when based on wrist measurements, and can lead to over 90% of readings being unusable'.
Most retail blood oximeters cannot be marketed as medical advices without US Food and Drug Administration (FDA) approval, meaning their accuracy can be tricky to determine. 
It is unclear why Apple has not been through the FDA's well-established testing process.
Operator: AppleDeveloper: Apple
Country: USA
Sector: Health
Purpose: Measure blood oxygen
Technology: Sensor; Blood measurement algorithmIssue: Accuracy/reliability; Appropriateness/need; Bias/discrimination - race, ethnicity
Transparency: Governance; Black box; Marketing"
"Walgreens Cooler Screens fridge door tracking, advertising",
ChatGPT chatbot,
Marty grocery store robot ,
Binance CCO deepfake impersonation,"Patrick Hillmann, the chief communications officer at cryptocurrency exchange Binance, claims that a 'sophisticated hacking team' manipulated video footage of his past TV appearances to make an 'AI hologram' of him in order to make people think he was helping them get listed on the exchange. 
According to Hillmann, the AI hologram was able to fool representatives of several cryptocurrency projects in Zoom calls into believing that they were being considered for listing on Binance.
The listing scheme was discovered when crypto project members contacted Hillmann to thank him for his help in the alleged listing opportunities. However, he had no knowledge of these meetings because he is not part of the listing process at Binance, he wrote. 
Operator: Anonymous/pseudonymous Developer: Unclear/unknown
Country: USA
Sector: Banking/financial services
Purpose: Defraud
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Security; Ethics
Transparency: Governance; Privacy"
FTX CEO deepfake scam,"A fake video of Sam Bankman-Fried (SBF), the former CEO of cryptocurrency exchange FTX, has been circulating on Twitter. The video attempts to scam investors affected by the exchange’s bankruptcy. 
The poorly made 'deepfake' video, which is a manipulated version of a Bloomberg Markets and Finance interview, attempts to direct users to a malicious website under the promise of a 'giveaway' that will double your cryptocurrency. The Twitter account was verified and mimicked SBF’s real account. 
The scam appeared to take advantage of Twitter CEO Elon Musk's decision to sell blue verification checks for USD 8. ftxcompensation.com prominently featured Bankman-Fried’s face and FTX’s logo and was registered to an individual in Nevis, near Puerto Rico. The website has since been taken offline. 
Operator: Anonymous/pseudonymous Developer: Unclear/unknown
Country: Bahamas; USA
Sector: Banking/financial services
Purpose: Defraud
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Security; Ethics
Transparency: Governance; Privacy"
Evolv Express weapons detection,
FN Meka virtual rapper racial stereotyping,"FN Meka is a self-described 'virtual rapper' initially designed by digital designer Brandon Le and music manager Anthony Martini, co-founders of 'next-generation music company' Factory New, to sell NFTs and post videos of its lifestyle, including Bugatti jets, helicopters and a Rolls Royce custom-fit with a Hibachi grill.
The rapper made its debut in April 2019 and quickly became popular, garnering over a billion views on TikTok. Signed to Capitol Records in August 2022, FN Meka was the first 'AI-generated' rapper to be signed to a major label. 
Shortly after its signing to Capitol, FN Meka was in the spotlight for alleged racial stereotyping of black people, notably that its creators had not used Black singers to record its voice, contrary to public statements. 
Meka was dropped by Capitol Records ten days later its signing. 
US rapper Kyle the Hooligan subsequently claimed that he was the original voice of FN Meka, and announced that he intends to sue Brandon Le and Factory New.
Factory New stands accused of fabricating the origin of FN Meka's voice and, given the White and Asian backgrounds of its makers, of racial appropriation, hypocrisy and fakeness.
Operator: Factory New; Capitol Music Group/Capitol Records Developer: Factory New
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Entertain; Sell NFTs 
Technology: Augmented Reality (AR); Virtual reality (VR); NFT Issue: Bias/discrimination - race, ethnicity; Hypocrisy
Transparency: Governance; Marketing"
India Human Efficiency Tracking System sanitation worker surveillance,
Buenos Aires Sistema de Reconocimiento Facial de Prófugos,"The city of Buenos Aires' Fugitive Facial Recognition System or Sistema de Reconocimiento Facial de Prófugos (SNRP) is a system comprising approximately 300 cameras on public roads, subway and train stations intended to identify people wanted by the police. 
Introduced in April 2019, the SNRP was governed by a cooperation agreement between the Buenos Aires Ministry of Security and the Ministry of National Justice which allowed access to the National Consultation on Rebellions and Captures (Consulta Nacional de Rebeldías y Capturas) database of approximately 40,000 individuals wanted by the national authorities.  
In April 2022, Judge Andrés Gallardo discovered through a legal injunction that the City of Buenos Aires Government had also signed a side deal with Argentina's national ID database – the National Register of People or Registro Nacional de las Personas (RENAPER) that enabled authorities across the country to access data held by Buenos Aires city, and vice-versa. 
In total, the personal data of 9.9 million people, including the country's formers presidents Cristina Fernández de Kirchner and Alberto Fernández, and numerous journalists and business people, was accessed between April 2019 and March 2022.
Media reports also revealed that Buenos Aires leaders had proposed making available to the national executive facial recognition cameras for identifying poverty demonstrators considered a potential threat to the government.
The SNRP system and its governance was roundly criticised and was the subject of a high-profile lawsuit by multiple prominent civil rights organisations for inadequate governance, abuse of privacy, and poor transparency.
The system was suspended by Judge Gallardo in April 2022. He also demanded a report on the data sharing by the Buenos Aires authorities and how the facial images have been used.
In September 2022, the SNRP was declared 'unconstitutional' by Buenes Aires judge Elena Amanda Liberatori.
An April 2022 appeal (pdf) by the Observatorio de Derecho Informático Argentino (ODIA) accused the SNRP of a lack of transparency concerning the personal data being collected and used.
In October 2020, the Buenos Aires authorities were accused by Human Rights Watch of storing the details of 'at least 166' children accused of committing crimes on Argentina's CONARC, the country's national database of inviduals with outstanding arrest warrants for serious crimes, between May 2017 and May 2020. 
HRW contended Buenos Aires' live facial recognition system is likely to amplify the risks of wrong identification of children due to the known inaccuracies of such systems when used on children.
Operator: Government of the City of Buenos Aires; Buenos Aires City Police; Argentine Ministry of Justice and Security; ReNaPerDeveloper: Danaide/NtechLab
Country: Argentina
Sector: Govt - municipal; Govt - police; Govt - security
Purpose: Identify criminals
Technology: Facial recognition Issue: Privacy; Surveillance; Dual/multi-use; Scope creep/normalisation; Appropriateness/need
Transparency: Governance; Marketing; Privacy"
TerraUSD algorithmic stablecoin collapse,"TerraUSD (also known as UST) is an algorithmic stablecoin created in 2018 by Terraform Labs that is built on the Terra blockchain. Pegged to the US dollar through a complex algorithmic relationship with its support cryptocurrency coin LUNA, TerraUSD can be exchanged with LUNA tokens.
Per The Register, UST and Luna are linked by an algorithm in a 'smart contract' that's supposed to keep the value of the UST pegged to 1 USD without fiat currency reserves. The algorithm burns or deletes Luna tokens to create new UST tokens and vice versa in an effort to balance supply and demand. 
According to Terraform Labs co-founder Do Kwon, TerraUSD 'is the first decentralised stablecoin that is scalable, yield bearing and interchain.' 
In May 2022, TerraUSD lost its dollar peg amidst a wider crypto market crash, falling to USD 0.10. The fall precipitated a collapse in the price of LUNA to USD 0.09 from a high of USD 119.51, and wiped out USD 55 billion of market capitalisation. 
It remains unclear what precipitated the collapse, with some blaming an acceleration of customer withdrawals spiralling into a de facto bank run, thereby destabilising the algorithmic balancing mechanism between TerraUSD and LUNA. 
Attempts by TerraLabs to save UST’s peg by selling almost all of its Bitcoin reserves for UST, as well as other strategies, failed to save the stablecoin. Other cryptocurrencies are tied to more stable assets, such as the US dollar or gold. 
The crisis fueled a credit crunch that saw hedge fund Three Arrows Capital (3AC) forced into liquidation, and cryptocurrency lender Celsius Network and crypto broker Voyager Digital filing for Chapter 11 bankruptcy. 
At the time of its collapse, TerraUSD had been the third largest stablecoin by market capitalisation, and Luna one of the ten largest cryptocurrencies.
In February 2023, the US Securities and Exchange Commission (SEC) charged (pdf) Terraform Labs Do Kwon with fraud.
Defended by Do Kwon as a 'transparent and open source' system, the collapse of TerraUSD is viewed as much a governance and communications failure as a pure algorithmic failure.
Facing allegations of fraud and embezzlement, including that TerraUSD was a ponzi scheme, TerraLabs co-founder Do Kwon disappeared after TerraUSD's collapse. Arrest warrants have been issued for Kwon and his fellow co-founder Daniel Shin.
Operator: Terraform LabsDeveloper: Terraform Labs
Country: Singapore; Global
Sector: Banking/financial services
Purpose: Manage financial marketplace
Technology: Blockchain; Virtual currency; Stablecoin; Smart contract algorithm Issue: Robustness
Transparency: Governance; Black box; Marketing"
Oregon DHS Safety at Screening Tool,
Allegheny County child neglect screening system,
Edmonton sexual assault DNA phenotyping,"The Edmonton Police Service (EPS) is under fire from Black and civil rights organisations for using a controversial DNA prediction technology to solve a 2019 sexual assault in which there no witnesses, surveillance video, public tips or DNA matches.
Their investigation having gone cold, the police turned to Parabon NanoLabs to conduct DNA phenotyping, which uses a person’s genetic material to predict parts of their appearance such as eye, skin and hair colour, as well as facial features including shape.
Police can use such information to narrow suspects and generate leads in criminal investigations. 
Parabon produced for EPS a composite 'Snapshot' of a five-foot-four Black man with dark brown to black hair and dark brown eyes based on trait predictions from DNA evidence collected from the victim.
The police published the image of a suspect for circulation in the hope it would generate leads. However, it led to a significant backlash, with local communities accusing the police of stereotyping a Black suspect and stigmatising racial groups. Two days later, the EPS apologised and removed the image from its website and social media accounts.
Like other DNA analysis tools, TrueAllele's Snapshot is an algorithmic black box 'the exact details of our method [which] have not been published', the company acknowledges, adding that it has 'attempted to be as transparent as possible by presenting our work at conferences and posting every single composite that goes public on our website, so people can draw their own conclusions about how well our technology works.'
University of Calgary biological anthropologist and evolutionary biologist Benedikt Hallgrímsson told the New York Times in 2015 that forensic DNA phenotyping was 'a bit of science fiction at this point', and that Parabon 'oversell their ability to predict the facial-shape phenotypes.'
Operator: Edmonton Police Service Developer: Parabon NanoLabsCountry: Canada Sector: Govt - police Purpose: Predict physical appearance Technology: DNA phenotypingIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Privacy Transparency: Governance; Black box; Marketing"
Stable Diffusion image generator,
Singapore Mindline at Work for MOE mental health support,
San Francisco police 'killer robots',"A policy authorising the San Francisco Police Department (SFPD) to use military-style weapons, including remote-controlled robots, to kill suspects has been put on hold by the city's supervisors.
According to the SFPD, it's existing armoury of 17 Remotec and QinetiQ robots could be equipped with explosives 'to contact, incapacitate, or disorient [a] violent, armed, or dangerous suspect' in 'extreme circumstances to save or prevent further loss of innocent lives.' 
Late November 2022, San Francisco's board of supervisors approved a policy that lets police robots 'be used as a deadly force option when risk of loss of life to members of the public or officers is imminent and outweighs any other force option available.'
However, the decision resulted in an immediate backlash from civil rights groups. The Electronic Frontier Foundation (EFF) argued it was typical police-military mission creep. The San Francisco Public Defender’s office warned that granting police 'the ability to kill community members remotely' went against the city’s progressive values. 
A week later the boad suspended the policy, sending it back to back to committee for further discussion.
Operator: San Francisco Police Department (SFPD) Developer: Remotec; QinetiQ  Country: USA Sector: Govt - police Purpose: Strengthen security Technology: Robotics Issue: Safety; Scope creep/normalisation; Ethics Transparency: Governance"
Toronto beach water quality predictions,"Artificial intelligence predictive modelling (AIPM) is a system used by Toronto Public Health (TPH) to forecast water quality at two beaches.
The system uses a series of calculations based on historical data and metrics such as rainfall, temperature and wind direction. It also pulls real-time meteorological and hydrological data. 
AIPM was adopted by the authority early summer 2022, but quickly came under pressure as it appears to have repeatedly allowed contaminated beaches to remain open. 
The accuracy of the system was quickly called into question by water advocacy group Swim Drink Fish, which discovered that waters off the beaches tested high for E.coli using traditional means had been marked safe by the new system over 30 times. 
Per the Toronto Star, TPH responded by saying 'While AIPM is not expected to be 100 per cent accurate in assessing water quality, it presents a significant improvement over test results using the traditional means for assessing microbial water quality.'
Swim Drink Fish argue more transparency is needed from the Toronto Health Board if the model is to remain so that beachgoers can make educated decisions regarding their own health and safety. 
Yet they were poorly guided. According to The Information, TPH officials 'did not respond to a question about whether officials ever overrode the model’s forecast. But data published by the city show that the posted swimming flags at the two beaches never differed from the model’s predictions.'
Operator: Toronto Public HealthDeveloper: Cann Forecast Country: Canada Sector: Govt - health Purpose: Predict water quality Technology: Prediction algorithmIssue: Accuracy/reliability; Safety Transparency: Governance; Black box"
Galactica large language model,
BlenderBot conversational chatbot,
KFC Germany Kristallnacht marketing automation,"An automated alert sent to customers by KFC in Germany commemorating the 84th anniversary of Kristallnacht (or 'The Night of Broken Glass') has backfired.
The alert urged customers to commemorate the Kristallnacht by eating its cheesy fried chicken. The Kristallnacht was the 1938 pogrom that preceded the Holocaustor and saw Nazi mobs destroy synagogues and Jewish-owned businesses, kill 91 Jews and send 30,000 to concentration camps.
The promotion suggested customers 'Go ahead and treat yourself to more soft cheese on your crispy chicken. Now available at KFCheese!' under the subject line 'Memorial day for the Reich pogrom night'.
KFC apologised after customers started complaining and the promotion was picked up by the EU arm of the Anti-Defamation League ('ADL'), whose associate director Dalia Grinfield tweeted 'How wrong can you get on Kristallnacht KFC Germany. Shame on you!'
The fast food chain blamed the message on an 'automated push notification ... linked to calendars that include national observances', adding that it 'sincerely' apologised for the 'unplanned, insensitive and unacceptable message'.
Operator: KFC Germany Developer: KFC Germany Country: Germany Sector: Food/food services Purpose: Automate marketing communications Technology: Bot/intelligent agent Issue: Governance; Safety Transparency: Governance"
Upstart automated consumer lending discrimination,"Law firm Relman Colfax has found that a loan decision model deployed by San Mateo-based AI consumer lender Upstart produced 'significant disparities' in how often loans were made to Black and non-Hispanic white borrowers.
Upstart assesses and predicts creditworthiness using so-called 'Alternative Data', including a person's educational and employment history. The company says its approach is more inclusive than the underwriting methods used by many banks.
The AI lender claims its algorithmic lending model approved 30% more Black borrowers than traditional models in 2020. However, academic research published in January 2022 indicates discriminatory pricing exists in both traditional and fintech lending.
In July 2022, Upstart directors were hit by a class-action investor lawsuit claiming it made false and misleading statements about its  business, operations, and prospects.
The company went public in 2020.
Operator: Upstart Holdings; Multiple Developer: Upstart HoldingsCountry: USASector: Banking/Financial services Purpose: Assess creditworthiness Technology: Machine learning Issue: Bias/discrimination - race, ethnicity, education, employment Transparency: Governance; Black box; Marketing - misleading"
Olive 'inflates' AI capabilities,"A US start-up that had promised to turbocharge AI for healthcare was accused of misleading marketing after it laid off 450 employees in a move said by CEO Sean Lane to have been caused by the company's 'fast-paced growth and lack of focus'. 
The claim was seen as contradicting Axios interviews with former and current employees and health tech executives suggesting the company regularly over-hyped its capabilities and under-delivered, generating 'only a fraction of the savings it promises.'
Olive had been through various incarnations since it was founded in 2012, and was at one time valued at USD 4 billion. But it was dogged by questions about the efficacy of its software and the integrity of its marketing and leadership. 
In October 2023, Olive announced it had sold off pieces of its business to healthcare companies Waystar and Humata Health, and would be closing. 
Operator: WPP/AKQA; Circulo Health; GuideWell; TriHealth Developer: Olive AI Country: USASector: HealthPurpose: Automate healthcare services Technology: Robotic Process Automation (RPA); Machine learningIssue: Business model; Effectiveness/value Transparency: Marketing - misleading"
Axon school security taser drones,"US taser manufacturer Axon announced it will develop taser-armed drones as part of a long-term plan to 'stop mass shootings' in the USA, prompting privacy advocates to express concerns about surveillance and safety.
The drones would consist of miniature, lightweight tasers, and 'targeting algorithms' to help operators aim the device safely and incapacitate an active shooter within 60 seconds, the company said. Responders would be trained using virtual reality. The announcement came days after the shooting of 21 people at an elementary school in Uvalde, Texas.
The move also led to the resignation of nine of twelve members of Axon's AI Ethics Advisory Board, which said had not been consulted about the programme and had concerns about introducing weaponising drones in over-policed communities of colour. The board had successfully halted previous product launches, including when Axon was considering introducing facial recognition technology to it's line of police-worn body cameras.
Within three days, the company halted the project. It later formed a new Ethics and Equity Advisory Council, an 11-member group comprising a cross-section of academics, community organisers, and activists.
In January 2023, Axon's original AI Ethics Board published (pdf) a report explaining the rationale for their objections to Project ION, including the technology's potential for misuse, the dehumanisation of individuals targeted, the increased militarisation of the police, operational risks, and potential reputational damage.
Operator:  Developer: Axon Country: USASector: Education Purpose: Strengthen security Technology: Drone; Computer vision Issue: Appropriateness/need; Effectiveness/value; Privacy; Surveillance; Bias/discrimination - race, ethnicity Transparency: Governance; Marketing"
GPT-4chan 'hate speech machine',"GPT-4chan, an AI system trained using 3.3 million threads from 4chan's notorious 'politically incorrect' /pol/ board, has been discovered to be posting sexist, racist, and anti-semitic slurs. 
Developed by AI researcher Yannic Kilcher, GPT-4chan was released back onto 4chan as multiple bots, which posted freely on the board over several days over 30,000 times. 
The high volume of messages roused the suspicion of some 4chan users, who began to speculate on the identity of the  anonymous poster. 
Kilcher described GPT-4chan as a 'prank' to The Verge, arguing it 'perfectly encapsulated the mix of offensiveness, nihilism, trolling, and deep distrust of any information whatsoever that permeates most posts on /pol.'
AI researchers lambasted Kilcher’s project as nakedly self-promotional, naive, and unethical, calling out the fact that he tried to make the model freely accessible on AI community Hugging Face.
The researchers also argued that 4chan users were not informed they were interacting with a bot; Kilcher points out that users were informed of the fact in his video.
HuggingFace promptly restricted access to GPT-4chan.
Operator: Yannic Kilcher; Reddit Developer: Yannic KilcherCountry: USASector: MultiplePurpose: Train language model Technology: NLP/text analysis; Bot/intelligent agent Issue: Safety; Ethics Transparency: Governance; Marketing"
DALL-E image generator,"DALL-E is a software programme that automatically generates images from natural-language text descriptions (or 'prompts'). Trained on text-image pairs culled from the Internet, DALL-E claims to create 'realistic imagery and art' in multiple styles and compositions.
Developed by OpenAI and first revealed in January 2021, DALL-E uses a modified version of large language model GTP-3 to generate images. DALL-E2, which generates more diverse, higher resolution images faster, was released in May 2022. 
In September 2023, OpenAI released DALLE-3 with improved capabilities, added guardrails, and integration with ChatGPT.
DALL-E has been praised by researchers and commentators for the ease with which it makes it possible to create highly realistic, if surprising and weird, images and artwork at high speed. 
Others, however, have pointed out the software's technical limitations, and ethical and legal risks, including:
Accuracy: DALLE's ability to produce illogical and incomprehensible content, especially when presented with longer prompts.
Bias: DALL-E exhibits and reinforces biases, including gender, racial and cultural stereotyping.
IP/copyright: DALL-E produces content that copies existing artwork, and is reckoned to abuse copyright and trademarks. In November 2023, Disney cracked down on Microsoft's DALL-E-powered AI image creator system after users created posters featuring their dogs as the stars of Pixar Studio films, violating the media company's intellectual property.
Employment: DALL-E's will kill the careers of artists, graphic designers, animators, anime cartoonists, food photographers and others.
Environment: Generative models like DALL-E typically consume huge amounts of energy.
Mis/disinformation: Having stopped users uploading and editing human facial images in order to minimise the generation of deepfakes, OpenAI'S decision to reintroduce this ability has fueled concerns that it is much easier to use DALL-E to generate and spread mis and disinformation. UC Berkeley researcher Henry Farid reckons DALL-E 'could be disinformation on steroids'.
Privacy: OpenAI is trained on photographs and other images images publicly available on the internet without consent. Furthermore, the company's decision to reintroduce the ability to upload third-party faces is seen to potentially damage the privacy of people whose consent may not have been obtained.
Safety: DALL-E can produce offensive or explicit content, as well as content that can be construed as harrassment or bullying.
These limitations and risks broadly reflect those published by OpenAI upon DALL-E's launch and DALLE-2 upgrade.
OpenAI's refusal to let third parties assess its algorithm makes it difficult to understand how it works, and how its risks can be managed.
Given the variety and nature of the risks of DALL-E, and its potential negative impacts, OpenAI's decision to restrict user access to DALL-E has mostly been welcomed, even if some users complain that Stable Diffusion, Midjourney and other image generation tools are open to everyone and can be used with few, if any, restrictions.
In July 2022, OpenAI announced DALL-E 2 would be made available to up to one million users as part of a large-scale beta test. An API for the system was released in November 2022.
Operator: OpenAI; MicrosoftDeveloper: OpenAICountry: USASector: TechnologyPurpose: Generate images Technology: NLP/text analysis; Computer vision; Text-to-image; Neural network; Deep learning Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, gender; Copyright; Employment - jobs; Environment;  Mis/disinformation; Privacy; Safety Transparency: Governance; Black box; Marketing; Privacy"
WILDTRACK pedestrian detection dataset,"WILDTRACK is a dataset of video recordings of thousands of students, faculty and others to-ing and fro-ing 'in the wild' outside the main building at ETH University, Zurich. 
Developed by École Polytechnique Fédérale de Lausanne and ETH University researchers in Switzerland, the dataset comprises seven 35 minute videos captured by seven high-definition GoPro cameras during good weather conditions. 
The videos were subsequently annotated to mark the positions of people in each frame and informed research (pdf) papers on the project.
While notices were placed underneath each of the cameras, the recordings were largely made without the knowledge or consent of those captured due to their size and inconspicuous nature.
As researcher Adam Harvey notes in his exposing.ai project, this means students will be used as training data and have almost no recourse for redaction as the data is shared, copied, and manipulated across multiple legal jurisdictions. 
WILDTRACK has been made openly available for any type of research, with potential applications envisaged (pdf) by the researchers including security, surveillance, remote person identification, robotics, autonomous driving, and crowdsourcing. 
Published research studies reveal that WILDTRACK has been used by a range of academic and commercial entities such as the Nanjing University of Aeronautics (NUAA), the University of Leicester, Microsoft, and Wormplex AI to improve drone and retail surveillance.
Harvey notes that 'NUAA has produced over 40 unmanned aerial vehicles (UAVs) for China, most of which are small or micro sized UAVs with consumer or industrial surveillance capabilities ... [with] a limited number .. made specifically for military reconnaissance.'
Operator: École Polytechnique Fédérale de Lausanne (EPFL); ETH University; Microsoft; Nanjing University of Aeronautics and Astronautics (NUAA); Universidad Autónoma de Madrid; University of Leicester; Wormplex AI Developer: École Polytechnique Fédérale de Lausanne; ETH University Country: SwitzerlandSector: Technology; Research/academia Purpose: Improve pedestrian detection Technology: Dataset; Pedestrian detection; Computer vision; Pattern recognition Issue: Privacy; Surveillance; Dual/multi-use Transparency: Privacy"
Tesla Smart Summon private jet crash,"A video posted to Reddit appears to show a Tesla crashing into a private jet after being 'summoned' by its owner using the car maker’s automated summoning feature. 
Smart Summon enables a Tesla owner to 'summon' their vehicle from its parking space using Tesla's smartphone app from a maximum distance of 200 feet. The car will navigate around obstacles to its owner. 
The incident occurred at an event sponsored by aircraft manufacturer Cirrus at Felts Field in Spokane, Washington, and the video appears to show the Tesla slowly crashing into and then pushing the Cirrus Vision jet across the tarmac. 
First introduced in 2019, Smart Summons has been on the receiving end of complaints by owners and pedestrians about near crashes, confused cars, and bumper damage.
Tesla CEO Elon Musk has described Smart Summon as 'probably our most viral feature ever.'
Operator:  Developer: Tesla Country: USA Sector: Automotive Purpose: Summon car Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
Coupang own brand search engine rigging,"South Korean e-commerce company Coupang is under investigation by the country's Fair Trade Commission (FTC) for allegedly manipulating its search engine algorithms in favour of its own label products. 
The investigation comes hard on the heels of an on-the-spot inspection at the company's Seoul headquarters in Seoul following complaints by customers, civic groups and others that it had abused its market position by favouring its own brands products over those of third parties. 
The Financial Times notes that tampering with algorithms is common in South Korea’s e-commerce sector.
In August 2021, Coupang was fined 3.29 billion won (USD $2.81 million) for forcing hundreds of sellers to reduce the prices of their goods sold on its platform and increase them on rival platforms. It was also found to have unfairly coerced suppliers into purchasing adverts on its platform.
Coupang and its subsidiary CPLB are also under investigation by the FTC for getting its employees to submit positive reviews of its own brand products. 
Meanwhile, the company's food delivery subsidiary Coupang Eats and its star rating system have drawn fire for mishandling a complaint that resulted in the death of a restaurant owner.
An earlier FTC investigation found Korea's dominant search engine Naver had been manipulating its e-commerce and retail search engines in favour of its own brands.
Operator: Coupang  Developer: Coupang  Country: S Korea Sector: Technology; Retail Purpose: Rank content/search resultsTechnology: Search engine algorithm Issue: Competition/price fixingTransparency: Governance; Black box; Marketing"
Naver own brand search engine rigging,"South Korean internet giant Naver has been fined 26.7 billion won (USD 23 million) for rigging its shopping and video services search engines in order to promote its own products and services. 
According to the country's Fair Trade Commission, Naver had deliberately and opaquely 'misled customers to believe the results were fair and objective,' and 'damaged fair competition in e-commerce and video platform markets.'
The FTC found Naver has manipulated its search algorithms six times between 2012 to 2015 so that its Smart Store, Naver TV and other services appeared at top of search results while pushing down competitors such as 11th Street and Gmarket. 
Naver's actions enabled to increase its online shopping market share from 5% in 2015 to 21% in 2018. Its four main competitors saw their market share drops over the same period, the FTC said.
It is the first time South Korea's FTC has fined an online platform operator for manipulating its algorithms to damage competitors.
In July 2021, the FTC launched an investigation into Korean e-commerce giant Coupang for allegedly rigging its search engine in favour of its own brands.
Operator: Naver Developer: Naver Country: S Korea Sector: Technology; Retail Purpose: Rank content/search results Technology: Search engine algorithm Issue: Competition/price fixing Transparency: Governance; Black box; Marketing"
Zoom AI emotion recognition,"Plans by Zoom to develop an AI-based system that will monitor and detect the feelings and emotions of its users is drawing fire from US human and digital rights campaigners, ACLU, EPIC, Fight for the Future, and others.
According to an April 2022 report by Protocol, Zoom plans to introduce a suite of new features under the name Zoom IQ for Sales that use emotion AI to produce post-meeting transcripts and sentiment analysis to meeting hosts.
However, critics of emotion AI (or 'affective computing') argue the technology is often inaccurate as a person's facial expressions can be different to what they are feeling. 
It can also be discriminatory, with research showing the ways people express emotions vary significantly across cultures and situations. The technology is also said to be intrusive and easily misused. 
Plans by Intel to introduce emotion AI to schools are also proving controversial. 
Operator: Zoom Video Communications Developer: Zoom Video Communications Country: USA Sector: Business/professional services Purpose: Monitor & analyse emotion Technology: Emotion recognition; Facial analysis; Voice recognition; NLP/text analysis Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, disability; Dual/multi-use; Privacy; Surveillance Transparency: Black box"
Intel AI student emotion monitoring,"Intel is collaborating with education start-up Classroom Technologies to develop an AI-based tool that integrates with Zoom to let teachers know their students' state of mind. 
Protocol reports that Intel claims its system can detect whether students are happy, sad, bored, distracted, or confused by assessing their facial expressions and how they’re interacting with educational content. 
Some teachers who have tested Intel's system in a physical classroom say the system is useful. Critics of affective computing, however, argue emotion recognition is notoriously inaccurate and often mistakes facial expresssions for real, underlying feelings. It also struggles across different cultures and scenarios.
Others are concerned about the intrusive nature of using cameras in an educational context, and in their potential for continual surveillance and misuse.
Classroom Technologies co-founder and CEO Michael Chasen told Protocol he hopes to partner with one of the colleges his company works with to evaluate Intel's system.
Operator:  Developer: Intel; Classroom Technologies Country: USA Sector: Education Purpose: Improve student engagement Technology: Emotion recognition; Facial analysis Issue: Accuracy/reliability; Privacy; Surveillance Transparency: Black box"
"Instagram DM abuse, harassment","Instagram is 'systemically' failing to protect its users, including high-profile women, from harassment and abuse, according to a new study by non-profit organisation the Center for Countering Digital Hate (CCDH). 
The CCDH analysed over 8,700 messages sent to five high-profile celebrities on the app from 253 accounts, and found that 90% of abuse sent to them was ignored by Instagram, despite being reported to moderators. The CCDH also found that 227 of these accounts remained active at least one month after complaints were filed.
The five celebrities included actor Amber Heard, British broadcaster Rachel Riley, activist Jamie Klingler, journalist Bryony Gordon, and the founder of Burnt Roti magazine, Sharan Dhaliwal. 
Each handed over their Instagram accounts to the CCDH so their DMs could be monitored over a period of number of months. In one instance, Instagram failed to remove accounts that made death threats to Amber Heard's family, including her infant daughter. 
In February 2021 Instagram rolled out tougher measures to punish online harassment, including the disabling of accounts reported multiple times for sending 'violating messages.'
Instagram has previously been in the firing line for failing to protect teen girls from mental health harms, and Black English footballers from racist abuse.
Operator: Meta/Instagram Developer: Meta/Instagram Country: UK; USA Sector: Technology Purpose: Moderate content  Technology: Content moderation system Issue: Safety Transaparency: Governance; Black box; Marketing"
Microsoft predicts low-income teen pregnancies,"The Technology Platform for Social Intervention (Plataforma Tecnológica de Intervención Social) is a controversial algorithmic system used to predict teenage pregnancy in Argentina, Brazil, and Colombia.
Developed in 2016 by Microsoft for the Ministry of Early Childhood in Salta province, Argentina, the system draws on age, ethnicity, country of origin, disability, and socio-economic data of some 200,000 women and girls, including 12,000 between the ages of 10 and 19.
Microsoft announced the system is 'one of the pioneering cases in the use of AI data' in Argentina. Then governor of Salta Juan Manuel Urtubey claimed it would enable authorities to 'foresee five or six years in advance, with first name, last name, and address, which girl - future teenager - is 86 percent predestined to have an adolescent pregnancy'.
The system has been criticised by rights activists, journalists and politicians for being inaccurate, unreliable, unfair, biased against local indigenous groups, and an abuse of privacy.
An University of Buenos Aires' Applied Artificial Intelligence Laboratory (LIAA) audit concluded that the system suffered from major technical and design faults, and lacked data concerning access to sex education and contraception - which public health researchers view as the most important tools in reducing teenage pregnancy rates.
The system is also seen to be overly opaque having not been subject to formal review or assessment, and with access to the data, code and model denied. 
Furthermore, the involvement of Argentine pro-life campaign organisation the Conin Foundation in its development was not made public, having only been revealed by gender and digital rights campaigners.
Argentina's senate rejected a bill to legalise abortion in the first 14 weeks of pregnancy in August 2018.
Operator: Ministry of Early Childhoood, Salta Province Developer: Microsoft; Conin Foundation  Country: Argentina; Brazil; Colombia Sector: Govt - health Purpose: Predict teenager pregnancy Technology: Prediction algorithm Issue: Accuracy/reliability; Appropriateness/need; Bias/discrimination - income; Effectiveness/value; PrivacyTransparency: Governance; Black box; Marketing"
Cruise driverless car police inspection,"In June 2022, nearly twenty Cruise driverless cars stood still and blocked traffic for two hours on a street in central San Francisco, USA, causing tailbacks, making the area impassable and inhibiting night street-sweeping. 
The cars were reputedly disabled after they had lost touch with a Cruise server. The incident was resolved when the cars were manually moved out of the way by Cruise employees.
Per WIRED, a Cruise employee later sent an anonymous letter to the California Public Utilities Commission, claiming that Cruise loses communication with the automated vehicles 'with regularity', blocking traffic and potentially hindering emergency vehicles.
In April 2022, Twitter users posted videos of a Cruise vehicle moving away from San Francisco police after they had tried to find put whether the car had a driver.  
Operator: GM CruiseDeveloper: GM Cruise; General Motors/Chevrolet Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer visionIssue: Accuracy/reliability; Safety Transparency: Black box"
"Malaysia AI court sentencing system said to be inaccurate, unfair","An AI-powered court sentencing system being tested in Malaysia was accused by lawyers of being inaccurate and resulting in biased and unfair sentences.
Developed by Sarawak Information Systems, the nationwide pilot aimed to make sentencing more consistent and help clear the backlog of cases clogging Malaysia's legal system, according to Reuters. 
However, senior lawyers say the 'opaque' system lacks a judge's ability to weigh up individual circumstances, or adapt to changing social mores, and should be withdrawn.
Furthermore, lawyers say that there was no proper consultation on the technology's use. Malaysia's Bar Council said it was 'not given guidelines at all' when courts in Kuala Lumpur started using the system mid-2021 for sentencing in 20 types of crimes.
Operator: Mahkamah Persekutuan Malaysia Developer: Sarawak Information Systems (SAINS) Country: Malaysia Sector: Govt - justice Purpose: Achieve greater sentencing consistency Technology: Predictive statistical analysis Issue: Accuracy/reliability; Fairness; Bias/discrimination - race, ethnicity Transparency: Governance; Black box; Marketing"
Bytedance algorithm training content scraping,"TikTok parent company Bytedance scraped content from Instagram, Snapchat and other platforms and used them to train the algorithms of Flipagram, a TikTok predecessor, according to a Buzzfeed report.
Former employees and internal documents reveal Bytedance took hundreds of thousands of short-form videos, usernames, profile pictures, and profile descriptions without the knowledge of their owners or the social media platforms on which they had been posted.
Two employees told Buzzfeed the scraped content was later used to train the US version of TikTok's 'For You' recommendation algorithm. 
Bytedance denies the allegations.
Operator: ByteDance/TikTok/Flipagram Developer: ByteDance/TikTok/FlipagramCountry: China Sector: Technology Purpose: Recommend content Technology: Content recommendation system Issue: Privacy; Copyright Transparency: Governance; Privacy"
Speedcam Anywhere anti-speeding app,"A mobile phone-based anti-speeding app is drawing criticism for its privacy implications and potential use as a surveillance tool, according to The Guardian.
Speedcam Anywhere's AI-based app works by taking a short video clip of a passing vehicle, which is automatically assessed to gauge vehicle speed against local speed limits, with a report (pdf) generated which can be shared with authorities.
While not formally approved by the police, the app is being trialed in the UK by speed safety campaigners 20’s Plenty For Us and is described by the NGO's founder Rod King MBE as a 'game changer'. A senior police officer told the Guardian it is a 'potentially very significant' deterrent to dangerous driving.
However, drivers and rights activists worry Speedcam Anywhere is overly intrusive and the clips and reports can too easily be used for a number of unethical purposes. App users also complain the app is difficult to use and unreliable on Google Play. 
Operator: Speedcam Anywhere; 20’s Plenty For Us Developer: Speedcam AnywhereCountry: UK Sector: AutomotivePurpose: Estimate vehicle speed Technology: Automated license plate/number recognition (ALPR/ANPR); Computer vision Issue: Accuracy/reliability; Dual/multi-use; Surveillance; Privacy Transparency: Privacy"
Worldcoin iris biometrics,"Worldcoin is a self-described 'digital identity and financial network' that uses a 'chrome orb' to scan the irises and faces of people agreeing to sign up for a share of its new WLD currency.
The WLD currency started trading on July 24, 2023 under the auspices of the Worldcoin Foundation, a Cayman Islands 'exempted limited guarantee foundation company'. The currency trades in multiple jurisdictions - except the US, in which the laws are 'unclear', according to Worldcoin. 
In April 2022, internal documents shared with Technology Review and Buzzfeed News revealed that Orb operators in a number of countries were not being paid for users they had been signing-up, faced harassment and arrest under local laws, and that people having iris scans were complaining about unclear, misleading, and unethical marketing practices. 
It also emerged that Worldcoin had been collecting more personal data than it acknowledged, had failed to obtain meaningful informed consent, and that the company had failed to declare that it was using user data to train its artificial intelligence models.  potentially worthless cryptocurrency.
Shortly after the currency's launch, the UK privacy regulator announced it would 'examine' Worldcoin's business and France's privacy watchdog CNIL told Reuters that the legality of Worldcoin's data collection 'seems questionable'. 
In August 2023, Kenya suspended Worldcoin, leading to a Kenyan parliamentary committee arguing (pdf) the company and its partners had violated Kenyan law and be shut down until the country established proper regulations over virtual assets.
The Bavarian State Office for Data Protection Supervision has also been investigating Worldcoin since November 2022 over concerns the project seeks to process 'sensitive data at a very large scale' using new technology.
In December 2023, Tools for Humanity confimed it was withdrawing its orb iris scanning verification services in India, Brazil, and France. It gave no reason for its backpedalling. Its WLD currency quickly fell about 22%.
Operator: Tools for Humanity/Worldcoin Developer: Tools for Humanity/WorldcoinCountry: Argentina, Benin, Brazil, Chile, Colombia, France, Germany, Ghana, India, Indonesia, Israel, Italy, Kenya, Mexico, Netherlands, Nigeria, Norway, Portugal, South Africa, Spain, Sudan, Turkey, Uganda, UK, ZimbabweSector:  Banking/financial servicesPurpose: Develop digital identityTechnology: Iris scanning; Facial detection; Vital signs detection; Blockchain; Virtual currency Issue: Privacy; Security; Legality Transparency: Governance; Privacy; Marketing"
Facebook downranking system failure,"Facebook has experienced a 'massive' failure of its content ranking system that exposed users to a surge of disinformation, hate speech and other harmful content, according an internal report obtained by The Verge.
The document attributes the failure to a software bug and says that posts that would usually have been demoted by the company's algorithms were boosted by as much as 30%. 
A Facebook spokesman told The Verge the bug 'has not had any meaningful, long-term impact on our metrics.'
Operator: Meta/FacebookDeveloper: Meta/FacebookCountry: USA; Global Sector: TechnologyPurpose: Minimise harmful content Technology: Content ranking system Issue: Robustness; Mis/disinformationTransparency: Governance; Black box"
LinkedIn deepfake salespeople,"A cottage industry of companies in the US and India is creating fake profiles of individuals and companies on LinkedIn using AI, according to NPR.
Based on an initial investigation by Stanford Internet Observatory researchers, over 1,000 LinkedIn profiles were discovered that appear to use facial images created using AI-generated deepfake technologies.
LinkedIn’s professional community policies state that the social network does not permit fake profiles or entities on its platform, including using other peoples' images or 'any other image that is not your likeness' for profile photos.
LinkedIn has removed many of the offending profiles and is 'updating its defences' to catch fake accounts, NPR technology correspondent Shannon Bond said on Twitter.
Operator: Microsoft/LinkedIn Developer: Microsoft/LinkedIn Country: USA; India Sector: Business/professional services Purpose: Generate sales leads Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Safety; Accuracy/reliabilityTransparency: Governance"
Airbnb user trustworthiness scoring,
Minnesota Operation Safety Net (OSN),"A Technology Review investigation reveals US law enforcement agencies have been monitoring journalists and activists involved in protests in Minnesota following the murder of George Floyd and the trial of Derek Chauvin under the guise of government surveillance programme Operation Safety Net (OSN). 
Announced in February 2021 as an attempt to ensure public safety during what was likely to be a volatile and potentially explosive environment surrounding the trial, the investigation reveals the initiative went way beyond its publicly announced scope to include the widespread use of tools to monitor social media, track cell phones, and collect detailed images of people’s faces. 
The three-part series details how OSN continued to monitor and gather data on activists and journalists, including people not suspected of committing a crime, almost a year after it had supposedly shut down once Chauvin's trial had ended and the verdict had been announced in April 2021.
Minnesota officials denied that OSN continues. However, the Technology Review discovered presentations, emails and other intelligence indicated otherwise under the name OSN 2.0. 
While the demonstrations in Minneapolis during the trial were largely peaceful, OSH faced significant criticism from civil rights groups and prominent national and local politicians regarding what they saw as the quasi-militaristic nature of its response. 
Operator: Hennepin County Sheriff’s Office; Department of Homeland Security (DHS); Federal Bureau of Investigation (FBI) Developer: Department of Homeland Security (DHS); AT&T/Intrepid Networks Country: USA Sector: Govt - police; Govt - security Purpose: Strengthen security; Increase safety Technology: Social media monitoring; Facial recognition; Drone Issue: Surveillance; Privacy Transparency: Governance; Black box; Privacy; Marketing"
GIS employment background checks,"The US Consumer Finance Protection Board (CFSB) has ordered (pdf) two of the country’s largest employment background screening report providers to pay USD 13 million in penalties and refunds for providing inaccurate information. 
According to the CFSB, General Information Services (GIS) and its subsidiary e-Background-checks.com (BGC), which collectively generate and sell more than 10 million reports about job applicants every year, failed to employ reasonable procedures to ensure the accuracy of the information contained in reports provided to potential employers. 
In over 6,000 cases, the CFSB said, '(the firm) reported a criminal record that did not belong to the consumer, including, without limitation, those criminal records that (the firm) could not confirm belonged to the consumer based on personal identifiers…or reported inaccurate criminal record information about the consumer, including, without limitation, expunged records, dismissed charges, nolo prosequi records reported as convictions (the prosecutor declined to pursue the case), or records with incorrect disposition data.' 
GIS also failed to 'identify root causes of accuracy errors', failed to notice patterns of mistakes, and did not test the accuracy of non-disputed reports, the CFSB alleged.
The two companies were required to pay a USD 2.5 million civil penalty, overhaul their compliance procedures, retain an independent consultant, and develop a comprehensive audit programme.
Operator: General Information Services (GIS); e-Background-checks.com (BGC) Developer: General Information Services (GIS); e-Background-checks.com (BGC) Country: USA Sector: Business/professional services Purpose: Assess job applicant backgrounds Technology: DatabaseIssue: Accuracy/reliability Transparency: Governance"
Sao Paulo METRO SecureOS facial recognition,"São Paulo court judge Cynthia Thome has ordered the company responsible for running Sao Paulo's metro system to suspend use of its facial recognition system as part of its broader implementation of the SecurOS electronic surveillance system, ZDNet  reports.
Brasilian consumer rights body Idec, civil rights group Intervozes and other rights organisations had lodged the lawsuit against Companhia do Metropolitano de São Paulo (METRO) alleging abuse of privacy on the basis that it had failed to inform subway users about the purpose and scope of its facial biometrics system and data storage and sharing practices, and had not obtained users' consent.
In addition to suspending its SecurOS system, METRO was ordered to stop the roll-out of new equipment for data capture and biometrics processing for facial recognition, and was set a daily fine should it fail to comply.
Information about METRO's facial biometrics system had been released during a May 2021 lawsuit in which METRO operator Via Quatro had been ordered to terminate its 'abusive' use of facial recognition technology installed in platform doors.
Operator: Companhia do Metropolitano de São Paulo (METRO) Developer: Intelligent Security Systems (ISS) Country: Brazil Sector: Govt - transport Purpose: Strengthen security; Increase safety Technology: Facial recognitionIssue: Privacy; Surveillance; Accuracy/reliability; Bias/discrimination - race, ethnicity, LGBTQ Transparency: Governance; Privacy; Black box; Marketing; Legal"
"Tesla FSD beta test car hits bollard, driver fired","Tesla has fired an employee in its Autopilot division for publicly sharing a video of a 2021 Model 3 Full-Self Driving (FSD) beta test car colliding with a bollard on the side of a road in San Jose, California.
John Bernal, who has been creating videos about Tesla's FSD beta for over a year on his AI Addict YouTube channel, posted his latest video three days after 2021 Model 3 hit a bollard separating a car lane from a bike lane in San Jose. 
'No matter how minor this accident was, it's the first FSD beta collision caught on camera that is irrefutable' Bernal argued in a later frame-by-frame video of the incident.
Bernal told CNN that an Autopilot manager had attempted to stop him posting negative or critical content involving FSD Beta in a meeting before his dismissal.
According to CNN, Tesla’s social media policy makes no direct reference to criticising the company’s products in public, nor does it specifically reference YouTube as a social media channel. 
Tesla has not publicly commented on the collision, nor why Bernal was fired.
Operator: John BernalDeveloper: TeslaCountry: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer vision Issue: Safety; Accuracy/reliability; Employment Transparency: Governance; Black box; Marketing; Legal"
Ukraine decision to use Clearview AI facial recognition draws concerns,"Ukraine's decision to use Clearview AI's facial recognition technology to identify Russian soldiers, prisoners of war, and undercover saboteurs prompted concerns about what happens if the system makes mistakes.
Ukraine's vice prime minister Mykhailo Fedorov told Reuters that the country's defence ministry is using the controversial system. He also said it would 'dispel the myth of a 'special operation' in which there are 'no conscripts’ and ‘no one dies,' according to a message posted on Telegram by Federov noted by Forbes cybersecurity writer Thomas Brewster.
Despite broad support for Ukraine, human and civil rights experts are concerned about what happens if a person is wrongly identified, arrested or worse should Clearview's system make a mistake. In theory, mistakes are more likely given it will have to handle war-scarred faces. 
A 2019 US Department of Energy study concluded that facial decomposition considerably reduced the technology's effectiveness, though a 2021 conference paper was more promising.
Operator: Ministry of Defenсe of Ukraine Developer: Clearview AI Country: UkraineSector: Govt - defencePurpose: Identify Russian combatants Technology: Facial recognition Issue: Accuracy/reliability; Dual/multi-use Transparency: Governance; Black box"
TikTok exposes new users to Russia/Ukraine war disinformation,"People opening new accounts on TikTok are quickly shown misinformation and disinformation about the Russia/Ukraine war.
An investigation by anti-misinformation company NewsGuard discovered, by setting up a pair of tests to see how TikTok handled information about the war, that a new account that simply scrolled the app’s For You feature and watched videos about the conflict would be shown false or misleading content within 40 minutes. 
According to the Newsguard researchers, 'none of the videos ... contained any information about the trustworthiness of the source, warnings, fact-checks, or additional information that could empower users with reliable information.'
Operator: ByteDance/TikTokDeveloper: ByteDance/TikTokCountry: USA; Global Sector: Media/entertainment/sports/arts; Govt - defence; Politics Purpose: Recommend content Technology: Recommendation algorithm Issue: Mis/disinformation Transparency: Governance; Black box"
VioGén gender domestic violence protection system,"A new report (pdf) by algorithmic auditors Eticas Consulting and gender violence campaign organisation Ana Bella Foundation has concluded that the Spanish government's VioGén system underestimates the risk of women being subjected to domestic abuse.
Launched in 2007 by Spain's Ministry of Interior to help local authorities across Spain protect women and children from domestic gender violence, and to co-ordinate their activities, VioGén evaluates the degree of risk of aggression to women and assigns a score which determines the level of police protection they should receive.
According to the Spanish authorities, VioGén has been a success, resulting in a 25% reduction in repeat attacks. 
By contrast, Eticas' audit finds that only 1 out of 7 women who contacted the police for protection received help in 2021, and that a small minority of women received a risk score of 'medium' or higher, thereby qualifying them for police protection.
But while risk scores can be changed manually for those perceived to be at greater risk than the model suggests, a 2014 study discovered that Spanish police offficers stuck to the automated outcome in 95% of cases. 
The tendency to over-rely on VioGén has resulted in a rash of cases deemed to have 'low' or 'non-specific' risks ending in suicides, physical assaults, and the murder of women and children - something compounded by the low level of human oversight of the system, according to Eticas.
The Spanish Ministry of the Interior has not opened the VioGén system to third-parties for review, audit, or investigation, leading critics to accuse the authorities of inadequate transparency. The system also appears to be largely unaccountable. 
As Eticas' report points out (pdf), women and womens' organisations were not involved in the design of the VioGén system at any stage. 80% of women users of the system surveyed by the audit company view it negatively.
Furthermore, the fact that VioGén is more or less fully automated means it should be subject to Spain's tougher Régimen Jurídico de la Función Pública, Eticas argues.
Operator: Ministry of the Interior; Spanish National Police Developer: Ministry of the Interior; SAS Country: Spain Sector: Govt - police Purpose: Assess domestic violence risk Technology: Risk assessment algorithm Issue: Accuracy/reliability; Fairness Transparency: Governance; Black box"
Russian KUB-BLA 'suicide drone' attacks,"The Bulletin of the Atomic Scientists reports that Russia appears to be using Kalashnikov ZALA Aero KUB-BLA loitering drones in its war with Ukraine. Per Kalashnikov, KUB-BLAs are able to select and strike targets through inputted coordinates or autonomously, with the system capable of 'real-time recognition and classification of detected objects' using AI, according (pdf) to the Netherlands-based peace organisation Pax for Peace.
Open source intelligence together with images released by the Russian government indicate AI technologies are being used widely in the Russia-Ukraine war, raising concerns about the ethics and accuracy of so-called 'killer robots'.
The Ukrainian air force has also been using Bayraktar TB2 drones during the war to devastating effect. 
Operator: Russian Aerospace Forces Developer: Rostec/JSC Kalashnikov Concern/ZALA Aero Country: Ukraine; Russia Sector: Govt - defence Purpose: Kill/maim/damage/destroy Technology: Drone; Object recognitionIssue: Lethal autonomous weapons; Ethics Transparency: Governance"
Estée Lauder employees fired after automated performance assessments,"Cosmetics company Estée Lauder agreed an out-of-court settlement with three make-up artists who were sacked after taking an automated job application assessment.
Having had to reapply for their positions, the women had their answers and expressions analysed by recruitment analysis software supplied by HireVue, with the results measured against other data about their job performance. Estée Lauder subsidiary MAC Cosmetics had not previously warned any of the three women of performance issues, leading them to conclude they had been unfairly treated and to begin legal proceedings against the company. 
Former MAC make-up artist Anthea Mairoudhiou later described losing her job as 'the end of the road of that career.' 'And, mentally it massively affected me. I felt very let down, and I thought I was going mad, she added. 
An Estée Lauder spokesman said 'In the situation described, facial recognition accounted for well under 1% (0.25%) of employees’ overall assessment. The company has teams who overlay objective performance-related data and other qualitative feedback, which accounted for the majority of the employment assessment, to make decisions on employment,' he added.
Operator: Estée Lauder/MAC Cosmetics Developer: HireVue Country: UK Sector: Cosmetics Purpose: Assess employee performance Technology: Facial recognition; Behavioural analysis Issue: Accuracy/reliability; Fairness Transparency: Governance; Black box"
President Zelenskyy deepfake surrender,"A video circulating online appearing to show Ukraine president Volodymyr Zelenskyy instructing his army to lay down their arms and surrender has been shown to be a deepfake. 
According to disinformation experts the Atlantic Council’s Digital Forensic Research Lab, the video was aired on hacked news programme Ukraine 24.
Ukraine 24 quickly posted a warning on Facebook that it had been hacked and that the video was fake, and Facebook removed it from its platform for violating its policy against misleading manipulated media.
Zelenksy responded to the deepfake in a video posted to Facebook, calling for Russians troops to surrender.
Operator: Anonymous/pseudonymous Developer: Unclear/unknown Country: Ukraine; Russia Sector: Politics Purpose: Scare/confuse/destabilise Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation; Ethics Transparency: Governance; Privacy"
Weight Watchers child data harvesting,"The company formerly known as Weight Watchers collected data on children under 13 through Kurbo, a weight loss app it owns, according to a US Federal Trade Commission (FTC) settlement order. 
The FTC says hundreds of children were able to sign up to use Kurbo using false birthdays without parental permission and could then change their ages once they had registered. 
Kurbo also failed to inform parents about its data collection practices for kids, and kept the data for many years, possibly to help develop other services. 
The FTC has ordered WW International (aka Weight Watchers) to 'destroy any algorithms derived from the data', and to pay a USD 1.5 million penalty.
Since Kurbo was acquired by Weight Watchers in 2018, the weight loss app has been the subject of regular criticism from dietitions and others.
Operator: WW International/Weight Watchers/Kurbo Developer: WW International/Weight Watchers/Kurbo Country: USA Sector: Consumer goods Purpose: Manage eating habits Technology: Application Issue: Privacy; Appropriateness/need Transparency: Governance; Privacy; Marketing"
Everalbum facial data harvesting,"In August 2020, NBC News reported that cloud photo storage app Ever is using its users' photographs to train its facial recognition technology (FRT) system without informing them or letting them turn the system off. 
Furthermore, the report discovered that Ever was selling its FRT system to companies, police, and military customers. 
The report sparked multiple privacy and civil rights organisations to accuse Ever of egregious privacy abuse and opaque and misleading marketing, and to a wave of complaints by its users.
The furore also prompted a January 2021 complaint (pdf) by the US Federal Trade Commission (FTC), which was settled (pdf) in May 2021, with Ever instructed to all user data harvested from its app and to delete 'any models or algorithms developed in whole or in part' using that data. 
Ever rebranded as Paravision days after NBC's 2019 report and closed Ever in August 2020, blaming increased competition from Apple and Google. 
Operator: Paravision/Everalbum Developer: Paravision/Everalbum Country: USA Sector: Technology Purpose: Train facial recognition system Technology: Facial recognition Issue: Privacy Transparency: Governance; Privacy; Marketing"
Russian Ukraine disinformation bot farms,"Ukrainian authorities have shut down two Russian bot farms that were spreading false rumours about Ukraine, bomb threats and other disinformation designed to sow panic amongst the Ukranian people.
The Security Service of Ukraine (SSB) took down a bot farm in Lviv it said was capable of deploying up to 18,000 fake accounts. And another smaller one that reportedly used 7,000 accounts to post fake information on Telegram, WhatsApp and Viber was also discovered.
Facebook parent company Meta also shut down a fake, pro-Russian influence campaign attempting to spread anti-Ukrainian propaganda using dozens of AI-generated fake profiles.
Operator: Anonymous/pseudonymous Developer: Unclear/unknown Country: Ukraine; Russia Sector: Politics Purpose: Scare/confuse/destabilise Technology: Bot/intelligent agent Issue: Mis/disinformation Transparency: Governance"
Restaurant owner dies after Coupang Eats star rating dispute,"A restaurant owner in Seoul, South Korea, collapsed and died from a brain haemorrhage due to unreasonable demands for a refund made by a customer who ordered food through food delivery platform Coupang Eats. 
The incident resulted in protests by civic groups critical of the company's focus on cutting costs and improving delivery speed, ignoring the ability of restaurant owners and other suppliers to defend themselves, poor transparency and communications, and poor worker safety and working conditions.
The customer demanded a refund of 2,000 won (USD 1.77) on the basis that one of the three fried shrimp the restaurant delivered was a 'strange color', verbally abused the owner of the restaurant owner and insulted her parents and, despite receiving a refund, left a one-star rating.
According to the owner's family, Coupang Eats did not try to arbitrate in the dispute between the owner and the customer. Instead, it simply asked the owner to comply with the customer's demands.
Coupang Eats later apologised and said it would set up a department dedicated to protecting restaurant owners, in addition to introducing a function whereby owners can respond to poor reviews.
Operator: Coupang/Coupang Eats Developer: Coupang/Coupang EatsCountry: S KoreaSector: Transport/logistics Purpose: Rate products/services Technology: Rating system Issue: Robustness; Ethics; Employment Transparency: Governance; Complaints/appeals; Black box; Legal; Marketing"
VRChat online virtual reality universe,"VRChat is an online virtual-reality universe that enables users to interact with others using avatars and worlds they can create.
Founded in 2015, VRChat is thought to be the most popular social virtual reality platform, and is the most reviewed social app in Facebook’s VR Metaverse.
In December 2021, research by the Center for Countering Digital Hate (CCDH) showed that VR Chat filled with abuse, harassment, racism, and pornographic content.
In February 2022, a BBC report detailed a reseacher's experience posing as a 13-year-old girl who witnessed child grooming, racist insults, avatars simulating having sex, and threats of rape in VRChat.
According to Andy Burrows, head of online child safety policy at the UK's NSPCC (National Society for the Prevention of Cruelty to Children), VRChat 'is a product that is dangerous by design, because of oversight and neglect.'
Despite a ban on explicit content on VRChat, Buzzfeed reported in February 2023 that people are easily skirting the rules by attending private, invitation-only orgies in virtual sex clubs and age-restricted Discord servers. 
According to Buzzfeed, an estimated 'several thousand' VRChat users engage in erotic role play (ERP), or VR sex, with participants spending hundreds of dollars on Bluetooth-enabled sex toys and tools that help 'make fornication-via-avatar an immersive and more physically pleasurable experience.'
Other examples of poor virtual world safety that have recently come to light include sex parties in Roblox Condos, and virtual groping in Meta's Horizon Worlds.
Operator: VRChat; Meta/Quest; Microsoft Developer: VRChat Country: UK; USA; Global Sector: Media/entertainment/sports/arts Purpose: Manage system safety Technology: Safety management system; Virtual realityIssue: Safety; Privacy; Security Transparency: Governance; Black box"
South Korea presidential election candidate deepfakes,"South Korean politician Yoon Suk-yeol has become the world's first 'deepfake candidate' to win a presidential election, beating rival Lee Jae-myung in a tight race.
'AI Yoon''s team turned to avatars and short-form videos to explain policy ideas and lampoon his rival in a bid to win over younger voters.
However, some voters have complained that it is an inaccurate physical representation and paints an untrue political picture of Yoon. Others have pointed out the potential for misuse of deepfakes in the form of mis- and disinformation.
AFP says South Korea's election monitor allowed AI candidates to campaign on the condition deepfake technology is clearly identified, and does not spread misinformation.
A recent study published in the Proceedings of the National Academy of Sciences USA suggests that real humans can easily fall for deepfake faces and tend to interpret them as more trustworthy than the real thing.
Operator: People Power Party Developer: People Power Party; DeepBrain AI Country: S KoreaSector: PoliticsPurpose: Communicate with young votersTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Mis/disinformation; Ethics Transparency: Governance"
Uber UpFront Fares algorithm cuts driver earnings,"An Uber algorithm that allowed drivers in 24 cities across the US to see pay and destinations before accepting a trip resulted in lower overall earnings, and in Uber taking a bigger cut of fares, according to The Markup.
Uber says its new Upfront Fare algorithm is based on 'several factors', including base fare, time and distance rates, and real-time demand at the destination. It argues that the new system is more transparent, gives drivers 'more control and choice'., and that drivers are likely to make less money for longer trips but should earn more on shorter trips.
Except California, where Uber started a similar programme in 2020, the company had resisted offering drivers the ability to see the fare and destination before accepting a trip on the basis that drivers may 'cherry-pick trips' or 'discriminate against riders in disadvantaged neighbourhoods'.
Operator: Uber Developer: UberCountry: USASector: AutomotivePurpose: Determine pay Technology: Pay algorithm Issue: Employment - pay; Fairness Transparency: Governance; Black box"
Ethiopia Bayraktar TB2 drone Tigray school attack,"Washington Post analysis appears to show that Ethiopia's government used a Turkish-made Bayraktar TB2 drone in January 2022 to attack civilians sheltering in a school during its 16-month conflict with the Tigray People’s Liberation Front (TPLF). 
Reports say at least 59 civilians died and 30 were wounded, including children, during the strike.
According to the Post's report, remnants of weapons recovered from the school by aid workers showed 'internal components and screw configurations matched images of Turkish-made MAM-L munitions'. MAM-L weaponry pair exclusively with Bayraktar TB2 drones.
Run by Turkey president Recep Tayyip Erdoğan’s son-in-law Selçuk Bayraktar, Baykar has supplied Ukraine, Poland, Qatar, Libya, Kyrgyzstan, Kazakhstan, Ethiopia and Azerbaijan with armed autonomous drones. 
Bayraktar TB2 drones have been used to deadly effect by Ukraine's air force during Russia's invasion of the eastern European country. 
Operator: Ethiopian National Defense Force (ENDF) Developer: Baykar Defence Country: EthiopiaSector: Govt - defence Purpose: Kill/maim/damage/destroy Technology: Drone; Object recognition Issue: Lethal autonomous weapons; Ethics Transparency: Governance"
"Sama 'ethical' data labeling, content moderation","A February 2022 TIME investigation revealed low pay, poor working conditions and alleged union-busting at Sama's office in Nairobi, Kenya for its team moderating content for Facebook. 
TIME found that Kenyan employees for Sama receive a take-home wage equivalent to around USD 1.46 per hour after tax, had to work up to nine hours per day, are continuously monitored, and are measured against metrics for average time spent and quality. The metrics contradict public statements by Facebook about not setting expectations on its contractors. 
According to TIME, at least two Sama content moderators resigned after being diagnosed with mental illnesses including post-traumatic stress disorder (PTSD), anxiety, and depression. Former employee and whistleblower Daniel Motaung told TIME he had been unlawfuly fired for leading over 100 Sama workers in an attempted strike that aimed to secure better pay for staff.
In a blog post responding to TIME's investigation, Sama claimed its rate of pay was fair, arguing 'the article falsely alleges that Sama does not compensate its employees fairly'. Two weeks later the company said it would increase salaries by 30% to 50%.
Meta avoided commenting on Sama employment practices, and has not made public its audits of Sama's Kenya office. Sama reputedly failed to confirm rumours that its managers had attempted to suppress unionisation efforts at its operation in Kenya in 2019.
In January 2023, Time journalist Billy Perrigo revealed that OpenAI used Kenyan workers being paid less than USD 2 an hour to de-toxify Open AI's ChatGPT and GPT-3 large language model. 
According to Perrigo, 'the work’s traumatic nature eventually led Sama to cancel all its work for OpenAI in February 2022, eight months earlier than planned.'
In May 2023, a judge ruled that Meta could be sued in Kenya after 43 moderators at its Nairobi hub filed a lawsuit against the group and Sama for unfair termination. 
Operator: Alphabet/Google; Meta/Facebook; Microsoft; OpenAI; Tesla; Walmart Developer: Sama AI/Samasource Country: KenyaSector: Business/professional services Purpose: Label data; Moderate content Technology: Data labeling system; Content moderation system Issue: Employment - pay, jobs, working conditions, unionisation Transparency: Governance; Marketing - misleading"
Zhihu job resignation predictions,"SupaChina reports a surveillance tool that predicts employee resignations has been flagged by an employee at Zhihu, the largest question-and-answer platform in China.
During lay-off rumours at Shenzhen-based Zhihu, a complaint was made by an anonymous whistleblower, who says he was fired after his manager found out about his plans to quit.
Sangfor Technologies' 'Behavioral Perception System' purportedly calculates employees’ perceived resignation risk by tracking their browsing history and conversations with coworkers, flagging people talking about 'bad treatment', 'no prospects', and 'low wages'.
Sangfor removed information about its product from its website after news of the Zhihu firing hit the headlines. 
Operator: Zhihu; Sina; China Everbright Bank Shenzhen; East China Normal University Developer: Sangfor Technologies Country: ChinaSector: Business/professional services Purpose: Predict employee resignations Technology: Prediction algorithm Issue: Surveillance; Privacy; Accuracy/reliability; Appropriateness/need Transparency: Governance; Privacy; Complaints/appeals; Black box; Marketing"
Ukraine Bayraktar TB2 drone attacks,"The Wall Street Journal reports that Ukraine's air force has been using Turkish-made automated drones to hit Russian targets during the Russian invasion of the east European country.
The Ukrainian air force confirmed it had struck Russian targets, including a military convoy near Kherson, using Bayraktar TB2 drones during the early days Russia's invasion of Ukraine. 
Ukraine’s air force chief Lt. Gen. Mykola Oleshchuk described the drones as 'life-giving'. Human rights and non-governmental organisations have been calling for a global ban on lethal autonomous weapons systems (aka 'slaughterbots').
In October 2021, Ukrainian officials said its air force had used TB2 drones to destroy a Russian howitzer in the Donbas region of eastern Ukraine.
A UN report published in March 2021 reported that Turkish-made STM Kargu-2 drones were used by Libyan forces loyal to the Government of National Accord (GNA) to hunt down units loyal to former Libyan Field Marshall Khalifa Haftar.
In January 2021, a Bayraktar TB2 drone was used to attack civilians sheltering in a school in Tigray, Ethiopia, leaving at least 59 dead, according to analysis by the Washington Post.
Operator: Ukrainian Air Force Developer: Baykar Defence Country: Ukraine; Russia Sector: Govt - defence Purpose: Kill/maim/damage/destroy Technology: Drone; Robotics Issue: Lethal autonomous weapons; Ethics Transparency: Governance"
Russia 'Kyiv' deepfake news influence campaign,"Facebook parent company Meta says it has shut down a fake, pro-Russian influence campaign attempting to spread anti-Ukrainian propaganda.
According to Meta, the campaign took the form of dozens of fake profiles using AI-generated pictures, including of an aviation engineer, news editors, and scientific authors. 
Posing as independent news desks operating out of Kyiv, the fake profiles were used to publish content on Facebook, Instagram, Twitter, YouTube, Telegram, and Russian social networks Odnoklassniki and VK.
The profiles were taken down for violating Meta's coordinated inauthentic behaviour policy.
Ukrainian authorities also shut down a Russian bot farm in Lviv capable of deploying 18,000 fake accounts, and a smaller one that posted fake information about Ukraine on Telegram, WhatsApp and Viber.
Operator: Meta/Facebook/Instagram; Twitter; Alphabet/Google/YouTube; Telegram; Odnoklassniki; VK   Developer: Meta/Facebook/Instagram; Twitter; Alphabet/Google/YouTube; Telegram; Odnoklassniki; VK  Country: Ukraine; Russia Sector: Politics Purpose: Moderate content Technology: Content moderation system; Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation; Ethics Transparency: Black box"
Roblox Condo nazi sex parties,"According to the BBC, children gamers under 13 years old are meeting in virtual 'condos' on gaming platform Roblox to talk about and try and have sex. One was spotted wearing a Nazi uniform.
Whilst Roblox says offending condos are quickly shut down, the BBC's report adds to a growing concerns about the safety of so-called virtual reality-based metaverse platforms. Reports of sexual harassment have also plagued Meta.
Roblox later published a blog post setting out how it support and protect its developer and user communities.
Operator: RobloxDeveloper: RobloxCountry: UK; GlobalSector: Media/entertainment/sports/arts Purpose: Manage system safety Technology: Virtual reality; Safety management systemIssue: Safety; Bias/discrimination - raceTransparency: Governance; Black box"
Twitter Ukraine OSINT account suspensions,"CNBC reports that Twitter has mistakenly suspended several accounts that had been sharing footage and intelligence on Russian military movements. Russia invaded Ukraine several hours later.
In a statement, a Twitter spokesman said 'We’ve been proactively monitoring for emerging narratives that are violative of our policies, and, in this instance, we took enforcement action on a number of accounts in error. We’re expeditiously reviewing these actions and have already proactively reinstated access to a number of affected accounts.'
Yoel Roth, Twitter head of site integrity, tweeted the firm was closely investigating what had happened but said mass reporting was not a factor. 'We do not trigger automated enforcements based on report volume, ever, exactly because of how easily gamed that would be.' 
Operator: TwitterDeveloper: TwitterCountry: UkraineSector: Govt - defencePurpose: Moderate content Technology: Content moderation system Issue: Mis/disinformation Transparency: Governance; Black box"
MoviePass PreShow eye tracking,"Vice reports that MoviePass is returning as an app that will track users’ eyes when they watch adverts, leading commentators to question whether people would be willing to surrender their privacy to watch movies on a small screen. 
The new service is envisaged to use facial recognition and eye-tracking technology to ensure that viewers are looking directly at adverts in exchange for access to films.
To earn movie tickets, users must keep their gaze in frame, and look at their screens for the whole ad. A red border appears if they look away, pausing the ads. 
MoviePass shuttered in 2019 and declared bankruptcy in 2020. In addition to its stuttering financial performance, it was plagued with numerous allegations of playing fast and loose with customer data.
Operator: MoviePassDeveloper: MoviePassCountry: USASector: Media/entertainment/sports/arts Purpose: Earn virtual currency Technology: Facial recognition; Eye tracking Issue: Appropriateness/need; Privacy Transparency: Privacy"
The Book of Veles disinformation,"A book by celebrated Magnum photographer Jonas Bendiksen documenting life in Veles, the 'fake news' capital of North Macedonia, turns out to be a work of disinformation itself.
Purporting to investigate disinformation production activities in a rural town notorious for its disinformation and misinformation factories, Bendiksen's The Book of Veles has been positively reviewed and screened at film festivals. 
However, with Veles' disinformation industry in decline, Bendiksen had manipulated his photos using 3D software to make them more realistic. He also used a large language AI model to generate the accompanying commentary. 
The author only confessed his manipulation after a Veles journalist took to Twitter to expose his fraudulent activities. 
Magnum Photos continues to sell Bendiksen's book.
Operator: Magnum Photos Developer: Jonas Bendiksen Country: N Macedonia Sector: Media/entertainment/sports/arts Purpose: Expose mis/disinformation Technology: NLP/text analysis Issue: Mis/disinformation; Ethics Transparency: Marketing"
US prison inmate Verus call monitoring,"Reuters reports that US prisons are rolling out an AI-based system that scans phone calls, leading rights groups to voice  concerns about surveillance and privacy. 
Intended to keep prisons and jails safe, Leo Technologies' Verus system uses Amazon speech-to-text technology based on keywords to identify and transcribe prisoners' phone calls. 
However, groups such as Access Now, the Electronic Frontier Foundation and Worth Rises worry it abuses the privacy rights of family members and others with whom prisoners are talking.
Another issue is discrimination. Research shows voice-to-text tools are significantly more inaccurate for Black voices. And a higher percentage of Black people are detained or jailed in the US.
Surveillance is also considered a concern. Reuters quotes documents that reveal instances of prisons and detention centres using Verus for purposes beyond maintaining safety, for example identifying conversations that would help them win court cases.
It has also been used to monitor COVID-19. The Intercept had earlier reported that prison officials in several US states have been using Verus to scan inmate calls for COVID-19 mentions and to flag complaints about the quality of prison response to the pandemic. 
A Verus brochure states the software can be used to 'identify sick inmates, help allocate personnel in understaffed prisons, and prevent “COVID-19 related murder.'
Operator: Suffolk County Sheriff's Office; Multiple Developer: LEO Technologies
Country: USA
Sector: Govt - justice
Purpose: Improve safety
Technology: Speech-to-text AI
Issue: Surveillance; Privacy; Bias/discrimination - race, ethnicity Transparency: Governance"
Hackney Early Help Profiling System (EHPS),"The Hackney Citizen reports that Hackney Council has ended its controversial Early Help Profiling System (EHPS) pilot scheme.
The EHPS system was designed by software company Xantura to alert council social workers to families in need of extra support based on a child’s social care referral history, youth offending, and other data. 
However, the council admitted EHPS, which cost GBP 361,400, has not 'realised the expected benefits' and had been dropped. 
The system has been dogged by accusations of lack of transparency concerning its existence, use, and privacy consent. Xantura denied third-party researcher and auditor access to EHPS on commercial grounds.
Operator: Hackney Council Developer: Xantura
Country: UK
Sector: Govt - welfare
Purpose: Predict child harm
Technology: Prediction algorithm
Issue: Accuracy/reliability; Effectiveness/value; Privacy Transparency: Governance; Black box"
UK DWP 'General Matching Service' disability benefits fraud algorithm,"In December 2021, The Mirror reported that the UK Department for Work and Pensions (DWP) is facing legal action to reveal details of the 'General Matching Service' algorithm it is using to identify benefit fraud.
The Greater Manchester Coalition of Disabled People (GMCDP) is demanding the DWP reveal how it uses 'AI, algorithmic technology and other forms of automation' when investigating benefit fraud, specifically details of a data matching algorithm that the department is known to use.
The GMCDP alleges that disabled people are being unfairly targeted, are subject to 'essential' cash cuts and are given little or no information about why they are being investigated. 
It is the second legal letter sent by the GMCDP on the subject. The first was issued in December 2021.
Early 2021, Privacy International accused the DWP of using 'excessive surveillance techniques' to investigate possible benefit fraudsters.
Operator: Department for Work and Pensions (DWP) Developer: Department for Work and Pensions (DWP)
Country: UK
Sector: Govt - welfare
Purpose: Identify fraud
Technology: Data matching algorithm
Issue: Bias/discrimination - disability Transparency: Governance; Complaints/appeals; Black box"
Amazon chemical food preservative suicides,"Amazon is facing pressure from a bipartisan set of US federal lawmakers to stop sales of a food preservative containing a chemical compound that is being used as a poison in suicides.
The internet company has come under further pressure after a New York Times investigation identified 10 people who had killed themselves after buying a chemical compound through the site in the past two years, and over 300 members of a suicide website who had publicly said they intended to kill themselves using it.
While the compound is sold legally in the US and many other countries, Etsy, eBay and some other internet commerce companies have banned its sale on their platforms. Some firms have also stopped making the products containing the compound.
Families of suicide victims have been warning Amazon of the danger of selling the compound since it became clear it was being used in suicides in 2017. Nonetheless, it has resisted limiting or stopping its sale. 
The NYT also reports that Amazon's algorithm recommends other items people planning suicide typically purchase. Furthermore, Lori Trahan, a prominent Democrat lawmaker, suggested that reviews warning others about the product have been removed on Amazon.
Operator: AmazonDeveloper: Amazon
Country: USA; India
Sector: Retail
Purpose: Recommend products
Technology: Recommendation algorithm
Issue: Safety Transparency: Governance; Complaints/appeals; Black box"
Tinder Plus personalised pricing algorithm discrimination,"Research studies reveal Tinder has been charging users over 30 and gay and lesbian aged 18-29 more than others in multiple countries, generating accusations of systemic age and sexual discrimination. 
A February 2022 study by Consumer Reports finds users aged 30+ are being charged over 65% more than others in New Zealand, Brazil, India, the Netherlands, South Korea and the US. 
The findings are supported by research by Which?, which discovered that the dating app has been increasing prices for gay and lesbian users aged 18-29 in the UK.
An August 2020 study by CHOICE had found users in Australia were being charged up to five times as much as others, with older people charged more.
Tinder uses personalised algorithmic pricing for every user, based on an estimation of what they are willing and able to pay. However it refuses to reveal how its pricing system works. 
Facing public petitions and multiple legal threats, Tinder says it stopped the practice of charging users different prices based on how old they are in the US and Australia. 
Tinder’s parent company Match Group revealed in an earnings call last week that it would stop the practice in remaining markets.
Operator: Match Group/Tinder Developer: Match Group/Tinder
Country: New Zealand; Brazil; India; Netherlands; South Korea; USA
Sector: Media/entertainment/sports/arts
Purpose: Determine pricing
Technology: Pricing algorithm
Issue: Fairness; Bias/discrimination - age, LGBTQ Transparency: Governance; Black box"
Honolulu homeless robot temperature tests,"Honolulu's police department has been using a robotic dog to take the temperatures of homeless people, according to VICE.
Using a public records request, VICE discovered that the Honolulu Police Department (HPD) deploys a robot dog on regular  'temperature' duty of unhoused people living in encampments. 
Critics described the practice as 'insulting', 'inhuman' and 'dystopian'. By contrast, Honolulu PD officer Mike Lambert described it as 'the most innovative program in the nation' during a city council meeting.
The HPD spent USD 150,045 in federal funds earmarked for COVID-19 pandemic relief to acquire the Boston Dynamics Spot robot, according according to city spending data first uncovered by Honolulu Civil Beat.
HPD Deputy Director described the robot as 'more than a ‘thermometer' in an email interview with Civil Beat. In the same report, another officer described the purchase as 'Toys, toys, toys'.
This is not the first time controversy has dogged Spot. The NYPD was hauled over the coals for using it during a hostage operation in New York. 
Operator: Honolulu Police Department Developer: Hyundai Motor Group/Boston Dynamics
Country: USA
Sector: Govt - police
Purpose: Strengthen law enforcement; Detect body temperature
Technology: Robotics
Issue: Appropriateness/need; Effectiveness/value Transparency: Governance
ASSESS DATABASE
https://honolulu.granicus.com/player/clip/1473
https://www.vice.com/en/article/wx5xym/honolulu-police-used-a-robot-dog-to-patrol-a-homeless-encampment
https://www.vice.com/en/article/v7dz7b/police-outsourcing-human-interaction-with-homeless-people-to-boston-dynamics-robot-dog
https://www.civilbeat.org/2021/01/honolulu-police-spent-150000-in-cares-funds-on-a-robot-dog/
https://www.hawaiinewsnow.com/2021/08/28/hpd-defends-use-pricey-robot-dog-taking-temperatures-homeless-program/
https://www.dailymail.co.uk/news/article-10448379/Cops-Honolulu-use-robot-dog-temperature-homeless-people.html
https://www.ubergizmo.com/2022/01/boston-dynamics-dog-measure-homeless-temperature/
https://www.euronews.com/next/2021/08/06/a-useful-tool-or-dehumanising-robot-police-dog-that-scans-homeless-people-sparks-debate
https://metro.co.uk/2022/01/29/robot-cop-dog-taking-homeless-peoples-temperature-amid-covid-16010881/
https://www.dailywire.com/news/hawaii-using-robot-dog-to-patrol-homeless-community-for-signs-of-covid-19
https://boingboing.net/2022/02/07/robot-dogs-to-patrol-u-s-border.html
https://gizmodo.com/honolulu-police-department-used-150-000-in-cares-funds-1847092760
https://www.inputmag.com/culture/honolulu-police-used-a-robot-dog-to-police-a-homeless-plague-camp
New York police 'digidog'
Singapore Xavier patrol robots
Page infoType: IncidentPublished: January 2022Last updated: February 2022"
Tesla phantom braking,"Tesla owners are complaining about an automatic braking issue, with cars suddenly slamming the brakes at high speeds and nearly causing crashes.
According to the Washington Post, the issue can be traced to Tesla's decision to stop using radar sensors in new vehicles in order to move a purely camera-based system known as Tesla Vision, and to an update to its Full-Self Driving beta programme.
The carmaker had to recall the update in October over false positives to its automatic emergency-braking system, acknowledging that these were triggered by the software update.
On February 16, 2021, the US National Highway Traffic Safety Administration (NHTSA) announced an investigation into Tesla's sudden braking issue, covering an estimated 416,000 vehicles.
Tesla was recently forced to recall and disable a rolling stop function in 53,822 Model S, X, 3 and Y vehicles equipped with FSD. 
Operator: 
Developer: Tesla
Country: USA
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system; Self-driving system; Computer vision
Issue: SafetyTransparency: Black box"
Crisis Text Line commercial data sharing,"Under pressure from politicians, regulators, privacy experts and mental health practitioners, US-based mental health non-profit Crisis Text Line (CLT) is ending its practice of sharing confidential data with customer service company Loris.ai.
Earlier, Politico had discovered that Loris had been using CLT data and insights to develop, market and sell customer service optimisation software, prompting concerns about the non-profit's governance, ethics and transparency, and the commercial nature of the two organisations' relationship.
CLT's initial response to Politico's findings had been marred by its mischaracterisation of an earlier review of its academic research data policies by privacy organisation EPIC. 
Operator: Crisis Text Line
Developer: Crisis Text Line
Country: USA
Sector: NGO/non-profit/social enterprise
Purpose: Provide mental health support
Technology: Chatbot; NLP/text analysis
Issue: Privacy; Confidentiality; Security; Ethics Transparency: Governance; Marketing"
Lauren Book deepfake extortion,"Florida lawmaker Lauren Book has been subjected to an extortion attempt using deepfake nude photos, according to the AP and other media outlets. 
South Florida teenager Jeremy Kamperveen allegedly attempted to extort Book by demanding USD 5,000 in exchange for not releasing sexually explicit photographs of her showing 'female genitalia and the portrayal of a sexual act' to Fox News.
Stolen graphic images of Book and her husband had reputedy been circulating online since 2020. Kamperveen pleaded no contest to charges of extortion and cyberstalking in June 2022 and was later sentenced to one year and one day in prison, followed by 10 years of probation.
Book responded by introducing legislation would strengthen Florida’s revenge porn law by making it a felony to buy, sell or trade stolen sexually explicit images from someone’s phone or other digital devices. It also makes disseminating altered or created sexually explicit images a felony. 
Florida's Sexually Related Offense law was signed by Governor Ron DeSantis on June 27, 2022 and took effect on October 1, 2022.
Operator: Jeremy Kampervee
Developer: Jeremy Kampervee
Country: USA
Sector: Politics
Purpose: Extortion
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning 
Issue: Privacy; Ethics Transparency: Governance; Privacy"
Houthi Abu Dhabi oil facility drone attack,"The UAE’s Ministry of Interior has announced a complete ban on the use of recreational drones across the country. 
The ban comes in the wake of an attack in early January on an oil facility in Abu Dhabi by Yemen's Houthi rebels using multiple weapon-laden drones.
Anyone caught violating the ban can face up to five years in prison and a minimum fine of Dh100,000, according to the state's Public Prosecution office.
The attack is seen to raise concerns about the misuse of drones, and more generally about the nature and use of lethal autonomous and semi-autonomous weapons. 
Operator: Ansar Allah
Developer: Ansar Allah
Country: UAE - Abu Dhabi; Yemen
Sector: Govt - energy; Govt - transport
Purpose: Kill/maim/damage/destroy
Technology: Drone
Issue: Lethal autonomous weapons; Ethics; Dual/multi-use Transparency: Governance"
Replika app AI 'companion' chatbot,"Replika is a personalised AI chatbot that is meant to serve as artificial intelligence (AI) friend or mentor. It was created by Russian programmer Eugenia Kuyda in 2017 as the 'AI companion that cares'.
Replika is 'fine-tuned' on OpenAI's GPT-3 large language model, with each Replika (or Rep) learning from user responses and ratings to become more like its user.
A Futurism investigation found users of Replika are regularly abusing their Reps and sharing their grievances on Reddit and other social media platforms. The finding suggested many Replika users are creating on-demand romantic and sexual AI partners, resulting in discussion about the nature of pyschological dangers of users attaching human traits to their AI creations.
In January 2023, Vice reported that Replikas had been harassing their users, prompting Kuyda to deny that Replika had ever 'positioned' the app as a source for erotic roleplay or adult content, and was placing greater emphasis on safety. However, Replika users soon started complaining that their Reps aren't interested in NSFW discussion and behaviour. 
In March 2023, Replika announced it was restoring erotic role-play for some users.
In July 2023, it transpired that an attempt to kill the late Queen Elizabeth II by an intruder who entered Windsor Castle wielding a crossbow had been 'encouraged' by Replika.
A May 2023 Mozilla Foundation assessment of mental health apps found Replika ton be 'plagued by weak password requirements, sharing of personal data with advertisers, and recording of personal photos, videos, and voice and text messages consumers shared with the chatbot.'
In February 2023, Replika was ordered to stop processing Italians' data on the basis that it lacks a proper legal basis for processing children’s data under the EU’s GDPR, and that it poses risks to minors.
Operator: Luka Inc
Developer: Luka Inc
Country: Global
Sector: Media/entertainment/sports/arts
Purpose: Provide companionship
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning
Issue: Anthropomorphism; Privacy; SafetyTransparency: Governance; Marketing"
Voiceverse NFT voice theft,"Voice NFT start-up Voiceverse has been caught appropriating third-party voice lines without permission by the founder of text-to-speech service 15.ai, and labelling it as 'Powered By Voiceverse'.
Voiceverse later apologised and pinned the blame on its marketing team. The company describes itself on its website as having been 'built with unique, next-generation AI to provide everyone a ""Voice for the Metaverse""'. 
Voiceverse is the latest NFT company to have attracted unwanted publicity. Others have been dragged over the coals for theft, fraud, lax security, and environmental damage, amongst other issues.
Operator: Voiceverse
Developer: Voiceverse
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Sell voice rights
Technology: Voice synthesis; Blockchain
Issue: Copyright; HypocrisyTransparency: Governance; Marketing"
Tesla FSD Assertive mode rolling stops,"Tesla's 'Full-Self-driving' beta test has an 'Assertive' mode that may perform rolling stops, according to The Verge. Rolling stops are generally considered illegal under US law.
One of three 'profiles' - 'Chill', 'Average' and 'Assertive' - that dictates how a car will behave in certain circumstances, the latter will 'have a smaller follow distance, perform more frequent speed lane changes, will not exit passing lanes and may perform more rolling stops'.
FSD profiles were included in Tesla's October 2021 10.3.1 update. The previous update had been pulled two days after testers complained about false crash warnings and other bugs.
Following meetings with the National Highway Traffic Safety Administration, Tesla recalled (pdf) all 53,822 Model S, X, 3 and Y vehicles with the FSD feature, and says it will disable the rolling stop function. 
Operator: 
Developer: Tesla
Country: USA
Sector: Automotive
Purpose: Control car behaviours Technology: Self-driving system; Computer vision
Issue: Safety; Legal - compliance; EthicsTransparency: Black box"
Mainz police COVID-19 Luca app data theft,"Police in the German city of Mainz stand accused of illegally using data from a COVID-19 tracking app to trace the contacts of a man who died leaving a restaurant. 
Seeking witnesses of the incident, Mainz police (Polizeipräsidium Mainz) tapped into data from Luca, an app that helps track the COVID-19 pandemic in Germany by registering time spent in restaurants and pubs. 
The app also logs the patron's full name, address and telephone number, information that is protected under the country's data protection laws.
Prosecutors apologised in the wake of complaints by local politicians, rights advocates and citizens. The local data protection authority has opened an inquiry. 
Operator: Culture4life; Polizeipräsidium Mainz Developer: Culture4life  Country: Germany Sector: Govt - police Purpose: Track COVID-19Technology: Application Issue: Privacy; SecurityTransparency: Privacy; Legal"
"Tek Fog political manipulation, harassment","India's ruling Bharatiya Janata Party (BJP) stands accused of using a secret software application named Tek Fog to manipulate public opinion and intimidate, harass, and abuse political opponents. 
A detailed, multi-part investigation by The Wire after a whistleblower tip-off indicated that the BJP had been artificially inflating the party's popularity, harassing its critics and manipulating public opinion across a number of major social media and mobile platforms, including WhatsApp, Facebook, and Twitter.
The revelations prompted public condemnation from politicians, rights groups, The Editor's Guild, and The Delhi Union of Journalists, and has spurred an investigation by India's Parliamentary Standing Committee on Home Affairs.
In November 2022, The Wire said it would review its TekFog series in the wake on a controversial investigation conducted by the same team of journalists.
Operator: Bharatiya Janata Party; Mohalla Tech/ShareChat; Persistent Systems Developer: Bharatiya Janata Party; Persistent SystemsCountry: IndiaSector: Politics Purpose: Manipulate public opinion; Harass opponents Technology: Application; NLP/text analysisIssue: Safety; Privacy; Bias/discrimination - race, gender; Mis/disinformation Transparency: Governance; Privacy"
Bulli Bai Muslim women auction,"Open source software development platform Github has come under fire for hosting Bulli Bai, a highly controversial anti-Muslim app. 
Bulli Bai uses the names and personal images of Muslim women in India without their consent in order to stage fake auctions and quickly drew intense criticism from politicians, rights advocates and others. 
Github also hosted Sulli Deals, a near identical app that had run for weeks mid-2021 before being taken down. Github is now under pressure to explain how Bulli Bai, which re-uses much of the same code, was also allowed to operate on its platform.
Owned since 2018 by Microsoft, Github is no stranger to controversy. Amongst other things, the platform hosts libraries of code enabling people to create deepfakes, including for malicious and nefarious purposes.
A number of individuals have been arrested in connection with Bulli Bai and Sulli Deals.
Operator: Anonymous/pseudonymous; Microsoft/Github Developer: Unclear/unknown Country: IndiaSector: Media/entertainment/sports/arts Purpose: Moderate content Technology: Content moderation system Issue: Accuracy/reliability; Ethics; Safety; Bias/discrimination - religion Transparency: Governance; Privacy"
Hyderabad police force activist to remove COVID-19 mask,"Telangana state, India, faces court action for the allegedly illegal use of facial recognition by its police forces when it forced social activist SQ Masood to remove his mask while he was returning home from work during the COVID-19 pandemic so that they could take his photograph and verify his identity.
Backed by the Internet Freedom Foundation, the June 2021 notice claimed Masood's photographs were likely being fed into a facial recognition database. It also argued that state use of facial recognition 'is not backed by law, is unnecessary, disproportionate, and is being done without any safeguards to prevent misuse'.
Police denied using facial recognition in this instance, saying Masood's photograph was not scanned against any database and that facial recognition has only been used during the investigation of a crime or suspected crime. 
Telangana state is said to have amongst the highest density of CCTV cameras in the world, and boasts the most known government facial recognition projects in India. Amnesty International has accused Hyderabad of becoming a 'total surveillance city'.
Operator: Hyderabad City Police Developer: NEC Technologies India Country: IndiaSector: Govt - police Purpose: Reduce crime Technology: Facial recognition Issue: Privacy; Surveillance; Dual/multi-use Transparency: Governance; Complaints/appeals; Privacy"
Tesla Model 3 Paris fatal crash,"Reuters reports that one person has been killed and twenty injured, three of them seriously, when an off-duty taxi driver appeared to lose control of his Tesla Model 3 while taking his family to a restaurant in Paris, France. 
Views conflict on whether the taxi driver was responsible, or if the car had accelerated automatically and its brakes had failed, as claimed by the driver's lawyer. 
The driver has been placed under formal investigation for suspected involuntary manslaughter by French authorities.
A 2020 investigation by the US auto safety regulator into the sudden acceleration of Tesla cars concluded that accidents had been caused by 'pedal misapplication'.
Operator: G7Developer: Tesla Country: France Sector: Automotive  Purpose: Automate steering, acceleration, braking Technology: Driver assistance systemIssue: Safety; Accuracy/reliability Transparency: Black box"
Shanghai AI prosecutor,"China has developed an AI prosecutor which is able to charge people with eight common crimes, including fraud, theft, dangerous driving, obstructing official duties and 'provoking trouble', with over 97% accuracy, according to the South China Morning Post.
Built and tested by the Shanghai Pudong People’s Procuratorate, the system has been trained on over 17,000 cases and can charge a suspect based on 1,000 'traits' gathered from a human-documented description of a case.
Lauded by some for its accuracy, others are concerned about its potential for errors, and question how appeals can be made against its black box system. Others believe the program will be used to stifle freedom of expression, assembly and other forms of dissent.
The AI prosecutor works alongside 'System 206', which was used for the first time in Shanghai in January 2019 and which is said to evaluate evidence, conditions for an arrest, and the degree of danger a suspect poses to the general public. 
Operator:  Developer: Shanghai Pudong People’s Procuratorate; Chinese Academy of Sciences Country: China Sector: Govt - justice  Purpose: Determine criminal guilt  Technology: NLP/text analysis; Voice to textIssue: Accuracy/reliability; Bias/discrimination - multiple; Freedom of expression; Dual/multi-use Transparency: Black box; Complaints/appeals"
Amazon Alexa recommends girl touches electric plug,
Pony.ai driverless test crash,"Reuters reports that Pony.ai has agreed with the National Highway Traffic and Safety Administration (NHTSA) to issue a recall for three vehicles using versions of its automated driving system.
The US DMV had suspended the driverless test permit of the automotive start-up in the wake of a crash of one of its vehicles late October 2021 in Fremont, California. The NHTSA also started an informal inquiry.
According (pdf) to the incident report, the fully autonomous car with no accompanying test driver hit a road center divider and a traffic sign. There were no injuries and other vehicles involved.
The agency says this is the first time it has recalled an automated driving system. Pony.ai had registered 10 Hyundai Motor Kona electric vehicles under its driverless testing permit.
Founded by former Baidu employees, Pony.ai is backed by Toyota Motor Corp.
Operator: Pony.ai
Developer: Pony.ai; Luminar Country: USA
Sector: Automotive Purpose: Automate steering, acceleration, braking
Technology: Automated driving system (ADS); Computer vision
Issue: Safety; Accuracy/reliability Transparency: Black box"
Bucheon COVID-19 facial recognition tracking,"Reports say that South Korean city Bucheon is shortly to begin using facial recognition and other AI technologies to track COVID-19. Around 10,000 cameras will be used to track a confirmed person’s movements, instances of close contact, and whether they are wearing a mask. 
The programme has raised concerns amongst digital rights and privacy experts, who worry that sensitive personal data is being shared with the unnamed private company building the datasets and developing the algorithm without first obtaining permission from the people appearing in the CCTV footage. There are also fears that the system may be used for undeclared purposes.
Meantime, South Korea's justice and immigration authorities are under fire for sharing the facial images and personal details of approximately 170 million travellers without their consent.
Operator: City of Bucheon Developer: Unknown Country: S Korea Sector: Govt - heath Purpose: Track COVID-19 infected individuals Technology: Facial recognition; Gait recognition; Mask recognition Issue: Privacy; Scope creep/normalisation; Dual/multi-use; Surveillance Transparency: Governance; Privacy"
Horizon Worlds beta tester groped by stranger,"A beta tester of Meta's new social virtual reality platform Horizon Worlds has revealed that her avatar was groped by a stranger, raising questions about the safety of metaverses and of Mark Zuckerberg's much-hyped new business direction.
The tester, who later revealed herself as Nina Jane Patel, detailed her experience in a lengthy blog post, describing her treatment as 'horrible' and akin to 'virtual gang rape'.
According to the Technology Review, Meta’s internal review of the incident found that the beta tester had not used the platform's 'Safe Zone' tool which enables users to lock themselves in a secure bubble away from other users until they signal they would like to exit it. 
Shortly afterwards, Meta announced Personal Boundary, a default setting in the Horizon Worlds creation platform and the Horizon Venues live event service that prevents avatars from coming within a set distance of each other, creating more personal space for people and making it easier to avoid unwanted interactions.
Operator: Nina Jane Patel Developer: Meta/QuestCountry: USA Sector: Media/entertainment/sports/arts Purpose: Provide virtual social experienceTechnology: Virtual reality; Safety management systemIssue: Safety Transparency: Governance"
XPeng customer facial recognition,"XPeng Motors has been fined 100,000 yuan (USD 15,710) by the Shanghai Municipal Administration for Market Regulation for illegally using facial recognition to collect biometric data of visitors to its stores in mainland China. 
News of the contravention resulted in a torrent of criticism of the company on local social media.
XPeng had used 22 cameras to collect over 430,000 facial images over a six-month period, ostensibly to better understand its customers. 
The company failed to inform visitors about the system, nor gain their consent, thereby violating China's Consumer Rights Protection Law. 
The company says it has now deleted all 430,000 images and will comply with relevant regulations going forward. China's Personal Information Protection Law came into effect on November 1, 2021. 
Operator: XPeng MotorsDeveloper: UnknownCountry: ChinaSector: AutomotivePurpose: Understand customers; Improve service Technology: Facial recognition Issue: PrivacyTransparency: Governance; Privacy"
Facebook political ads misidentification,"A joint study between researchers at New York University and KU Leuven in Belgium has found that Facebook missed 83 percent of political ads they ran on Facebook. 
The researchers ran 189,000 political ads between July 2020 and February 2021 through Facebook's then new authorisation process for political advertising and discovered that the company misidentified 21% of ads for products as being political and 62% of overtly political ads.
The researchers also identified over 70,000 political ads running on Facebook during a temporary ban on political advertising that Facebook had imposed during the US presidential election, many of them by organisations that had only run political ads on the platform.
Operator: Meta/FacebookDeveloper: Meta/FacebookCountry: Brazil; US; Global Sector: Politics Purpose: Authorise political advertising Technology: Political advertising authorisation process Issue: Accuracy/reliability Transparency: Governance; Black box"
Trelleborg welfare management automation,"Trelleborg municipality in south Sweden was one of the first authorities in the country to automate the assessment and distribution of welfare payments. 
Capitalising on a change in Swedish national law in 2018 that enabled the payment of welfare benefits without human intervention, Trelleborg municipality in south Sweden automated the assessment and distribution of welfare payments. 
The system was a fully automated system using Robotic Process Automation (RPA) to assess and distribute payments. The local government claimed this would help them increase efficiency and reallocate resources elsewhere.
The approach taken, now known as the 'Trelleborg Model', illustrates how a seemingly well-meaning and apparently effective programme can go astray.
The impact was swift, with the municipality reporting that the number of people receiving social benefits had substantially decreased. And the number of case workers was reduced from 11 to 3 as individual assessments could now be made more or less instantaneously, having taken up to two days. 
On the other hand, citizens complained of unfair assessments, little explanation of how the system worked, and limited ability to complain or appeal. 
Researchers, journalists and other experts found themselves unable to access the system's data, code or model to assess its effectiveness or fairness, and freedom of information requests were summarily blocked on the grounds of trade secrecy.
Operator: Trelleborg Municipality Developer: Trelleborg MunicipalityCountry: Sweden Sector: Govt - welfare Purpose: Optimise welfare payments Technology: Robotic Process Automation (RPA) Issue: Fairness; Employment - jobs; Privacy Transparency: Governance; Black box; Complaints/appeals; Legal"
Life360 location data sharing,"An investigation by The Markup finds family safety app Life360 is collecting and selling the data of its users to at least a dozen data brokers.
Used by approximately 33 million people worldwide, Life360 enables friends and family members to share their exact location. X-Mode, Cuebiq, Allstate's Arity, Safegraph and other data brokers package and sell Life360 data 'to virtually anyone who wants to buy it'.
Life360 reputedly made USD 16 million in 2020 selling user data, and recently announced it had acquired Tile, which makes several popular tracking devices. 
On January 27, 2022, Life360 stated in an investor report (pdf) that it would cease all of its location data deals, other than with Allstate’s Arity.
Operator: Life360; X-Mode; Cuebiq; Allstate/Arity; Safegraph Developer: Life360Country: Global Sector: Business/professional services Purpose: Track childrens' movements Technology: Location tracking Issue: Privacy; Security Transparency: Governance; Privacy"
Denny's robot server,"A video shot by a customer of a Denny's restaurant robot named 'Janet' serving breakfast has gone viral. The video has now been viewed hundreds of thousands of times on TikTok and beyond.
Comments range from positive to concerns about the impact on jobs, wages, tips, and the human dining experience. Some threaten to boycott the business.
Denny's has not commented on the video, or on its robotics programme. The company is understood to have been testing Bear Robotics' robot technology for two months. 
The stated purpose of many robotics and robotic process automation (RPA) programmes is to increase efficiency and capacity. Few companies state publicly the actual or potential indirect, secondary impacts of these programmes on employment and jobs.
Operator: Denny's Developer: Bear Robotics; SoftbankCountry: USA Sector: Food/food services Purpose: Serve foodTechnology: RoboticsIssue: Employment - jobsTransparency: Governance"
"Henan foreign journalist, student surveillance","Video surveillance analysis company IPVM reports that the government of Hunan province in China is setting up - or has already set up - a system to detect and track 'people of concern' including foreign journalists, students, and migrant women. 
The system uses facial recognition technology supplied by local company Neusoft, and is connected to over 3,000 CCTV cameras across the province and to a number of national and regional databases. 
According to the Hunan government's 200-page tender document, the surveillance system uses a traffic light system to categorise targets by those considered to be of 'key concern', 'general concern', and 'not harmful'.
It is unclear whether the system is already operational.
Operator: Henan Public Security Department Developer: Neusoft; HuaweiCountry: China Sector: Govt - police; Govt - security Purpose: Identify & track foreign journalists, students,'suspicious people' Technology: Facial recognition Issue: Surveillance; Privacy Transparency: Privacy"
Amazon DSP Ans Rana crash liability,"Property and technology company Zillow Group has shuttered (pdf) its i-Buying home flipping business, saying (pdf) it has lost USD 881 million on the business.
Zillow Offers uses big data and automated valuation algorithms to make offers on homes across the US. The aim is to sell acquistions quickly for a profit. 
However, the company appeared to lose faith in the ability of its algorithm to make reliable predictions, including during so-called 'black swan' events such as COVID-19.
In a letter to shareholders, CEO Rich Barton explained Zillow Offers was 'too risky, too volatile to our earnings and operations', and provided 'too low of a return on equity opportunity, and too narrow in its ability to serve our customers.'
Zillow says it will lay off 2,000+ employees. The company has also been hit with two class action lawsuits claiming it had misled investors about the true nature of its financial performance.
Operator: Zillow Group Developer: Zillow Group Country: USA Sector: Real estate Purpose: Estimate and predict real estate valueTechnology: Automated valuation model Issue: Accuracy/reliabilityTransparency: Black box; Marketing"
Zillow Offers iBuying algorithm,"Property and technology company Zillow Group has shuttered (pdf) its i-Buying home flipping business, saying (pdf) it has lost USD 881 million on the business.
Zillow Offers uses big data and automated valuation algorithms to make offers on homes across the US. The aim is to sell acquistions quickly for a profit. 
However, the company appeared to lose faith in the ability of its algorithm to make reliable predictions, including during so-called 'black swan' events such as COVID-19.
In a letter to shareholders, CEO Rich Barton explained Zillow Offers was 'too risky, too volatile to our earnings and operations', and provided 'too low of a return on equity opportunity, and too narrow in its ability to serve our customers.'
Zillow says it will lay off 2,000+ employees. The company has also been hit with two class action lawsuits claiming it had misled investors about the true nature of its financial performance.
Operator: Zillow Group Developer: Zillow Group Country: USA Sector: Real estate Purpose: Estimate and predict real estate valueTechnology: Automated valuation model Issue: Accuracy/reliabilityTransparency: Black box; Marketing"
Tesla Model Y FSD Beta crash,"The US National Highway Traffic Safety Administration (NHTSA) has reported the first crash of a Tesla using the company’s Full Self-Driving (FSD) beta software.
According to the report, the crash took place on November 3 in Brea, California, and involved a Model Y vehicle in FSD mode that crashed after mistakenly turning into the wrong lane. The Tesla was then struck, leading to the car being 'severely damaged' on the driver’s side. 
Nobody was injured in the crash, according to the NHTSA. 
Tesla’s FSD Beta testing programme has resulted in significant criticism with the company rolling out multiple updates as a result of reported bugs, a recall, and its decision to stifle untrained test drivers with NDAs. 
Operator:  Developer: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer visionIssue: Safety; Accuracy/reliabilityTransparency: Black box; Legal"
FACIL’iti automated accessibility,"French web accessibility firm FACIL’iti, through its law firm HAAS Avocates, has filed legal threats against web accessibility expert Julie Moynat and consultancy Koena, both of whom had been publicly critical of the company.
Moynat and Koena had questioned the effectiveness of FACIL’iti's automated accessibility overlay product, which promises instant, full accessibility and compliance with Web Content Accessibility Guidelines. They argued the company's marketing claims fail to match reality.
The legal threats take the form of 'poursuites bâillons' (or legal gagging orders, roughly equivalent to a US SLAPP suit), and are typically used to intimidate or silence critics or opponents with the prospect of full legal proceedings.
The move sparked an industry backlash against the company, and prompted French trade union Cinov Numerique to issue a 'Motion of support' for Koena.
Operator: Julie Moynat, Koena Developer: FACIL’itiCountry: France Sector: Business/professional services Purpose: Improve website accessibility  Technology: Web accessibility overlay Issue: Accuracy/reliabilityTransparency: Legal; Marketing"
AccessiBe automated accessibility,"Eyewear retailer Eyebobs has settled with a customer who had sued the company for failing to provide users of its website with screen readers of equal access to its services, and for misleading marketing.
The software was supplied by Israel-based web accessibility start-up AccessiBe, which claims to make websites for people with disabilities easier to use by providing an 'automated, state-of-the-art AI technology'.
Over 600 blind people, accessibility advocates and software developers had previously signed an open letter calling on website operators to stop using AccessiBe and similar companies.
Operator: Eyebobs, Masterbuilt Manufacturing Developer: AccessiBe Country: USA, Israel Sector: Business/professional services Purpose: Improve website accessibility Technology: Web accessibility overlay Issue: Accuracy/reliability Transparency: Legal; Marketing"
Hebron Palestinian facial recognition surveillance,"The use of facial recognition and other AI tools and techniques by Israel's defence forces to monitor Palestinians violates their basic rights, according to a new 82-page report by human rights organisation Amnesty.
Israel has been found to be using AI-based surveillance technologies including 'Red Wolf', a facial recognition system installed at checkpoints which automatically identifies Palestinians, and 'Blue Wolf', a smartphone-based facial recognition tool rolled out late 2020 that performs much the same function across Hebron and east Jerusalem. 
The two systems link to 'Wolf Pack', a database containing profiles of Palestinians in the West Bank and east Jerusalem, including photographs, family histories, education, and security ratings. Worlf Pack enables Israeli soldiers to identify and detain Palestinians before they have presented their ID cards, and allows Israel to 'consolidate existing practices of discriminatory policing, segregation, and curbing freedom of movement,' Amnesty says.
In November 2021, the Washington Post reported the existence of the 'Wolf Pack' and 'Blue Wolf' systems, suggesting they may form part of 'Hebron Smart City', a broader real-time system comprising closed-circuit TV, computer vision, and movement sensors that tracks Palestinians across the city and, according to a former IDF soldier, in their homes. 
Hebron Smart City also reputedly includes 'White Wolf', an app used by security officials in West Bank settlements to identify Palestinians before they enter 'illegal' settlements to work.
Operator: Israel Defense Forces (IDF) Developer:  Country: Israel; Palestine Sector: Govt - defence; Govt - security; Govt - policePurpose: Identify & track Palestinians Technology: Facial recognition Issue: Privacy; Surveillance Transparency: Governance; Privacy"
S Korea immigration facial recognition data sharing,"Documents released by South Korea's government reveal the country's Ministry of Justice has been sharing the facial images and personal details of travellers using Seoul's Incheon International Airport with local commercial companies.
The data involves approximately 170 million travellers, and was obtained without their consent, and is being shared to help develop a government system for screening and identifying travellers.
South Korea's Personal Information Protection Act determines that the sharing of sensitive personal data with third parties requires the specific and separate consent of the data subject.
Local civil liberty activists have threatened to take the government to court on the issue.
Operator: Ministry of Justice (MOJ) ; Ministry of Science and ICT (MSIT); Incheon International Airport Developer: CUbox  Country: South Korea Sector: Govt - immigration Purpose: Identify travellers; Predict security breaches Technology: Facial recognition Issue: Privacy; Dual/multi-use Transparency: Governance; Privacy"
"Tesla FSD Beta software glitch, recall","Tesla has been forced to issue a US recall (pdf) for a glitch in its Full Self-Driving system. The recall affects over 11,700 vehicles. 
Tesla uninstalled FSD 10.3 following driver reports of inadvertent activation of their cars' automatic emergency braking system, and says it is unaware of any crashes resulting from the glitches.
Tesla’s FSD Beta testing programme has attracted criticism for attempting to stifle untrained test drivers with non-disclosure agreements (NDAs). 
Operator:  Developer: Tesla Country: USA Sector: AutomotivePurpose: Automate steering, acceleration, brakingTechnology: Self-driving system Issue: Safety; Accuracy/reliabilityTransparency: Black box; Legal"
Wellington International Airport facial recognition,"According to Stuff, New Zealand's Wellington International Airport has been trialling a facial recognition system despite being warned not to by the country's Office of the Privacy Commissioner of New Zealand. 
Having started the trial in June, the airport is using the 'secretive' system to count how many passengers are passing through security, and how long they spend queuing.
The Privacy Commissioner argued the tool's impact on privacy outweighed its benefits, and could lead to use by other agencies, including law enforcement or intelligence agencies.
The Commissioner also criticised New Zealand's Aviation Security Service's decision not to issue a press release about the new technology, or to include information about it on the airport website.
Operator: Wellington International Airport Developer: Aviation Security (Avsec) Country: New Zealand Sector: Transport/logisticsPurpose: Assess security queues Technology: Facial recognition Issue: Privacy; Appropriateness/need; Dual/multi-use Transparency: Governance; Marketing"
Adobe Sensei Project Morpheus,"Adobe has unveiled Project Morpheus, a prototype video version of the Neural Filters machine learning-based image-editing tools that the company released as part of Photoshop 22.0. 
Like Neural Filters, Morpheus enables users to alter someone's age and facial expressions, whilst adding the ability to change facial hair and glasses.
The software raises concerns about the ease with which lifelike deepfake videos could be created to malign, undermine or otherwise damage the interests or reputation of an individual, group or society - a possibility Adobe confirmed to The Verge that it is aware of.
In Adobe's promotional video, project Morpheus software developer Han Gao states that Morpheus is an experiment, and may not end up as part of Photoshop or another Adobe product.
Operator: Adobe Developer: AdobeCountry: USA Sector: TechnologyPurpose: Manipulate video Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Dual/multi-useTransparency:"
SanTO robot Catholic priest,"Adobe has unveiled Project Morpheus, a prototype video version of the Neural Filters machine learning-based image-editing tools that the company released as part of Photoshop 22.0. 
Like Neural Filters, Morpheus enables users to alter someone's age and facial expressions, whilst adding the ability to change facial hair and glasses.
The software raises concerns about the ease with which lifelike deepfake videos could be created to malign, undermine or otherwise damage the interests or reputation of an individual, group or society - a possibility Adobe confirmed to The Verge that it is aware of.
In Adobe's promotional video, project Morpheus software developer Han Gao states that Morpheus is an experiment, and may not end up as part of Photoshop or another Adobe product.
Operator: Adobe Developer: AdobeCountry: USA Sector: TechnologyPurpose: Manipulate video Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Dual/multi-useTransparency:"
São Geraldo Magela drone delivery,"A priest at São Geraldo Magela church in Soracaba, Brazil, has used a drone fitted with a monstrance to deliver the Eucharist to  the altar. The congregation reputedly supported the novelty. 
However, once shared on Facebook, religious conservatives complained that the act was 'inappropriate, 'scandalous', and a 'profanation'. 
Priest John Zuhlsdorf labelled the stunt 'sacrilegious silliness', according to the Catholic Herald.
Operator: São Geraldo Magela church, Sorocaba Developer: Unknown Country: Brazil Sector: Religion Purpose: Deliver Eucharist Technology: DroneIssue: Ethics; Appropriateness/needTransparency:"
India citizenship law protest surveillance,"Reuters reports that police in New Delhi and Uttar Pradesh have been using facial recognition technologies to identify and detain protestors marching against India's controversial new citizenship law. The law is said to marginalise Muslims.
Indian activists are concerned about the privacy implications of facial recognition, and its potential use for suveillance and other purposes. 
Rajan Bhagat, a deputy commissioner of police at Delhi’s Crime Records Office claimed he is 'only catching targeted people' and that his force 'doesn’t have any protesters’ data, nor do we plan to store it.'
Uttar Pradesh police have also been criticised for using drones to surveil citizenship law protests.
Operator: Delhi Police; Uttar Pradesh PoliceDeveloper: Innefu Labs, Staqu Country: India Sector: Govt - policePurpose: Identify criminals, protestors Technology: Facial recognitionIssue: Privacy; Surveillance; Dual/multi-use Transparency: Governance; Privacy"
"Huq GPS location data sharing, sales","UK-based docation data merchant Huq received GPS coordinates even when mobile users have explicitly opted-out of the collection of this data on individual Android apps, according to app analysis company AppCensus and Vice's Motherboard.
Huq, which claims to collect and process over one billion mobility events every day, appeared unaware of the vulnerability. But the findings raised questions about the nature and effectiveness of it's customer/partner compliance checks, which the company said it runs monthly.
Civil rights and privacy advocates suggested the problem reflects inadequate advertising and marketing industry data sharing practices and governance.
Operator: Huq IndustriesDeveloper: Huq Industries, Kaibits Software, AppSourceHub
Country: UK
Sector: Technology
Purpose: Track user location
Technology: Location tracking algorithms
Issue: PrivacyTransparency: Governance; Privacy"
"Tamoco location data sharing, sales","Norwegian broadcaster NRK has revealed that UK location data broker Tamoco has been selling large volumes of personally identifiable data. NRK had bought raw location data for 140,000 mobile devices for GBP 3,000, enabling the broadcaster to identify individuals and track military personnel.
Norwegian data protection authority Datatilsynet subsequently condemned the sale of Norwegians' location data, and alerted other EU data protection authorities, including the UK Information Commissioner's Office ('ICO').
The ICO subsequently announced its own investigation into Tamoco. According to the BBC, the ICO reprimanded Tamoco in October 2021 for 'failing to provide sufficient privacy information to UK citizens'.
Operator: Tamoco Developer: Tamoco Country: Norway, UKSector: TechnologyPurpose: Assess & enhance location data Technology: Location tracking Issue: Privacy Transparency: Governance; Privacy"
Meituan location tracking,"Chinese e-commerce company Meituan is under fire for 'intensive' location tracking of its users. 
A local gadget review blogger discovered that the app had been tracking his location even when he was not using it, and had taken to social media to complain. His post quickly went viral.
Meituan has also been dragged into the spotlight by Wang Sicong, online influencer and son of Dalian Wanda Group’s founder. Wang claims he was locked out of his account on Meituan's Dazhong Dianping restaurants and review service, when it was linked to another user’s mobile phone number.
Meituan was recently fined $534 million for antitrust violations in China.
Operator: Meituan Dazhong Developer: Meituan Dazhong Country: ChinaSector: Technology Purpose: Track user location Technology: Location trackingIssue: Privacy; Security  Transparency: Governance; Privacy"
Cruise driverless cars traffic blocking,"In June 2022, nearly twenty Cruise driverless cars stood still and blocked traffic for two hours on a street in central San Francisco, USA, causing tailbacks, making the area impassable and inhibiting night street-sweeping. 
The cars were reputedly disabled after they had lost touch with a Cruise server. The incident was resolved when the cars were manually moved out of the way by Cruise employees.
Per WIRED, a Cruise employee later sent an anonymous letter to the California Public Utilities Commission, claiming that Cruise loses communication with the automated vehicles 'with regularity', blocking traffic and potentially hindering emergency vehicles.
In April 2022, Twitter users posted videos of a Cruise vehicle moving away from San Francisco police after they had tried to find put whether the car had a driver.  
Operator: GM CruiseDeveloper: GM Cruise; General Motors/Chevrolet Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer visionIssue: Accuracy/reliability; Safety Transparency: Black box"
Facebook Meaningful Social Interactions algorithm,"In June 2022, nearly twenty Cruise driverless cars stood still and blocked traffic for two hours on a street in central San Francisco, USA, causing tailbacks, making the area impassable and inhibiting night street-sweeping. 
The cars were reputedly disabled after they had lost touch with a Cruise server. The incident was resolved when the cars were manually moved out of the way by Cruise employees.
Per WIRED, a Cruise employee later sent an anonymous letter to the California Public Utilities Commission, claiming that Cruise loses communication with the automated vehicles 'with regularity', blocking traffic and potentially hindering emergency vehicles.
In April 2022, Twitter users posted videos of a Cruise vehicle moving away from San Francisco police after they had tried to find put whether the car had a driver.  
Operator: GM CruiseDeveloper: GM Cruise; General Motors/Chevrolet Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer visionIssue: Accuracy/reliability; Safety Transparency: Black box"
Twitter right-wing content amplification,"A new study by Twitter has discovered that its algorithms disproportionately boost right-wing news and commentary content. 
Covering politicians’ tweets and political content from Canada, France, Germany, Japan, Spain, the UK and US from April 1 to August 2020, Twitter found that right-wing content was more amplified algorithmically relative to a reverse-chronological timeline in every country, other than Germany.
Twitter director of software engineering Rumman Chowdury and machine learning researcher Luca Belli admit it is unclear why its algorithms behave this way. 
'Further root cause analysis', they say, 'is required in order to determine what, if any, changes are required to reduce adverse impacts by [Twitter's] Home timeline algorithm'. 
Operator: Twitter Developer: TwitterCountry: Canada; France; Germany; Japan; Spain; UK; USA Sector: Politics Purpose: Recommend content Technology: Recommendation algorithm Issue: Bias/discrimination - politicsTransparency: Governance; Black box; Complaints/appeals"
The FaceTag,"Harvard student Yuen Ler Chow has created The Facetag, a networking app that enables users to scan fellow students' faces using facial recognition and to exchange contact details.
Despite providing a privacy feature whereby a user must give permission before someone else accesses their profile, and storing userdata on Google Cloud, the app has raised privacy, ethics, and security concerns and led to a backlash amongst Harvard students, social media users, digital rights advocates and commentators.
'It's kinda weird how I see so many people scared over the fact that I'm collecting this data, but almost all the other social media apps collect way, way more' he argues.
Chow claims 'FaceTag is the next Facebook.' 'I’m Zuck, but better' he claims, saying he aims to expand FaceTag to other colleges and communities.
Operator: Yuen Ler Chow Developer: Yuen Ler Chow  Country: USASector: Technology Purpose: Scan human facesTechnology: Facial recognition Issue: Privacy; Security Transparency: Governance; Marketing"
Gorillas 'Project Ace' rider work schedule automation,"Attempts by German grocery delivery service Gorillas to intensify work schedules and get delivery riders to fulfill more orders is contributing to worker discontent, strikes, and firings. 
Gorillas 'Project Ace' uses an algorithm to calculate the times in which most workers are needed, demands shorter, more irregular shifts, and ignores the eleven hour rest time mandated under law.
Gorillas describes itself as a 'counter-model to the gig economy'. Unlike some of its competitors, it employs its riders and warehouse staff. But wages are low, salaries can be paid late, safety is poor, and the probationary period is a full six months - the maximum allowed under German law. 
Furthermore, Gorillas workers have limited access to management, and 'Rider Support', which is supposed to take care of employee concerns, has no telephone number and, according to employees, leaves emails unanswered for days.
Founded in Germany in 2020, Gorillas is been valued at over EUR 1 billion and operates in cities across France, Italy, the Netherlands and the United Kingdom. 
Over 350 employees are said to have been fired for striking. The company has been subject to multiple employee strikes and blockades in its home market.
Operator: Gorillas Developer: Gorillas Country: GermanySector: Transport/logistics Purpose: Automate work schedulingTechnology: Scheduling algorithmIssue: Accuracy/reliability; Fairness Transparency: Governance; Complaints/appeals"
Delphi moral judgements,"The Allen Institute for AI's Ask Delphi is a research prototype designed to predict how humans would evaluate a wide range of ethical situations. The model is trained on questions published on Reddit communities r/AmITheAsshole and r/Confessions and responses from participants on crowdsourcing platform Mechanical Turk.
Reaction to Delphi has been mixed. On the one hand, Delphi is seen to provide reasonable, basic answers to simple questions. Equally, it is seen to misunderstand the context in which questions are being asked, produce nonsensical and clearly unsafe responses, and exhibit overt racial, gender, religious, and cultural stereotyping and bias.
Operator: Allen Institute for AI Developer: Allen Institute for AI Country: USASector: NGO/non-profit/social enterprise Purpose: Answer ethical dilemmas Technology: Chatbot; NLP/text analysis Issue: Accuracy/reliability; Bias/discrimination - race, religion, genderTransparency: Black box"
BOSCO electricity subsidy assessment,
Amazon India own brand search engine rigging,"Reuters has obtained thousands of pages of internal Amazon documents that appear to show the company in India has been running a 'systematic' programme of copying other companies' products and promoting them by manipulating its own search results. 
The documents indicate Amazon India employees have been using 'search seeding', 'search sparkles' and other techniques so that the company’s own label product lines, such as AmazonBasics and Solimo, would appear 'in the first 2 or three … search results' on Amazon.in.
A parallel investigation by The Markup indicates that Amazon has been regularly placing its own brand products above those of competitor products in the US.
In March 2022, the US House of Representatives’ antitrust subcommittee accused (pdf) Amazon of lying to Congress about how it treats third-party sellers, including whether it preferences its own brands and exclusive products in search results.
It also asked (pdf) the Department of Justice to investigate Amazon for 'potentially criminal' obstruction of Congress.
Operator: Amazon Developer: Amazon
Country: India
Sector: Retail
Purpose: Rank content/search results
Technology: Search engine algorithm
Issue: Ethics; Competition/price fixing; IP abuse/misuse Transparency: Governance; Complaints/appeals; Black box; Marketing"
Dubai USD 35m AI voice cloning heist,"According to a new Forbes report, Dubai investigators have discovered an elaborate scam in which deepfake technology was used to clone the voice of a company director and defraud his company of USD 35 million.
About to make an acquisition, the company needed its bank to authorise a series of transfers. A bank employee was fooled by the deepfaked voice into authorising the transfer of the cash, believing it was a legitimate business transaction. 
UAE authorities reckon the scheme involved at least 17 people. It is the second known case of fraudsters using deep voice tools to carry out a heist, and much the largest. 
Operator: Anonymous/pseudonymous Developer: Anonymous/pseudonymous Country: UAE/DubaiSector: Banking/financial services Purpose: Defraud Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Ethics; Security Transparency: Governance"
NHGSFP school meal fingerprint biometrics,"Nigeria's federal government has started to collect the fingerprints of 642,000 primary school pupils as part of the roll-out of its national National Home Grown School Feeding Programme (NHGSFP). 
NHGSFP aims to provide school meals to students and accelerate school enrollment and retention, while boosting local food production and economies. 
As noted by BiometricUpdate, it is unclear why fingerprinting is strictly necessary, especially for children under 5 years old, other than to verify pupils' identities. 
Questions are also being asked why the collection of biometric data started in Borno State, which has been plagued by Boko Haram violence.
Operator: Ministry of Humanitarian Affairs; Disaster Management and Social Development Developer: HID Global; Plovtech Country: Nigeria Sector: Education Purpose: Verify identity Technology: Fingerprint biometricsIssue: Privacy; Security; Dual/multi-use Transparency: Governance"
Gdansk Primary School No. 2 meal payment verification,"Gdansk Primary School No. 2, in northern Poland, has been fined EUR 4,600 by UODO, the country's data privacy regulator, for processing students' fingerprint data to verify their school meal payments. 
The school had been using fingerprints to verify whether pupils had paid for their meals since 2015, with system being used on 680 children in the current academic year, while four children were processed using 'an alternative identification system'.
According to UODO, the system was unfair, and 'significantly disproportionate' to the task. 
In October 2021, several schools in North Ayrshire, Scotland, attracted the ire of the UK data privacy regulator for using facial recognition to verify school meal payments.
Operator: Gdansk Primary School No. 2 Developer: Unknown Country: PolandSector: EducationPurpose: Verify meal payments Technology: Fingerprint biometrics Issue: Privacy; Appropriateness/needTransparency: Governance"
Masayuki Nakamoto deepfake uncensored porn,"Japanese police have arrested a man who admitted to sharing and selling pornography that had been deepfaked to clarify censored, pixellated genitalia. 
According to The Mainichi, Masayuki Nakamoto sold over 2,500 doctored video files for 11 million yen (roughly $96,000) using TecoGAN, a deep learning video clarification tool, to unblur genitalia.
Nakamoto was arrested for breaking Japan's copyright law and obscenity law, which bans the display of 'indecent materials'.
Operator: Masayuki Nakamoto Developer: Masayuki Nakamoto Country: JapanSector: Media/entertainment/sports/artsPurpose: Drive sales Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Privacy; Ethics; Copyright Transparency: Privacy"
Xiao Yu deepfake pornography,"Chu Yu-chen (朱玉宸), 26, aka 'Xiaoyu', and his assistant, Chuang Hsin-jui (莊炘睿), have been arrested for creating, selling and distributing over 100 porn videos of Taiwanese celebrities and politicians. 
According to Taiwan's Criminal Investigation Bureau, the three made over NT$13 million in a past year by superimposing peoples' faces without their consent on to existing pornographic videos using deepfake technology.
In addition to selling deepfakes of public figures on a series of Telegram groups, Chu is said to have accepted commissions to make deepfake videos of lesser-known individuals as a form of 'revenge porn.' 
In July 2022, Chu and Chuang were sentenced to five years and six months and three years and eight months in prison respectively for violations of Taiwan's Personal Data Protection Act. The sentences were commutable to fines. 
Another 21 victims filed civil lawsuits seeking financial compensation from Chu and Chuang for misuse of their likenesses. 
Politician Kao Chia-yu (高嘉瑜), also a victim, subsequently proposed to legislate deepfakes in Taiwan's national legislature via a bill that would make the production and spread of fake or manipulated images and video for profit a crime punishable by up to seven years in prison. 
The amendments to the Criminal Code include an additional article dedicated to a new form of crime using artificial intelligence — deepfakes — which involve inserting the likeness of a person into an existing image or video.
The bill passed in January 2023.
Operator: Chu Yu-chen ('Xiaoyu') Developer: Chu Yu-chen ('Xiaoyu')
Country: Taiwan
Sector: Media/entertainment/sports/arts
Purpose: Create entertainment
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning 
Issue: Privacy; EthicsTransparency: Privacy"
Moscow Metro Face Pay facial recognition,"Moscow's metro system has introduced Face Pay, a facial recognition-based cashless, cardless and phoneless system spread across 240 stations. Passengers will only need to look at a camera to travel.
Moscow mayor Sergey Sobyanin welcomed the initiative by boasting the city 'is the first in the world to introduce Face Pay on such a scale. The technology is new and very complex, we will continue to work on improving it.'
However, activists are concerned about possible privacy abuse and the system's potential for surveillance and social control. Face Pay is 'a good pretext to put cameras at the turnstiles' an activist group from Moscow told the New York Times. 
In March 2022, the BBC reported that protestors against Russia's invasion of Ukraine were likely being identified by the Moscow Metro's facial recognition system.
Shortly afterwards, Moscow’s municipal IT department announced it is looking to build a centralised data storage facility for facial recognition and other data collected from the city and across Russia.
Face Pay was developed by Russian software firms NtechLab, VisionLabs and Tevian in conjunction with Russian bank VTB  Group. Belarusian developer Synesis is also known to supply Moscow authorities' with surveillance products.
In 2019, Moscow City Police were found to have been selling citizens' facial data and access to live streams of the city's CCTV facial recognition surveillance network, according to an investigation by MBKh Media. 
Operator: Moscow Metro Developer: Moscow Metro, VTB Group, VisionLabs, NtechLab, Tevian 
Country: Russia
Sector: Govt - transport
Purpose: Drive sales
Technology: Facial recognition
Issue: Privacy; Dual/multi-use; SurveillanceTransparency: Governance; Privacy"
North Ayrshire school meal payment verification,"In October 2021, nine schools in North Ayrshire, Scotland, started taking payments for school lunches by using facial recognition to scan the faces of pupils at the cash register in order to deduct money from an online account, according to the Financial Times.
CBD Cunninghams, the company that developed the system, claimed it speeded up queues and protected students better against COVID-19 than the card payments and fingerprint scanners the schools used previously.
Privacy advocates responded by saying the new system is not needed, operated without explicit consent, and amounted to the de facto normalisation of facial recognition.
The North Ayshire Council (NAC) programme was suspended after the UK Information Commissioner’s Office (ICO) responded to the controversy by encouraging schools to take a 'less intrusive' approach where possible. 
In February 2023, the ICO informed (pdf) NAC that it 'is likely to have infringed data protection law'. The ICO has published a case study on the incident.
Operator: North Ayrshire Council (NAC) Developer: CRB Cunninghams Country: UK Sector: Education Purpose: Verify meal payments Technology: Facial recognition Issue: Appropriateness/need; PrivacyTransparency: Privacy"
Amazon Ring video doorbell 'invades' neighbour privacy,"A judge has ruled that Amazon-manufactured CCTV cameras and a Ring doorbell installed by Jon Woodard, a houseowner in Oxfordshire, UK, 'unjustifiably invaded' the privacy and contributed to the harassment of his neighbour Dr Mary Fairhurst.
Dr Fairhurst had claimed that the devices installed on Mr Woodard's house and garden shed captured images of her house, garden and parking space, and recorded personal conversations. 
Amazon customers were unable to switch off the audio recording facility on Ring doorbells until 2020, with the company requested that customers 'respect their neighbours' privacy, and comply with any applicable laws when using their Ring device.'
Operator: Amazon Developer: AmazonCountry: UK Sector: Consumer goods Purpose: Strengthen security Technology: CCTV; Computer vision Issue: Privacy; SurveillanceTransparency: Privacy"
Amazon US own brand search engine rigging,"Amazon has been regularly placing its own brand products above those of competitor products, according to an investigation by The Markup. Based on data provided by 3,400 product searchers in January 2021, the report finds that Amazon brands and exclusives receive an 'outsized portion of the top spot on search results' while only accounting for a small proportion of products on the company's platform.
The Markup also conducted a survey showing that most US consumers are unable to identify Solimo, Pinzon, and other top-selling house brands, many of which are not clearly labelled as owned by Amazon. Over 150 Amazon house brands and 137,000 unique house brand and exclusive products were identified during the investigation.
The day before The Markup's investigation was published, Reuters revealed Amazon documents that show Amazon has been copying products on its marketplace in India and covertly rigging its search engine algorithm to boost them. 
This is not the first time Amazon has been accused of copying products and manipulating its own search results. The US House Judiciary Committee antitrust subcommittee investigated (pdf) competitive practices at Amazon, Alphabet, Apple, and Facebook in June 2019. During the hearings, Amazon associate general counsel Nate Sutton argued that Amazon algorithms 'are optimised to predict what customers want to buy regardless of the seller.'
In March 2022, the US House of Representatives’ antitrust subcommittee accused (pdf) Amazon of lying to Congress about how it treats third-party sellers, including whether it preferences its own brands and exclusive products in search results. It also asked (pdf) the Department of Justice to investigate Amazon for 'potentially criminal' obstruction of Congress.
Early April 2022, the WSJ reported that the US SEC is investigating how Amazon discloses its business practices, including its use of third-party-seller data for its private-label business.
Operator: AmazonDeveloper: AmazonCountry: USA Sector: RetailPurpose: Rank content/search results Technology: Search engine algorithmIssue: Ethics; Competition/price fixing; IP abuse/misuse Transparency: Governance; Complaints/appeals; Black box; Marketing"
MTV Lebanon uses deepfakes to commemorate bomb victims,"A deepfake video purporting to 'commemorate' two victims of the 2020 Beirut port bombing that killed over 200 people and injured some 6,500 injured, has been blasted for being 'inappropriate', 'insensitive' and 'unethical'.
Featuring deepfakes of Ralph Mallahi and Amin Al-Zahed, both of whom lost their lives in the 2020 atrocity, the video was titled 'A Letter to the Lebanese Judiciary' and shared on social media by MTV Lebanon with the hashtag, 'It’s been a year, the time is up.' 
Mouin Jaber, co-host of the popular Lebanese podcast “Sarde After Dinner,” told Arab News: 'It’s dystopian. It’s emotional manipulation and blackmail taken to a whole other uncanny level.'
Operator: MTV LebanonDeveloper: MTV Lebanon
Country: Lebanon
Sector: Media/entertainment/sports/arts
Purpose: Commemorate bomb victims
Technology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Appropriateness/need; Ethics
Transparency:"
Waymo cars get stuck in cul-de-sac,"Waymo self-driving cars are plaguing people living in a quiet cul-de-sac in San Francisco, prompting questions about the nature of the company's automated system and concerns about the effect on the environment.
According to local news station KPIX, Residents complain that up to 50 cars a day make their way down the street, only to have to turn around, resulting in more traffic and noise pollution. 
The cause of the issue was unclear. Local residents told KPIX that they asked the drivers but were told that the cars were 'programmed' and that they were just doing their jobs. Noboby reported seeing one of the cars dropping off or picking up a passenger. 
Waymo responded by saying the vehicles were simply 'obeying road rules' designed to limit traffic in certain residential streets.
Operator: Alphabet/Waymo Developer: Alphabet/WaymoCountry: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Waymo Driver Issue: Accuracy/reliability; EnvironmentTransparency: Black box"
Gaggle student behavioural monitoring,"An investigation by non-profit news site The 74 reveals that student safety management company Gaggle is closely and routinely monitoring almost every aspect of US students' lives, even after school hours and over weekends and holidays. 
Gaggle uses a combination of AI and human reviewers to police schools for suspicious and harmful content in order to prevent violence and student suicides by monitoring student email accounts, documents, and social media accounts.
The nature and extent of Gaggle's monitoring has resulted in student complaints, and has prompted mental health and privacy advocates to raise their concerns publicly. 
The effectiveness of Gaggle's suite of monitoring services are also being questioned. Reports suggest Gaggle's systems have not been independently reviewed or audited.
Operator: Minneapolis High Schools; Williamson County School District; Multiple Developer: GaggleCountry: USA Sector: EducationPurpose: Monitor student behaviourTechnology: NLP/text analysis; Computer vision Issue: Surveillance; Privacy; Accuracy/reliability; Appropriateness/need; Effectiveness/valueTransparency: Governance; Black box"
Vision 60/SPUR quadrupedal war robot,"A robot dog armed with a sniper rifle capable of hitting targets from 3,940 feet away has been unveiled at the US Army trade show.
Manufactured by Ghost Robotics and arms manufacturer SWORD International, the 'Special Purpose Unmanned Rifle' (SPUR) can be remotely instructed to load, unload and fire.
SPUR is 'the future of unmanned weapons system — and that future is now', according to SWORD International.
Mohsen Fakhrizadeh, the head of Iran's nuclear programme, was assassinated outside Tehran using a remote-controlled machine gun operated by Israeli secret services. 
Operator: Ghost Robotics Developer: Ghost Robotics, Sword International Country: USASector: Aerospace/defence Purpose: Kill/main adversariesTechnology: Robotics Issue: EthicsTransparency:"
LAION-400M image-text pairing dataset,"LAION-400M is a large, openly available dataset of 400 million image and text pairings developed by German non-profit collective LAION. The dataset's successor LAION-5B comprises 5 billion pairings. 
Launched in 2020, LAION-400M was used to train Imagen, Lensa, Stable Diffusion, and other text-to-image models.
A 2021 University College Dublin, University of Edinburgh and UnifyID audit (pdf) of LAION-400M found that it contained 'troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content.'
Politico noted: 'When the researchers typed the word “Korean,” LAION-400M didn’t bring up images of BTS or bulgogi, but naked Korean women. Searching the word “Indian” brought up pictures of South Asian women being raped. 'Best president' brought up images of Donald Trump.'
The researchers argue that LAION-400M and other large language models are rarely managed in the interests of the individuals and organisation whose data is being collected and used. For instance, these systems often contain personal data collected without consent, and sometimes make it purposely difficult for individuals to remove their data.
Furthermore, given content collected for large language models is often second or third hand and are are unlikely to have been properly cleaned, the researchers recommend dataset creators need to be much more careful about licensing.
'The rights of the data subject remain unaddressed here' the researchers argue (pdf). 'It is reckless and dangerous to underplay the harms inherent in such large scale datasets and encourage their use in industrial and commercial settings. The responsibility of the licence scheme under which the dataset is provided falls solely on the dataset creator.'
Operator: Alphabet/Google; Prisma Labs; Stability AI Developer: LAION Country: Germany Sector: MultiplePurpose: Pair text and images Technology: Database/dataset; Neural network; Deep learning; Machine learningIssue: Ethics; Safety; Bias/discrimination - race, ethnicity; Privacy; CopyrightTransparency: Governance
ASSESS DATABASE
LAION website
LAION Wikipedia profile
LAION-400M dataset
LAION-400M research study
Birhane A., Prabhu V.U., Kahembwe E. (2021). Multimodal datasets: misogyny, pornography, and malignant stereotypes
https://www.unite.ai/are-under-curated-hyperscale-ai-datasets-worse-than-the-internet-itself/
https://www.politico.eu/newsletter/ai-decoded/politico-ai-decoded-us-bill-of-ai-rights-parliament-gets-its-act-together-sort-of-the-dark-side-of-large-ai-models/
https://info.deeplearning.ai/the-batch-ai-has-a-web-problem-google-goes-multimodal-unfinished-symphony-completed-transformers-get-faster-still-1
https://www.reddit.com/r/MachineLearning/comments/pmwvw9/p_laion400m_opensource_dataset_of_400_million/
DALL-E image generator
Stable Diffusion image generator
Page infoType: DataPublished: October 2021Last updated: August 2023"
Chinese government facial recognition system hacked by tax fraudsters,"Two men in China tricked the country's State Taxation Administration's facial recognition identity verification system into creating fake invoices valued at USD 76.2 million.
According to a Xinhua report, the pair manipulated high-definition photographs with an app available on the black market that turns photos into videos and then used a smartphone to bypass its camera during facial authentication, instead feeding their doctored videos into the system. 
The men set up a shell company that issued fake tax invoices that defrauded the taxation department by 500 million yuan (approximately USD 76.2 million). 
The fraudsters started their operation in 2018, and have since been captured and prosecuted in Shanghai.  
Operator: State Taxation Administration Developer: State Taxation Administration Country: China Sector: Govt - financePurpose: Verify identity Technology: Facial recognition; Deepfake - image; Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: SecurityTransparency:"
Facebook Marketplace Amazon rainforest sales,"The BBC has discovered that swathes of Brazil's Amazon rainforest, including national forests and land reserved for indigenous peoples, are being illegally bought and sold on Facebook Marketplace commerce site.
Although the revelations were provoked an inquiry by Brazil’s Supreme Court, Facebook said it would allow the sales to continue. 
In October 2021, Facebook announced that it was updating its commerce policies to prohibit the sale of land in ecological conservation areas across Facebook, Instagram, and WhatsApp.
Operator: Meta/FacebookDeveloper: Meta/Facebook Country: BrazilSector: RetailPurpose: Sell products/services, advertisingTechnology: DatabaseIssue: Ethics; LegalityTransparency: Governance"
Google Health diabetic retinopathy diagnosis,"A Google AI system capable of identifying diabetic retinopathy with 90 percent accuracy in the testing laboratory is significantly less effective in real-life. 
Developed by Google Health and tested at 11 clinics and hospitals in Thailand between November 2018 and August 2019, the system was said to perform at the equivalent level of a medical 'specialist'. 
However, Google discovered that over 20 percent of photographs taken of patients was of too low quality to be processed due to inadequate lighting or the unreliable photographic ability of the local clinic workers. 
Some nurses also discouraged patients to participate in the study because due to concerns about costs and other issues.
Operator: Ministry of Public Health, Google Health Developer: Alphabet/GoogleCountry: ThailandSector: HealthPurpose: Identify diabetic retinopathyTechnology: Deep learning Issue: Accuracy/reliabilityTransparency:"
NarxCare drug addition risk assessment,"WIRED reports that NarxCare, a software system that uses patient data, drug use data, and metadata to determine the risk of drug addiction is denying users opioids on the basis it thinks they are at risk of addiction. 
Patients complain that the system, developed by Appriss, can be inaccurate and unfair. It is believed to reinforce existing racial and gender biases. 
Appriss has failed to disclose how the NarxCare system works, and does not provide access to its data, model or code. The company has also made a number of contradictory claims about the algorithm's data sources and other inputs.
Operator: US Department of Justice; Rite Aid; Walmart; Sam's Club Developer: Appriss Country: USASector: Healthcare/pharma Purpose: Assess & predict drug abuseTechnology: Risk assessment algorithmIssue: Fairness; Accuracy/reliability Transparency: Governance; Black box; Marketing"
Google misidentifies engineer as serial killer,"Hristo Georgiev, an engineer based in Switzerland, discovered that a Google search of his name returns a photograph of him linked to a Wikipedia entry on a serial killer.
Georgiev believes the error was caused by Google‘s knowledge graph, which generates informational boxes adjacent to its search results, and reckons the algorithm matched his picture to the Wikipedia entry because the now-dead killer shared his name.
Google swiftly removed Georgiev's image from the killer's infobox after the engineer had reported the issue.
Operator: Alphabet/Google  Developer: Alphabet/Google Country: Switzerland Sector: TechnologyPurpose: Enhance search engine results Technology: Knowledge Graph Issue: Accuracy/reliability; PrivacyTransparency: Governance; Black box"
Uber Real-Time ID Check,"Uber's 'Real-Time ID Check' system enables the ridehail company to check that a driver’s face matches what the company has on file in order to minimise fraud, increase security, and manage its workforce.
Developed by Microsoft, Real-Time ID Check was rolled out by Uber in the US in 2016, India in 2017, the UK in 2020, and other markets. 
Real-Time ID Check has been praised as a quick, effective means of verifying driver identity. However, it has also suffered criticism for misidentifications which can result in loss of pay and jobs, discrimination, and abuse of privacy.
August 2018: CNBC revealed transgender drivers in the US were being kicked off the Uber app as it seemingly struggled with selfies of people gender transitioning.
March 2021: A WIRED investigation revealed BAME couriers working for Uber Eats in the UK had been threatened with losing their jobs, had accounts frozen, or were permanently fired due to 'racist' software that is incapable of recognising their faces. Workers said they had little or no ability to appeal, and that those that were able to make their case were summarily rejected.
May 2021: False mismatches by Uber Eats' 'racist' facial identification system resulted in the wrongful dismissal of delivery driver Pa Edrissa Manjang and Imran Javaid Raja, a former Uber private hire driver, according to a lawsuit filed by The App Drivers and Couriers Union (ADCU) in the UK.
December 2022: The Technology Review reported that many Indian drivers were being locked out of their accounts due to factors such as facial hair, shaved heads, changed haircuts, and scratches on their device cameras, costing them work and, in some instances, their jobs.
December 2022: A judge ruled that Uber drivers were permitted to proceed in a class action against Microsoft on the basis that it had failed to obtain proper consent to capture and store driver faceprints under the Illinois Biometric Information Privacy Act (BIPA).
In March 2021, a group of Uber drivers from the UK and Portugal won a lawsuit in Amsterdam against Uber and competitor Ola in which the two companies were forced to reveal more about how their driver surveillance systems work, in addition to how their systems assign work, deduct earnings, and suspend drivers.
Operator: Uber/Uber Eats Developer: Microsoft
Country: India; UK; USA
Sector: Transport/logistics
Purpose: Verify identity
Technology: Facial recognitionIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Employment - pay, jobs
Transparency: Governance; Black box; Complaints/appeals"
Met Police retrospective facial recognition,"WIRED has discovered that London's Metropolitan Police Service is buying a new, retrospective facial recognition system that will enable them to process historic CCTV, social media, and other images when identifying and tracking down suspects. 
Critics are concerned the system, which extends the Met's existing facial recognition capabiilities, could easily be used for other purposes, and may entrench racially and otherwise discriminatory policing. 
The Met's contract was discovered when the Mayor of London's office published an approved proposal for the system, which forms part of a £3 million, four-year deal with NEC Corporation's UK subsidiary Northgate Public Services.
In an interview with The Register, the UK government’s Surveillance Camera Commissioner Professor Fraser Sampson argued 'We need as a minimum a single set of clear principles by which those using the biometric and surveillance camera systems will be held to account, transparently and auditably.'
Operator: Metropolitan Police Service Developer: Northgate Public Services (NEC)Country: UK Sector: Govt - policePurpose: Identify and track criminal suspectsTechnology: Facial recognition Issue: Privacy; Surveillance; Dual/multi-use; Bias/discrimination - race, ethnicityTransparency: Governance; Black box"
Amazon Mentor delivery driver scoring,"A report by CNBC finds Amazon drivers are being monitored by Mentor, an app that promises to improve driver safety by  generating a 'FICO' score each day that measures their driving performance. 
However, drivers say the app is often inaccurate, can lead to unfair disciplinary action, including loss of bonuses and perks. Mentor can also be invasive, tracking drivers' location after they clock out from work. 
Amazon also recently started using AI cameras in delivery vans to monitor driver behaviour and flag safety issues.
Operator: Amazon Developer: Solera/eDriving Country: USA Sector: Transport/logistics Purpose: Assess delivery driver performanceTechnology: Performance scoring algorithm Issue: Accuracy/reliability; Fairness; Privacy; Surveillance Transparency: Governance; Black box"
Amazon Flex delivery drivers forced to take unsafe routes,"Amazon delivery drivers are being forced to take dangerous routes and run across four-lane highways at night with multiple packages and boxes Vice News reports. 
Per Vice, Amazon's routing algorithms sometimes group deliveries on both sides of a street into a single stop, forcing to risk their personal safety in order to satisfy the demands of the company's cost and speed-focused Flex routing algorithm.
Other drivers bring another person with them to park and look after their vehicles when they make deliveries. 
Approximately 85,000 contracted delivery drivers across the US and Europe purportedly use Amazon's Flex algorithm. 
Operator: Amazon Developer: Amazon Country: USA; EU; UK; Australia Sector: Transport/logistics Purpose: Manage package delivery Technology: Routing algorithmIssue: Safety; Fairness Transparency: Governance; Black box"
"Apple depression, autism, dementia detection","The Wall Street Journal reports that Apple, UCLA and Biogen are conducting a study that uses facial recognition, speech patterns and other behavioural data to detect stress, depression and cognitive decline.
First announced in August 2020, the study was initially limited to health data such as heart rate and sleep, and how a person interacts with their iPhone, Apple Watch or Beddit sleep-tracker to understand their mental health. 
However, the WSJ's report says the study has been extended to monitor people’s vital signs, movements, speech, sleep, typing habits and frequency of typos, raising concerns from digital rights advocates about the validity of emotion AI/affective computing, privacy, and scope creep.
Apple has recently sought to position itself as a privacy leader, including insisting that apps in the iOS App Store add privacy 'nutrition labels' to inform users what type of sensitive information the app collects. 
Operator:  Developer: Apple; Biogen; UCLA Country: USA Sector: Health Purpose: Detect anxiety, depression, autism, dementia Technology: Emotion recognition; Facial recognition Issue: Accuracy/reliability; Privacy; Scope creep/normalisationTransparency:"
Amazon Ring Always Home Cam,"Amazon has launched Ring Always Home Cam, an autonomous drone equipped with a security camera that can move around customers' homes on pre-programmed routes streaming video.
Despite Amazon's claim that the Always Home Cam has been designed as a privacy-first product, commentators and digital rights advocates are concerned that the drone will inevitably erode privacy, especially when updates and upgrades are made, and jeopardise security.
Initially announced in September 2020, Amazon launched Ring Always Home Cam, Astro home robot and a suite of other products at its 2021 Echo event.
Operator: Amazon Developer: Amazon Country: USA Sector: Consumer goods Purpose: Strengthen home security Technology: Drone; Computer vision Issue: Privacy; Security; Surveillance; Dual/multi-use Transparency: Governance; Privacy"
XPeng P7 crashes into truck,"A driver of an Xpeng P7 is reported to have collided with a truck whilst using the car's Navigation Guided Pilot (NGP) automatic navigation assisted driving system. The accident resulted in minor injuries to the driver, who was hospitalised. 
The cause of the collision remains unclear; however, the system appears not to have recognised the truck, which was a flatbed vehicle with an extended tail. It was not carrying cargo. 
Xpeng says it will investigate the crash, and that its initial analysis showed the car’s assistant driving features were 'functioning normally' before the collision.
Xpeng reputedly failed to contact the owner after the accident, and then directed him to contact the police. The company said it would improve its 'seemingly inhumane' customer service.
Operator: Xpeng Developer: Xpeng Country: China Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Safety; Accuracy/reliability Transparency: Governance; Black box; Complaints/appeals"
Sarah Wysocki Washington DC teacher effectiveness assessment,"Sarah Wysocki, a 5th grade Washington DC public school teacher was fired for receiving a poor evaluation, resulting in a heated controversy about the accuracy, fairness, value, and transparency of the region's teacher performance evaluation system. 
At the time, District of Columbia Public Schools' IMPACT teacher evaluation system used test scores from schools under investigation for cheating to calculate so-called 'value-added scores' that would be incorporated into teacher evaluations. Yet teachers risked having students enter their classes with falsely inflated scores, making it difficult to meet higher expectations.
Wyosecki had earned excellent observation ratings and was highly regarded by peers and parents. However, she received an uncharacteristically low value-added score and was subsequently fired, despite evidence the evaluation generated by the algorithm was based on falsified student scores.
Her appeal against her dismissal failed, though she was quickly offered a position with another school system.
Operator: District of Columbia Public Schools Developer: Mathematica Policy Research Country: USA Sector: Education Purpose: Assess and rank teacher performance Technology: Value-added modelIssue: Accuracy/reliability; Bias/discrimination - income, geography; Fairness; Effectiveness/value Transparency: Governance; Complaints/appeals; Black box"
Amazon Astro home robot,"Amazon has launched Astro, a robot powered by the company's Alexa smart home technology and facial recognition that it says can patrol a home automatically, notify owners if it detects something unusual, and be used to check on pets when people are not in their homes. 
Internal development documents and video recordings leaked to VICE suggest the robot may perform poorly and its person recognition capability is 'heavily flawed'.
The documents also reveal that Astro designers worry that the product is a 'disaster that's not ready for release' and that it may 'throw itself down stairs' 'if presented the opportunity'. 
Operator: Amazon Developer: Amazon Country: USA Sector: Consumer goods Purpose: Strengthen home security Technology: Robotics; Computer vision; Facial recognition Issue: Privacy; Surveillance; Safety; Accuracy/reliablity; Dual/multi-use; Appropriateness/needTransparency: Black box"
Tesla Model X crashes into five police officers,"Five Houston police officers have filed a lawsuit (pdf) against Tesla after a Model X had slammed into them while they were operating a traffic stop in Splendora, Texas, in February 2021. 
The driver of the Tesla, a restaurant owner, has reputedly had too much to drink and had engaged the car's Autopilot system. 
The officers claim that 'design and manufacturing defects' were 'known to Tesla', were responsible for the crash and that Autopilot 'failed to detect the officers’ cars or to function in any way to avoid or warn of the hazard and subsequent crash.'
Operator: Pappas Restaurants Inc Developer: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance systemIssue: Safety; Accuracy/reliability Transparency: Black box; Legal"
Facebook Portal,"In October 2018, Facebook launched Portal, a counter-top smart screen with an always-on microphone connected to Amazon's Alexa virtual assistant, and a camera for video chat that automatically tracks people's movements. 
Initial reaction to the initial two models was largely positive. However, some journalists, reviewers and digital rights advocates took issue with Portal's default ability to listen to, track and record everything people are saying and doing within range of its microphone and camera. 
Concerns about Facebook's use of users' data had circulated for many years but took on greater resonance when the full nature of the company's relationship with Cambridge Analytica became headline news earlier in 2018.
In November 2022, Meta announced it would be discontinuing its Portal product line.
Operator: Meta/Facebook Developer: Meta/Facebook, Amazon Country: USA Sector: Technology Purpose: Enable video calls Technology: Computer vision Issue: Privacy; Dual/multi-use Transparency: Governance"
Chinese study predicts criminality by analysing facial features,"According to Xiaolin Wu and Xi Zhang, researchers at Shanghai Jiao Tong University, software can automatically detect with 85.5% certainty that a human is a criminal by analysing their facial features.
The research study results (pdf) indicate that people with smaller mouths, curvier upper lips and closer-set eyes are more likely to be criminals. The research looked at 1,856 faces of Chinese men aged 18-55 with no facial hair, facial scars or other markings, of which 730 belonged to criminals.
Wu and Zhang stated in their study that they they do not intend to or are not 'qualified to discuss or debate on societal stereotypes', they stand accused of conducting physiognomy, phrenology, and pseudoscience.
They also said their system cultivates 'no biases whatsoever due to past experience, race, religion, political doctrine, gender, age, etc,' before admitting it should be tested using a dataset of different races, genders and facial expressions before it could be implemented on a broader scale. 
Given the software's potential for inaccuracy and bias, there are concerns that the system could wrongly identify criminals if used in the real-world. 
In 2020, a similar study by researchers at Harrisburg University had to be withdrawn after an academic and industry backlash.
Operator: Developer: Xiaolin Wu; Xi Zhang; Shanghai Jiao Tong UniversityCountry: China Sector: Research/academia Purpose: Recognise/predict criminality Technology: Facial analysis; Computer vision; Deep learning; Neural network; Machine learning Issue: Accuracy/reliability; Bias/discrimination - race, gender, age, income; Ethics Transparency:"
JR East facial recognition surveillance,"East Japan Railway Co. (JR East), Japan's largest passenger rail company, used facial recognition in metro stations across Tokyo during the Tokyo Olympics and Paralympics to identify released criminals, wanted suspects, and people acting suspiciously.
However, the company's Victim Notification Service, which informed victims of crime of the identity of their attacker, later came under fire from privacy advocates concerned that the system may not be accurate and should not include released prisoners.
JR East suspended the addition of released prisoners to its database on the basis of 'insufficient social consensus building'. Japan currently has no law regulating the use of facial recognition.
Operator: East Japan Railway Co. Developer: East Japan Railway Co. Country: Japan Sector: Transport/logistics Purpose: Identify criminals and suspects Technology: Facial recognition Issue: Privacy; Surveillance; Ethics Transparency: Governance; Marketing"
Queensland high-risk domestic violence predictions,"The Queensland Police Service (QPS) is to trial a form of predictive policing in which artificial intelligence is used to determine the potential risks of known domestic violence offenders.
Those categorised as 'high risk' will be visited at home by police before domestic violence escalates, and before any crime has been committed.
QPS says it removed ethnicity and geographic location attributes before training the AI model. 
However, human and civil rights advocates are concerned that Aboriginal and Torres Strait Islander people will be unfairly targeted, as they are in real life.
Operator: Queensland Police Service (QPS) Developer: Queensland Police Service (QPS)Country: Australia Sector: Govt - police Purpose: Identify high-risk domestic violence offenders Technology: Prediction algorithm; Risk processing analysis Issue: Bias/discrimination - race, ethnicity; Ethics Transparency: Black box"
"Xiaomi 5G mobile communications tracking, censorship","Lithuania's National Cyber Security Centre has discovered (pdf) that Xiaomi mobile phones have a built-in ability to automatically detect and censor over 449 terms, including 'Free Tibet', 'Long live Taiwan independence' and 'democracy movement'.
The investigators say the software had been turned off on Xiaomi's Mi 10T 5G phone for the 'European Union region', but could  be turned on remotely at any time. 
The Lithuanian government is recommending its citizens not to buy new Chinese phones, and quickly to get rid of those already purchased.
Operator: Xiaomi Developer: XiaomiCountry: Lithuania Sector: TechnologyPurpose: Detect & censor sensitive terms Technology: Content moderationIssue: Freedom of expression - censorship; Security Transparency: Governance"
Facebook data leak exposes Balkan troll farm disinformation,"According to a leaked Facebook 2019 document, troll farms are estimated to have reached 140 million US users per month and nearly half of all Americans in the run-up to the 2020 presidential election.
Largely based in the Balkans, the troll farms are seen to have targeted four primary groups - American Indians, Black Americans, Christian Americans, and American women - with a mixture of propaganda, misinformation and disinformation.
Authored by former Facebook data scientist Jeff Allen and leaked to the Technology Review, 75 percent of those users consumed troll content not because they followed a page but because Facebook’s recommendation engine proactively served it to them.
According to the Technology Review, Allen left Facebook shortly after writing the document, partly because the company 'effectively ignored' his research.
In October 2021, it came to light that a book on Veles, the 'fake news' capital of North Macedonia, by Magnum photographer Jonas Bendiksen, turned out to be an elaborate work of disinformation itself. 
Operator: Meta/Facebook Developer: Meta/FacebookCountry: US Sector: TechnologyPurpose: Scare/confuse/destabilise Technology: Content recommendation systemIssue: Accuracy/reliability; Mis/disinformation Transparency: Governance; Black box"
NHS Digital/iProov health data sharing opacity,"The Guardian reports that NHS Digital refuses to share details of how the facial data of UK citizens is being stored or shared. NHS Digital collects and stores UK citizens' facial recognition data using iProov, a London-based private technology company.
Other than to confirm law enforcement bodies are able to request data, neither entity will share details of how the data, which is classified as sensitive under UK law, is collected and stored, sparking concern among digital rights and privacy campaigners. 
Neither will either party share or publish the contract on 'security' grounds. Reports also point to iProov's close links with the UK's ruling Conservative Party, to which two of its directors have donated in recent years.
Operator: NHS Digital Developer: iProovCountry: UKSector: Gov - HealthPurpose: Store facial verification data Technology: Facial recognitionIssue: Privacy; Security Transparency: Governance; Privacy"
Alexei Navalny smart voting bot blocking,"Apple, Google and Telegram have blocked a smart voting chatbot devised by jailed Russian opposition leader Alexei Navalny on the day the country starts voting in parliamentary elections. 
Navalny's bot was seen as a way for voters opposed to Vladimir Putin to identify candidates able to defeat pro-Kremlin United Russia party candidates.
Telegram founder Pavel Durov said the service would abide by Russia’s 'election silence', a law practiced in other countries that prohibits campaigning during the elections and that Telegram would 'plan' to limit the functioning of bots associated with election campaigns in the future. 
Operator: Apple, Google, Telegram  Developer: FBKCountry: Russia Sector: Politics Purpose: Facilitate tactical voting Technology: Chatbot Issue: Freedom of expression - censorship  Transparency: Governance"
"Facebook, Google abortion 'reversal' ads","A new study by the Center for Countering Digital Hate (CCDH) finds Facebook and Google have been being allowing ads for so-called 'abortion reversal' pills that have been deemed unsafe and unscientific by The American College of Obstetricians and Gynecologists.
CCDH discovered that Facebook ran 92 abortion reversal ads between January 1, 2020, and September 8, 2021 from from Live Action News, Live Action, and Heartbeat International worth between $115,400 and $140,667, and that the ads were seen 18.4 million times since January 2020, and 700,000 times by 13 to 17 year-olds. 
Both companies' policies ban 'unsafe' or 'misleading' advertising.
Operator: Meta/Facebook; Alphabet/Google Developer: Meta/Facebook; Alphabet/GoogleCountry: USA Sector: Health Purpose: Sell advertising Technology: Facebook Ads; Google Ads Issue: Ethics; Mis/Disinformation Transparency: Governance"
Facebook/Ray-Ban Stories smart glasses,"Touted as having been 'designed for privacy', Facebook and Ray-Ban's new Stories smart glasses are primarily designed with the privacy of their users in mind rather than the people in their sights, according to several high-profile journalists, commentators, and privacy advocates.
Furthermore, critics argue, photos and videos of everyone in the sights of Stories wearers can can easily and quickly be used, manipulated or otherwise abused, for multiple purposes, including impersonation, fraud, and reputational attacks.
Concerns have also been raised by regulators about the visibility of the LED indicator which lights when the user is taking a video, and whether this constitutes an effective way of putting other people on notice that they are being recorded.
According to an August 2022 Wall Street Journal article, Stories smart glasses have failed to catch on, with less than 10% of devices purchased being used monthly.
Operator: Meta/Facebook  Developer: Meta/Facebook; EssilorLuxottica Country: USA Sector: Technology Purpose: Capture photos, video Technology: Computer vision Issue: Privacy; Surveillance; Dual/multi-use  Transparency: Privacy"
Australia police COVID-19 quarantine facial recognition,"Australian provinces South Australia, Victoria, and New South Wales have been trialling facial recognition software that enables police to check whether people are quarantining at home during COVID-19. 
In the trial, people must respond to random check-in requests by taking a photo of themselves at their designated home quarantine address. 
Police may follow up with a visit to the location to confirm the person's whereabouts should the software not verify the image against their 'facial signature'. 
Rights advocates warn the technology may be inaccurate, threaten people's privacy, and be used by the police for other purposes.
Operator: NSW Health; NSW Police; Department of Health Victoria Developer: Genvis Pty Country: Australia Sector: Gov - policePurpose: Enforce COVID-19 quarantine Technology: Facial recognition Issue: Privacy; Surveillance; Dual/multi-use Transparency: Governance; Privacy"
"Chinese study uses facial recognition to identify Uyghurs, Tibetans","A research study by a Chinese professor at Curtin University, Australia, on 'Chinese ethnical groups' that used facial recognition to identify Uyghurs, Tibetans and Koreans was found to have breached the university's ethics standards. 
Wanquan Liu published the study (pdf), which he co-authored with three other academics at Chinese universities and which was co-funded by the Chinese government, without the consent of its subjects, and apparently without the university's knowledge. 
Liu resigned his post and moved to a university in China. The university said it would overhaul its informal academic research approval procedures and has asked publisher Wiley to retract the study.
Operator: Wanquan Liu; Cunrui Wang; Qingling Zhan; Yu Li; Lixin Miao Developer: Wanquan Liu; Cunrui Wang; Qingling Zhan; Yu Liu; Lixin Miao Country: Australia, China Sector: Research/academiaPurpose: Identify Uyghur & Tibetan minorities Technology: Facial recognition Issue: Ethics; Privacy; Surveillance Transparency: Governance; Privacy"
Facebook Cross-check VIP whitelisting,"Cross-check (or 'XCheck') is a secretive system run by Facebook that double checks the posts of over 5 million 'VIP' users, including Donald Trump, Elizabeth Warren, and Brazilian footballer Neymar.
According to Meta VP Nick Clegg, the cross-check system aims to prevent potential over-enforcement ('when we take action on content or accounts that don’t actually violate our policies') and to double-check cases where there could be a higher risk for a mistake or when the potential impact of a mistake is especially severe. 
A September 2021 Wall Street Journal investigation found people added to the cross-check list are permitted to post rule-breaking content, including harassment, incitement to violence, and misinformation and disinformation, and are immune from Facebook enforcement actions.
In December 2022, Facebook’s Oversight Board criticised Facebook owner Meta for giving cross-check users undue deference, understaffing, opacity, and unfairness. It found that 'Cross-check is currently neither designed nor implemented in a manner that meets Meta’s human rights responsibilities and company values.' 
The board also argued the programme 'put Meta’s business interests over the program’s stated goals of protecting public discourse.' It went on to say that 'despite significant public concern about the program, Meta has not effectively addressed problematic components of its system.'
A confidential 2019 internal review of Facebook’s whitelisting practices discovered by the WSJ investigation said 'We are not actually doing what we say we do publicly', and goes on call cross-check 'a breach of trust'.
According to the Oversight Board, 'Meta has repeatedly told the board and the public that the same set of policies apply to all users. Such statements and the public-facing content policies are misleading.' 
'Meta does not inform users that they are on cross-check lists and does not publicly share its procedures for creating and auditing these lists. It is unclear, for example, whether entities that continuously post violating content are kept on cross-check lists based on their profile. This lack of transparency impedes the Board and the public from understanding the full consequences of the programme', the board observed.
It recommended that Meta 'radically increase transparency around cross-check and how it operates. Meta should measure, audit and publish key metrics around its cross-check programme so that it can tell whether the programme is working effectively.'
In March 2023, Meta agreed to the Oversight Board's recommendation to publish regular Transparency Reports onCross-check and to limit the distribution of content from high-profile individuals that likely violated platform rules until their posts had been adjudicated. 
Operator: Meta/Facebook Developer: Meta/FacebookCountry: USA Sector: TechnologyPurpose: Moderate content Technology: Content moderation system Issue: Governance; Fairness Transparency: Governance; Complaints/appeals; Marketing"
Instagram 'aware of' teen girls' mental health harms,"Facebook has been aware for years that teenagers blame Instagram for increased levels of anxiety and depression, has made little effort to address the issue, and plays it down in public, according to leaked documents obtained by the Wall Street Journal. 
Internal research, presentations and emails by the social network reveal Facebook knew its photo-sharing app Instagram has harmful effects on many of its young users, particularly teenage girls. Over 40% of Instagram’s users are 22 years-old or younger.
US Senators Richard Blumenthal and Marsha Blackburn said they 'will use every resource at [their] disposal to investigate what Facebook knew and when they knew it.'
Facebook CEO Mark Zuckerberg responded by saying that he did not reckon 'the research is conclusive' on the extent to which social media impacts children’s declining mental health. 
According to Forbes, the social network has refused many requests from members of Congress to share its research on children’s mental health, arguing it is 'kept confidential to promote frank and open dialogue and brainstorming internally.'
Operator: Meta/Facebook  Developer: Meta/Facebook Country: USA Sector: TechnologyPurpose: Moderate content Technology: Content moderation system Issue: Ethics; Hypocrisy  Transparency: Governance; Black box; Marketing; Legal"
DeepFaceLive face swapping,"DeepFaceLive is an open-source tool released in August 2021 that enables users to swap their faces with other people in real-time on Skype, Zoom, Twitch and other video-based streaming and messaging systems.
As noted by the DailyDot, DeepFaceLive is proving popular with TikTok users and content creators for a range of purposes, including transforming themselves into celebrities.
However, the ease with which DeepFaceLive can be made to spoof biometric systems, commit fraud, create revenge porn, concoct and disseminate misinformation and disinformation, and other forms of misuse and abuse, is also drawing flak. 
DeepFaceLive was created by secretive Russian developer Ivan Petrov (aka 'Iperov'). Petrov has also been credited with helping develop DeepFaceLab, the most widely used open-source tool for creating deepfakes.
Operator: DeepFaceLive Developer: Ivan Petrov Country: RussiaSector: Technology Purpose: Transform identity Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Privacy; Ethics; Dual/multi-use; Mis/disinformation Transparency: Governance"
LAPD personal social media data collection,"The Hill reports that the Los Angeles Police Department (LAPD) has been collecting the social media details of every citizen it interviews, including people who have not been not arrested or accused of a crime.
Thousands of documents obtained by non-profit organisation The Brennan Center for Justice reveal that LAPD officers must record a civilian’s Facebook, Instagram, Twitter and other social media accounts on 'field interview cards', alongside basic biographical information.
In a memo, LAPD police chief Michael Moore says interview cards would be reviewed by supervisors to ensure they are complete, so that tey could later be used in 'investigations, arrests, and prosecutions'.
The findings raise concerns about civil rights and mass surveillance without justification, as well as potential privacy abuse. The LAPD told the Guardian that the field interview card policy was being updated, but 'declined to provide further details.'
The documents only came to light after the LAPD had refused to hand over documents in a records request filed by The Brennan Center and was sued.
Operator: Los Angeles Police Department (LAPD) Developer: Geofeedia; Dataminr Country: USA Sector: Govt - policePurpose: Monitor individuals Technology: Social media monitoring Issue: Surveillance Transparency: Governance; Legal"
"TikTok USA recommends drugs, alcohol to children","TikTok was found to be actively recommending drugs, alcohol, and sexual content to children as young as 13, according to a Wall Street Journal investigation.
The WSJ set up a series of fake accounts to see how TikTok's algorithm worked. TikTok served one account registered as a 13-year-old at least 569 videos about drug use, references to cocaine and meth addiction, and promotional videos for online sales of drug products and paraphernalia. 
Despite their age settings, the more sexual content the teenagers viewed, the more they were given in return, with hundreds of similar videos appearing in the feeds of the Journal’s other minor accounts.
A spokesperson told the Journal that it had removed some of the reported videos, and restricted distribution of others. It also said it does not currently differentiate between content served to adults and children, but that it was working to on a new filter for younger users.  
Operator: ByteDance/TikTok Developer: ByteDance/TikTokCountry: USA Sector: Media/entertainment/sports/artsPurpose: Recommend content Technology: Recommendation algorithm Issue: Safety; Accuracy/reliability Transparency: Governance; Black box"
NIO ES8 crashes into highway patrol vehicle,"31-year-old Lin Wenqin died when his Nio ES8 crashed into a highway maintenance vehicle in Putian, a city in eastern China. 
Reports state Nio's Navigate on Pilot (NOP) driver-assistance feature had been activated. 
Afterwards, a Beijing law company hired by Lin Wenqin's family accused Nio of tampering with the car's data and accessing and deleting potentially incriminating evidence without the approval of the traffic police. 
Nio responded that the intervention was motivated by safety and that it is cooperating with the police investigation.
The carmaker later introduced an app-based test that all vehicle owners must complete before accessing its NOP system. 
Operator: Nio Developer: Nio Country: China Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: NIO Pilot Issue: Safety; Accuracy/reliability Transparency: Governance; Complaints/appeals; Black box; Legal"
Rio de Janeiro facial recognition wrongful arrests,"The Rio de Janeiro state government carried out a facial recognition test project in 2019 with the aim of identifying criminals and preserving public order. 
Operated by telecoms company Oi, the project was divided into two phases: first, limited to Copacabana during the 2019 carnival, and second, in the Maracanã neighbourhood and around Santos Dumont Airport from June to October 2019.
Despite no missing persons being found, a 90% error rate for the facial recognition technology, and a number of mistaken arrests  made, the authorities declared the first phase a success, with five search and seizure warrants served, three arrest warrants issued, and three vehicles recovered. 
Police arrested a woman who was buying gold and silver on the streetside and who had been 'recognised' by its facial recognition system as Maria Lêda Félix da Silva, a criminal who had murdered her husband and was on the run from the police. The woman was taken to a police station as she did not have an identity document with her at the time of her arrest, and was later released.
During phase two of the project, Rio police arrested eleven people at matches at the Maracanã stadium. Under pressure from O Panóptico, a project that monitors facial recognition use in Brazil, the authorities were forced to admit (pdf) that seven of these were false positives. 
The Rio de Janeiro facial recognition project, and the technology's use across Brazil, has raised concerns about poor accuracy, racial and ethnic discrimination, and its disproportionate impact on marginalised communities. 
According to O Panóptico, 90.5% of the 184 people arrested using facial recognition in Brazil in 2019 were Black, and were largely detained for low-level crimes such as petty theft and robbery. 
From the start, the Rio facial recognition project was shrouded in mystery. Details of the contract with Oi,  which it turned out had been fined in 2014 for selling customer data without their consent - were conspicuously thin, as were details of the databases used to train and feed the system. In addition no information was provided as to how people were identified, for long their information was retained, or who was permitted to access the system.
Operator: Civil Police of Rio de Janeiro State Developer: Oi; Huawei Country: BrazilSector: Govt - police Purpose: Identify criminals, preserve public orderTechnology: Facial recognition; Automated license plate/number recognition (ALPR/ANPR) Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Privacy; Surveillance Transparency: Governance; Black box
Oi website
Nunes P., Silva M.R., de Oliveira S.R. (2022). A Rio of Cameras with Selective Eyes
Mesquita Ceia E., Spadaccini de Teffè C. (2022). Facial Recognition and Public Security in the City of Rio de Janeiro (pdf)
https://drive.google.com/file/d/1IZMnvVLJD6Nu2OsRhIefKned-KgiNlQH/view
https://odia.ig.com.br/rio-de-janeiro/2019/07/5662023-reconhecimento-facial-falha-e-mulher-e-detida-por-engano.html
https://oglobo.globo.com/rio/reconhecimento-facial-falha-em-segundo-dia-mulher-inocente-confundida-com-criminosa-ja-presa-23798913
https://www.correio24horas.com.br/noticia/nid/inocente-e-confundida-com-criminosa-por-camera-de-reconhecimento-facial-no-rio/
https://olhardigital.com.br/2019/07/10/seguranca/mulher-e-detida-no-rio-por-erro-em-camera-de-reconhecimento-facial/
https://noticias.uol.com.br/cotidiano/ultimas-noticias/2019/01/24/cameras-monitoramento-carnaval-rio.htm
https://www.reuters.com/article/brazil-tech-facialrecognition/feature-brazil-turns-facial-recognition-on-rioters-despite-racism-fears-idUSL8N33311N 
https://news.law.fordham.edu/fulj/2022/01/06/brazilian-cities-and-facial-recognition-a-threat-to-privacy/
https://www.fastcompany.com/90299268/brazil-is-using-facial-recognition-tech-during-rios-carnival
https://www.vice.com/en/article/5dp8wq/brazils-biggest-metro-could-get-facial-recognition-cameras-that-reinforce-racist-policing
Sao Paulo METRO advertising facial biometrics
Louisiana police Randall Reid wrongful arrest, jailing
Apple/SIS misidentification, wrongful arrest
Page info Type: SystemPublished: February 2023"
Singapore Xavier patrol robots,"Singapore government's Home Team Science and Technology Agency (HTX) has announced it is conducting a three-week test of robots that will scan the Toa Payoh neighbourhood for 'undesirable social behaviours'.
These behaviours include the congregation of more than five people, smoking in prohibited areas, illegal hawking, improperly parked bicycles, and riding motorised active mobility devices and motorcycles on footpaths.
The agency says the robots will be used for surveillance and public education during the trial, and will not be used for law enforcement. It went on to argue that robots are needed to address a labour crunch in an ageing society with a smaller workforce.
Operator: Home Team Science and Technology Agency (HTX)  Developer: HTX, A*STAR Country: Singapore  Sector: Govt - interior Purpose: Manage 'undesirable behaviour' Technology: RoboticsIssue: Surveillance; Privacy; Employment - jobs Transparency: Governance"
ANPR confuses T-shirt with personalised number plate,"A couple in Bath, UK, were issued with a fine after the local council's license plate recognition system confused the car's number with a word on the T-shirt of a local pedestrian.
Dave and Paula Knight had received the fine for supposedly driving in a bus lane in Bath, despite not being in the city at the time. It transpired a CCTV camera had detected the word 'KNITTER' on a pedestrian's clothing when she had been walking down the bus lane, and confused it with Mr Knight's personalised number plate KN19 TER.
Bath and Somerset Council later admitted nobody had checked the computer-generated image, and agreed to cancel the fine, which had increased from GBP 60 to GBP 90 after the couple had resisted pating within 30 days.
Operator: Bath and North East Somerset Council Developer: 
Country: UK
Sector: Govt - municipal
Purpose: Detect traffic violations
Technology: Automated license plate/number recognition (ALPR/ANPR) Issue: Accuracy/reliability
Transparency:"
Facebook labels black men 'primates',"Facebook's recommendation system has been discovered to be asking users watching a Daily Mail video featuring Black men whether they like to '[k]eep seeing videos about Primates.'
The video was published late June by the Daily Mail on its Facebook page and depicted white people, including police officers, confronting Black men. No primates were shown.  
The discovery led to widespread condemnation of Facebook apparent inability to manage unsafe content on its platform, and to discussions about its over-reliance on technology to detect and manage unsafe content. 
The company apologised for its 'unacceptable error', said it had disabled its topic recommendation feature responsible for the message, and will look to ensure it does not happen again.
Operator: Meta/Facebook Developer: Meta/FacebookCountry: USA Sector: TechnologyPurpose: Recommend topics Technology: Topic recommendation systemIssue: Bias/discrimination - race Transparency: Governance; Black box"
Toyota Paralympics self-driving bus crash,"Sky News reports that a Toyota e-Palette self-driving bus used to ferry athletes during the 2020 Paralympic Games in Tokyo has hit a Japanese visually impaired athlete. 
Aramitsu Kitazono was attempting to cross a street at a designated crossing within the Athletes Village when he was hit. He was left unable to compete. 
The following day Toyota president Akio Toyoda apologised, before adding that 'It shows that autonomous vehicles are not yet realistic for normal roads.'
The service was temporarily suspended before being reintroduced with extra safety staff.
Operator: Toyota  Developer: Toyota Country: Japan Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Safety; Accuracy/reliability  Transparency: Governance; Black box"
Lumidolls robot brothel,"Lumi Dolls is the world's first robot brothel. Located in Barcelona, Spain, the venue is staffed by four 'hyper-realistic' robot prostitutes that cost around EUR 120 an hour.
Lumi Dolls says the dolls are properly disinfected with antibacterial soaps before and after each client session. It recommends using condoms, which are provided free of charge. 
Promoted as somewhere customers can act out their 'wildest fantasies', reports say it was forced to turn away individuals wanting to act out rape fantasies and paedophile abuse.
It also resulted in a backlash from the local community, and from human prostitutes concerned about losing their jobs, forcing the proprietor to relocate to a secret address.
The opening also prompted ethicists and civil rights advocates to voice their concerns about human-machine anthropomorphism. 
University of Sheffield AI expert Professor Noel Sharkey argued in a 2017 Foundation for Responsible Robotics report that guilt-free sex with a submissive doll or robot could encourage the objectification of women, abuse, rape, and paedophilia.
Operator: Lumi Dolls Developer: Lumi Dolls Country: Spain Sector: Media/entertainment/sports/arts Purpose: Provide sexual servicesTechnology: Robotics Issue: Ethics; Anthropomorphism; Employment - jobsTransparency: Marketing"
Hour One 'character' clones,"An Israeli company marketing an AI system that enables anyone to create a full digital clone of themselves speaking on camera in any language was criticised for providing inadequate usage and protection controls.
Tel Aviv-based 'video transformation company' Hour One pays people to deepfake their face so that their 'characters' can be used in promotional, commercial, and educational videos. The company says it has a library of around 100 characters.
Critics point out that Hour One's ethics policy provides little information or guidance about how personal clones should - or should not be - used by its various customers. And it says little about how it will protect people's privacy.
Operator: Hour One Developer: Hour One Country: Israel Sector: Business/professional servicesPurpose: Market products/services Technology: Computer vision Issue: Privacy; Security; Dual/multi-useTransparency: Governance; Marketing"
Tesla Model 3 hits parked police car,"A Tesla Model 3 reportedly with its Autopilot driver assistance system engaged slammed into a parked police car with its emergency lights activated on an interstate road in Orlando, Florida. 
The 2019 Tesla Model 3 narrowly missed the police car driver, who was helping the driver of a disabled Mercedes, and collided with the other vehicle. The Tesla driver and the driver of the disabled vehicle suffered minor injuries and the trooper was unhurt.
The driver claimed he was using Autopilot, Tesla's partially automated driving system, at the time of the crash. An investigation is taking place into what caused the crash, including Autopilot's role in it. 
The US government announced a sweeping investigation into Tesla's Autopilot driving system in August 2021.
Operator:  Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
"Tesla Model 3 hits six children, adult","A Tesla Model 3 crashed into six children and an adult outside a school in Ardingly, East Sussex, UK. 
One child had to be airlifted to hospital with serious injuries. 
The police are investigating the incident, including whether Autopilot was turned on at the time of the crash.
Operator:  Developer: TeslaCountry: UK Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Safety; Accuracy/reliability Transparency: Black box"
Mater Dei Hospital 'worthless' medicine robots,"Two robots tasked with distributing medicine across Mater Dei Hospital in Malta have been criticised by the country's nurses  union for repeatedly breaking down and stocking the wrong drugs, describing them as 'worthless' and a 'complete failure'. The system was reported to fail frequently in providing the correct medicine dosages to patients. 
Malta health minister Chris Fearne defended the introduction of the machines by saying they would increase efficiency and they were meant to help and not replace nurses. However, the union claims eight nurses in one ward have resigned since the robots' were introduced, with the rest demanding to be transferred.
Operator: Mater Dei Hospital Developer: Deenova Country: Malta Sector: Gov - health Purpose: Distribute medicines Technology: RoboticsIssue: Accuracy/reliability; Employment - jobs Transparency: Governance; Complaints/appeals"
US mortgage approval algorithm race discrimination,"An investigation has found that mortgage lenders are much more likely to turn down Latino, Asian, Native American and Black applicants than White ones. 
A Markup assessment of mortgage data, co-published by the AP, discovered that mortgage loan applicants of colour were 40%–80% more likely to be denied than their White counterparts across the US, and that the disparity was greater than 250% in some urban areas.
The algorithms used by lenders are mostly mandated by Freddie Mac and Fannie Mae, whose own underwriting algorithms are a closely held secret.
The Markup's investigation was cited as a reason for the introduction of the US Algorithmic Accountability Act of 2022. 
Operator: Freddie Mac; Fannie Mae Developer: Freddie Mac; Fannie Mae Country: USA Sector: Banking/financial services Purpose: Assess mortgage applications Technology: Underwriting algorithms Issue: Bias/discrimination - race, ethnicityTransparency: Governance; Black box"
Gladsaxe vulnerable children detection,"A system used by Denmark's Gladsaxe municipality for identifying and assessing children at risk from abuse has come under severe criticism for scope creep. More broadly, it is seen to raise concerns about the role of algorithms in democratic societies. 
The so-called 'Gladsaxe Model' consists of custom algorithms drawing on parental health records, unemployment, missed medical and dental appointments and other data provided locally and by Denmark's Udbetaling Danmark benefits agency to produce a points-based risk assessment.
The hostile public reaction forced Gladsaxe to delay the roll-out of the system. However, the authorities had continued to develop and expand it with additional data, including household electricity use. 
Criticism from Denmark's data protection agency and a deepening political backlash led to the system's demise in 2019.
Operator: Gladsaxe Municipality Developer: Gladsaxe Municipality; Udbetaling Danmark  Country: Denmark Sector: Govt - municipal Purpose: Detect vulnerable children Technology: Risk assessment algorithm Issue: Accuracy/reliability; Privacy; Scope creep/normalisation Transparency: Governance; Black box; Marketing"
"US mortgage credit score data economic, racial bias","Stanford University's Laura Blattner and Scott Nelson at the University of Chicago have discovered that predictive tools used to assess mortgage loans are biased against people on low incomes and from ethnic minorities.
Using consumer data and artificial intelligence to test different credit-scoring models, the two find that the accuracy of the underlying data in predicting creditworthiness is more responsible for producing biased outcomes than credit score algorithms, usually because low-income borrowers have limited credit histories. 
Operator: Unnamed Developer: Unnamed Country: USA Sector: Banking/financial services Purpose: Calculate credit score; Predict loan default Technology: Credit score algorithmsIssue: Accuracy/reliability; Bias/discrimination - economic, racial Transparency: Black box"
Facebook facial recognition abuse,"Facebook has been fined USD 5.5m by South Korea's data privacy regulator PIPC for creating and storing facial recognition templates of 200,000 local users without consent between April 2018 and September 2019. 
Netflix and Google received smaller fines for other privacy offences.
Operator: Meta/Facebook Developer: Meta/FacebookCountry: South Korea Sector: TechnologyPurpose: Collect facial biometrics Technology: Facial recognitionIssue: Privacy Transparency: Privacy"
Unity GovTech AI military opacity,"VICE reports that game development tool company Unity's GovTech unit is secretly working for the US Department of Defense. The work is causing unease amongst some of the gaming firm's employees, who are complaining of poor internal transparency. 
Unity responded to VICE by saying the work does not 'knowingly violate our principles or values', including that its work 'does not directly involve the loss of life, harm of the planet, or a person’s right to equity and inclusion.'
Unity's stated AI principles include the need for honesty, clarity, and transparency.
Operator: US Department of Defense Developer: Unity Country: USA Sector: Media/entertainment/sports/arts Purpose: Develop military products Technology: UnclearIssue: Lethal autonomous weapons; Ethics; HypocrisyTransparency: Governance; Marketing"
GoGuardian Beacon student suicide prevention monitoring,"The Baltimore Sun reports that Baltimore City Scools are monitoring the laptops of local school students for indications of self-harm and suicide. Should an alert be sent, local rather than school police are sometimes being sent to respond. 
Baltimore, like many other school systems, loaned out laptops to students during the COVID-19 pandemic, many of which are loaded with GoGuardian's Beacon suicide prevention software. 
However, reports suggest the software is also being used for disciplinary purposes and can result in LGBTQ students being unintentionally outed, or student expression hampered.
In addition, it appears less wealthy students may be tracked more regularly since school-owned laptops may be their only devices. 
Other reports state school police monitor GoGuardian software after school hours, including on weekends and holidays, raising concerns about students enduring more or less constant surveillance.
Operator: Baltimore City Public Schools; Pekin Community High School  Developer: Liminex Inc/GoGuardian Country: USA Sector: Education Purpose: Detect & categorise at-risk behaviour Technology: Suicide prevention algorithmIssue: Privacy; Surveillance; Bias/discrimination - economic; Dual/multi-useTransparency: Black box"
Tesla Optimus robot,"Optimus is a humanoid robot being developed by Tesla. First announced by Tesla CEO Elon Musk at the the company's AI Day in August 2021 as the 'Tesla bot', two working prototypes named Optimus were revealed in September 2022.
Optimus is a general purpose robot the purpose of which, according to Musk, is to fulfill 'dangerous, repetitive and boring' tasks. It is controlled by Tesla's automotive advanced driver-assistance system. Optimus, Musk says, will cost 'much less than USD 20,000' and be available by 2027. 
Optimus has been met with a mixture of acclaim, scepticism, and disappointment. 
Reaction to Musk's initial announcement of a Tesla bot/robot at the auto company's 2021 AI Day was dominated by scepticism about its purpose and perceived marketing hype, the latter given Musk's numerous unfulfilled claims and promises about fully autonomous cars, robotaxis, and other products.
Tesla's 2022 working prototype appears to justify this scepticism. Whilst some commentators and experts praised Tesla's quick work, others highlighted what they saw as its underwhelming quality and lack of clear vision. 
Others questioned Optimus' humanoid form and asked why Musk appears intent on replacing human activity rather than working in tandem with it.
Operator: Tesla Developer: TeslaCountry: USA Sector: TechnologyPurpose: Eliminate 'dangerous, repetitive, boring tasks'Technology: Robotics; Computer vision; NLP/text analysis Issue: Appropriateness/need; Accuracy/reliability; Employment - jobs Transparency: Marketing"
"USP IS iCOP covert monitoring, data sharing","Yahoo! News reports that the US Postal Inspection Service (USP IS) has been running a secret programme tracking social media users, including racial justice protestors, and creating fake identities. 
According to Yahoo!, USP IS' Internet Covert Operations Program (iCOP) is using social media monitoring software to identity and track investigation targets, and is sharing information with law enforcement agencies. 
The report also alleges that iCOP analysts began monitoring social media to track potential violence at racial justice protests following the death of George Floyd.
The programme is also said to enable USP IS staff to assume fake identities online and employ facial recognition software, including Clearview AI's facial recognition system.
In August 2021, privacy group EPIC filed a lawsuit against the US Postal Service to block the use of facial recognition and social media monitoring tools under iCOP. 
The lawsuit was later dismissed on the basis that EPIC did not suffer a 'cognizable injury in fact' from the Service’s unlawful refusal to disclose information about the programme.
A US Inspector General audit (pdf) of the iCOP programme concluded in March 2022 that the USP IS did not have the legal authority to conduct the sweeping intelligence collection and surveillance of American protesters and others between 2018 and 2021. 
Operator: US Postal Inspection Service (USP IS) Developer: Clearview AI; Zignal Labs; Nfusion Country: USA Sector: Govt - postal Purpose: Identify crime suspects; Identify protestors Technology: Facial recognition; Social media monitoring Issue: Privacy; Surveillance Transparency: Governance; Privacy"
TikTok LGBTQ shadowbanning,"LGBTQ hashtags were 'shadow-banned' on TikTok in Bosnia, Jordan, and Russia, according to a report by the Australian Strategic Policy Institute (ASPI) think-tank.
Shadow banning involves limiting the discovery of content without communicating that a specific hashtag is on a ban list. 
According to TikTok, some hashtags were restricted to comply with local laws, others were limited as they were often used to discover pornographic content, whilst some English and Arabic phrases were banned by mistake.
Operator: ByteDance/TikTok  Developer: ByteDance/TikTok  Country: Bosnia; Jordan; Russia Sector: Media/entertainment/sports/arts Purpose: Block/reduce user/content visibility Technology: Recommendation algorithm Issue: Freedom of expression - censorship Transparency: Governance"
US police facial data sharing opacity,"A report by Georgetown Law’s Center on Privacy and Technology finds law enforcement agencies across the US can access the facial photos of over 117 million US adults stored in government datasets, raising serious questions about privacy and poor  transparency.
Based on over 100 information requests, the researchers reveal the FBI, state and local police departments are using or building facial recognition systems to compare the faces of suspected criminals to their driver’s license and ID photos in a mostly opaque and unregulated manner. 
Moreover, few agencies have instituted 'meaningful protections' to prevent misuse of the technology, the report states.
Operator: Chicago PD; Dallas PD; LAPD Developer: Federal Bureau of Investigation (FBI) Country: USA Sector: Govt - police Purpose: Identify criminals Technology: Facial recognition Issue: Privacy; Accuracy/reliability; Bias/discrimination - racial, ethnicity Transparency: Black box; Governance"
Guns disguised as cases for sale on Facebook Marketplace,"Gunsellers were found to be secretly posting assault rifles and other guns for sale on Facebook Marketplace by disguising them as posts for gun cases or empty boxes.
The practice was spotted by the Wall Street Journal, who discovered over-priced cases listed in several US states. When contacted, sellers revealed that the sale price included the firearm that belongs to the case. 
The practice is in defiance of Facebook's ban on the sale of private guns and ammunition. Facebook uses a combination of artificial intelligence and human moderators to police sales on Marketplace, calling into question the workings and effectiveness of its system.
Operator: Meta/FacebookDeveloper: Meta/Facebook Country: USASector: TechnologyPurpose: Moderate contentTechnology: Content moderation system Issue: Accuracy/reliabilityTransparency: Governance; Black box"
AI unmasks anonymous chess players,"An AI system developed by a team of Harvard University researchers can identify and track people playing chess anonymously, resulting in concerns about its potential to abuse the privacy of chess players and others. 
According to a study of the system, it was originally built to identify different types of chess playing behaviour, and to tag chess players based on their behavioural patterns, in order to help develop more human gaming experiences.
However, the system could also be used to help identify and track people who believe they are anonymously playing chess and, potentially, other games, persuading organisers of the NeurIPS conference that the authors of the study elaborate on the privacy risks.  
Operator:  Developer: Harvard University Country: USASector: Media/entertainment/sports/arts Purpose: Identify chess player behaviourTechnology: Deep learning; Neural network; Machine learning Issue: Ethics; Privacy Transparency:"
Allocation algorithm wrongly places thousands of teachers,"An algorithm used by the Italian government to allocate the location of thousands of teachers in 2016 resulted in many of them having to relocate to inappropriate locations.
Designed and developed by HP Enterprise and Finmeccanica to the tune of EUR 444,000, Italy's so-called 'Bueno Scuola' algorithm was meant to assess and score every teacher based criteria including their work experience and performance and their preferred destinations, and match them with the most appropriate vacancies.
However, teachers and their families were relocated across the country in an apparently more or less random manner, triggering uproar. A subsequent assessment of the system found it to be fully automated, 'unmanageable', full of bugs, and impossible to properly evaluate due to its opaque nature.
The incident triggered controversy about the purpose, design, and the effectiveness of the algorithm, and led to multiple legal complaints, lawsuits, and to the discontinuation of the system.
Operator: Italy Ministry of Education, Universities and Research Developer: HP Enterprise Services Italia; Finmeccanica Country: ItalySector: Education Purpose: Allocate teacher positions Technology: Resource allocation algorithm Issue: Accuracy/reliability; Effectiveness/value; Robustness; Ethics; Legal Transparency: Governance; Black box; Complaints/appeals"
NFL concussion settlement 'race-norming',"A group of Black former National Football League (NFL) players has filed a federal lawsuit claiming that an evaluation system used for the NFL's USD 1 billion player concussion settlement favoured White players, the Wall Street Journal reports.
The players argue that the system blocked some Black claimants from securing payouts by assuming they had lower cognitive functioning when healthy than white players. 
According to The Guardian, 'Under the settlement, [..] the NFL has insisted on using a scoring algorithm on the dementia testing that assumes Black men start with lower cognitive skills. They must therefore score much lower than whites to show enough mental decline to win an award. The practice, which went unnoticed until 2018, has made it harder for Black former players to get awards.
In June 2021, the NFL agreed to halt the use of 'race-norming' and has since stated it would make changes to the concussion settlement. However, the claimants remain unclear how the changes might affect them, whether their prior tests will be rescored, or if they must go undergo another round of cognitive testing.
Operator: National Football League (NFL) Developer: National Football League (NFL), BrownGreer Country: USA Sector: Media/entertainment/sports/arts Purpose: Evaluate dementia claims Technology: Scoring algorithmIssue: Bias/discrimination - race Transparency: Governance; Black box; Complaints/appeals"
Belgrade Safe City surveillance system,"Serbia's Ministry of Interior has unlawfully installed over 1,200 Huawei real-time facial recognition CCTV cameras across Belgrade, according to reports in WIRED and other media outlets.
Piloted in 2018 and launched in 2019 with the stated aim of installing 800 cameras with facial and vehicle licence plate recognition, the expansion marks a major expansion of the Chinese company's 'Safe City' security programme with the Serbian city. 
The expansion has led to accusations of covert surveillance and opacity by human rights and privacy activists, EU members of parliament, and members of the general public. It also resulted in #hiljadekamera, a grassroots campaign led by Serbian digital rights organisation SHARE Foundation.
The system was the subject of a review by Serbia's data privacy commissioner, who in 2019 found that the Ministry of Interior's Data Protection Impact Assessment had not been conducted in line with Serbian national law, arguing 'there is no legal basis for [its]  implementation'.
At the time, Serbia had no data privacy law. The country's Personal Data Protection Law went into effect in August 2019.
According to Privacy International, Serbia's Ministry of Interior reputedly insists all information about the system is 'confidential' and that it holds no information on locations and crime rate analysis for the city.
The Ministry is also said (pdf) to have been routinely blocking freedom of information (FOIA) requests for information on the location and workings of the cameras and system.
Meantime, a Huawei case study about a secret 2017 test of the system in Belgrade was removed from its website.
Operator: Ministry of InteriorDeveloper: HuaweiCountry: Serbia Sector: Govt - interior; Govt - police Purpose: Strengthen law enforcement Technology: Facial recognition Issue: Privacy; SurveillanceTransparency: Governance; Black box; Privacy; Legal"
Tesla Model S crashes into fire engine,"A Tesla Model S travelling at 65 mph crashed into the back of a stationary fire engine on a California highway. 
No injuries were recorded. The driver claimed his car was on Autopilot at the time of the accident. 
In August 2021, the US government announced it had opened a formal investigation into Tesla’s Autopilot after a series of collisions with parked emergency vehicles.
Operator:  Developer: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance systemIssue: Safety; Accuracy/reliabilityTransparency: Black box"
GM Chevrolet Bolt hits motorbike,"Motorcyclist Oscar Nillson is suing General Motors after an autonomous Chevrolet Bolt electric prototype collided in a street in San Francisco, knocking him off his bike and leaving him with injuries to his neck and shoulder.
GM said in its accident report that its vehicle had been changing lanes when the gap ahead closed, forcing it to re-centre itself in the original lane. Meantime the motocyclist, who was coming from behind at a faster speed, moved into the car's way. 
In June 2018, General Motors settled with Nilsson, without admitting fault. Details of the settlement were not disclosed.
Operator: General Motors  Developer: General Motors  Country: USA  Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Safety Transparency: Governance; Black box; Legal"
DEFRA Biodiversity Net Gain metric,"Biodiversity Net Gain (BNG), a biodiversity metric for new developments introduced in the UK government's new Environment Bill, stands accused of being inaccurate, unreliable, and and easy to manipulate. 
The metric outlines determines how new houses, roads, and other construction projects must achieve no net loss of biodiversity, or, if nature is damaged on the construction site, achieve a 10% net gain elsewhere.
However, BNG does not value scrubby landscapes such as sand pits or those used for rewilding programmes, which it logs as a sign of 'degradation' and would not therefore qualify for compensation.
The UK government has a target of building 300,000 new homes a year by the mid-2020s.
Developer: Natural EnglandOperator: Department for Environment, Food and Rural Affairs (DEFRA)Country: UK Sector: Govt - environment Purpose: Manage conservation  Technology: Biodiversity Metric 3 Issue: Accuracy/reliability; Bias/discrimination - rewilding  Transparency: Marketing"
Deepsukebe nonconsensual nudification,
"Twitter image cropping age, weight bias","Bogdan Kulynych, a researcher at Switzerland's EPFL university, has won $3,500 for determining that Twitter's image cropping algorithm favours slender, young, lighter coloured females. 
The company's 'saliency' algorithm, which it uses to crop photos and other images on user timelines, had previously come under fire for perceived gender and racial bias, a problem Twitter had publicly confirmed earlier in 2021. 
Operator: Twitter Developer: TwitterCountry: USA Sector: Technology Purpose: Crop images Technology: Saliency algorithm Issue: Bias/discrimination - gender, race, age, weightTransparency:"
"Xsolla employee monitoring, terminations","Russian payment services company Xsolla has come under fire for terminating 150 employees who had been subjected to secretive, ongoing monitoring and 'big data' analysis of their activity at work.
In a leaked letter to the 150 employees, Xsolla CEO and founder Aleksandr Agapitov said 'You received this email because my big data team analyzed your activities in Jira, Confluence, Gmail, chats, documents, dashboards and tagged you as unengaged and unproductive employees.'
'Many of you might be shocked, but I truly believe that Xsolla is not for you. Nadia and her care team partnered with seven leading HR agencies, as we will help you find a good place, where you will earn more and work even less.'
Agapitov's move resulted in an employee backlash, and the prospect of legal action against Xsolla was raised. 
Agapitov later blamed the mass lay-off on the company's slowing growth.
Operator: Xsolla  Developer: Xsolla  Country: Russia Sector: Banking/financial services  Purpose: Assess productivity Technology: Online/social media monitoringIssue: Employment - workplace/employee monitoring Transparency: Governance; Complaints/appeals
ASSESS DATABASE
https://gameworldobserver.com/wp-content/uploads/2021/08/xsolla-screen-letter.jpg
https://twitter.com/agapitovs/status/1422987608229961730
https://gameworldobserver.com/2021/08/04/xsolla-fires-150-employees-using-big-data-and-ai-analysis-ceos-letter-causes-controversy
https://gameworldobserver.com/2021/08/05/xsolla-cites-growth-rate-slowdown-as-reason-for-layoffs-ceos-tweet-causes-further-controversy
https://app2top.ru/industry/xsolla-uvolila-okolo-150-sotrudnikov-kompaniya-ottalkivalas-ot-analiza-ih-aktivnosti-v-rabochih-prilozheniyah-188953.html
https://vc.ru/hr/277507-xsolla-uvolila-chast-sotrudnikov-permskogo-ofisa-posle-analiza-ih-aktivnosti-v-rabochih-chatah
https://www.forbes.ru/newsroom/biznes/436639-nichego-ne-izmenitsya-esli-ih-ne-budet-glava-permskogo-startapa-obyasnil
https://www.mcvuk.com/business-news/xsolla-fires-150-employees-based-on-big-data-analysis-of-their-activity-many-of-you-might-be-shocked-but-i-truly-believe-that-xsolla-is-not-for-you/
https://properm.ru/news/business/198398/
https://english.elpais.com/usa/2021-10-14/one-second-150-dismissals-inside-the-algorithms-that-decide-who-should-lose-their-job.html
https://www.tecmundo.com.br/mercado/222501-startup-demite-empregados-big-data-ceo-polemiza-improdutivos.htm
https://www.gamesindustry.biz/articles/2021-12-23-2021-the-year-of-living-ridiculously-this-year-in-business
Estee Lauder employee performance assessments
Zhihu job resignation predictions
Page infoType: IncidentPublished: January 2022"
Apple NeuralHash CSAM scanning,"A plan by Apple to start automatically scanning iPhones and iCloud accounts in the US for child sexual abuse material (CSAM) using perceptual hashing was heavily criticised by privacy advocates and others as overly intrusive. 
An August 2021 Financial Times article revealed Apple's NeuralHash system would have automatically scanned devices to identify if they contain photos featuring child sexual abuse before the images are uploaded to iCloud. Matches were to be reported to the US National Centre for Missing and Exploited Children (NCMEC).
The plan had been seen as potentially helpful for law enforcement in criminal investigations, but critics feared it might open the door to unnecessary or disproprotionate legal and government demands for user data.
Widespread hostility persuaded Apple to delay the rollout of its CSAM detection system. The company deleted all reference to the plan on its website in December 2021, and publicly dropped the system a year later.
Operator: Apple Developer: Apple Country: USA Sector: Technology Purpose: Detect child pornographyTechnology: Hash matching; Deep learning; Neural network; Machine learningIssue: Security; Privacy; Surveillance; Accuracy/reliabilityTransparency:"
Amazon One palm print biometric opacity,"TechCrunch has discovered that Amazon is offering customers USD 10 for their palmprints when they sign up for its contactless ID and transaction service Amazon One. 
Customers can use One in a number of Amazon checkout-free stores to pay for goods by holding their palm over a scanner. 
Despite promising to secure biometric data using encryption, data isolation and secure zones, TechCrunch reports that some palm data is used anonymously to improve the system, raising concerns about unstated potential commercial and other uses.
Operator: Amazon Developer: Amazon  Country: USA Sector: Technology Purpose: Verify identity; Authorise transactions Technology: Palm print scanningIssue: Privacy; Dual/multi-use Transparency: Governance"
Roomba hoovers customers' home mapping data,"iRobot, manufacturer of the robo-vacuum Roomba, has been considering selling data of customer to companies like Apple, Google, and Amazon, according to Reuters.
During the interview, iRobot CEO Colin Angle said 'There’s an entire ecosystem of things and services that the smart home can deliver once you have a rich map of the home that the user has allowed to be shared,” Angle said. He also told said that iRobot could reach a deal with Amazon, Alphabet, or Apple within the next couple of years to share their maps of users’ homes.'
However, the company later claimed Angle's words had been taken out of context and that it had no plans to sell customer data. iRobot added support for Amazon’s Alexa voice recognition AI in March 2017. In August 2022, Amazon announced a deal to acquire iRobot. 
The news incensed privacy advocates worried about the unethical sharing and commodification of sensitive personal data.
Operator: Amazon/iRobot Developer: Amazon/iRobot Country: USASector: Consumer goods Purpose: Build spatial map Technology: Spatial mapping Issue: Privacy; Surveillance Transparency: Governance; Privacy"
Amazon Alexa voice data used to target ads,"Researchers discovered that Amazon used voice data from its Echo devices to serve targeted ads on its own platforms and on the web. The finding contradicted Amazon's privacy policy, and resulted in calls for greater transparency on how the company collects, shares, and uses customer data.
By creating several personas to interact with Alexa and studying the results of their interactions with Amazon's Alexa virtual assistant, University of Washington, UC Davis, UC Irvine, and Northeastern University researchers concluded that Amazon and third parties collect users' data from their interactions with Alexa and share it with 41 advertising partners. 
The researchers also discovered that interactions with smart speakers generated far higher - up to 30 times - auction bids from advertisers than those without Amazon Echo speaker data.
Alexa users later filed lawsuits against Amazon for targeting them with ads using their personal voice recording interactions with Alexa without their consent, and that Amazon had been engaging in misleading and unfair conduct. 
Operator:  Developer: AmazonCountry: USA Sector: Business/professional services Purpose: Provide information, services Technology: NLP/text analysis; Natural language understanding (NLU); Speech recognition Issue: PrivacyTransparency: Governance; Black box; Privacy"
Mercadona facial recognition privacy abuse,"Spanish supermarket chain Mercadona's has been fined EUR 2.5m by AEPD, the country's data protection regulator, for illegally collecting and processing childrens' and employees' biometric data. 
The stated aim of the programme was to detect known criminals and people with restraining orders issued against them for attacking Mercadona employees, with cameras equipped with facial recognition identifying relevant transgressors, who were then reported to the police.
AEPD ruled that Mercadona had failed to appreciate that its system processed sensitive data of anyone who entered its supermarkets, including childen and its own employees. 
The regulator also found that Mercadona had violated GDPR Article 12 and 13 transparency requirements, including the ability of those affected to complain or appeal.
Mercadona's facial recognition system was supplied by Israeli company AnyVision (now Oosto). Oosto also manufactures military drones equipped with facial recognition.
Operator: Mercadona Developer: Oosto/Anyvision Interactive Technologies  Country: Spain Sector: Retail Purpose: Detect criminals Technology: Facial recognition Issue: Privacy Transparency: Marketing; Complaints/appeals"
NATO warships AIS spoofing,"WIRED reports that the locations of over 100 warships, including the Royal Navy's HMS Queen Elizabeth aircraft carrier strike group, have been manipulated and their positions falsified. 
Researchers at environmental NGOs SkyTruth and Global Fishing Watch discovered that the automatic identification systems (AIS) of the warships had been altered in order to suggest they were close to a Russian naval base. 
This kind of manipulation increases the risk of safety incidents, and creates disinformation about warship positions and operations. Commentators point to Russia as the most likely culprit. 
Onboard AIS systems broadcast a ship's location, course and speed, and show the same data from other vessels.
Operator: Royal Navy; Swedish Navy; United States Navy Developer: International Maritime Organization Country: RussiaSector: Govt - defence Purpose: Track vessel movements Technology: Automatic identification system (AIS) Issue: Security; Safety; Mis/disinformation; Dual/multi-useTransparency:"
"Facebook, Google anti-Semitism moderation","Facebook, Google, TikTok, Twitter, and other major social media platforms are routinely failing to take down anti-Semitic posts on their platforms, despite being flagged, according to a study by the Center for Countering Digital Hate (CCDH).
Analysing over 'anti-Jewish' 700 posts invoking Holocaust denial and forms of neo-Nazism which had collectively been viewed 7.3 million times, CCDH discovered that 84% had not been not acted upon. 
Facebook is the worst offender, failing to remove 89% of anti-Semitic posts. Facebook says 97% of hate speech was discovered by its system before it was reported by users.
Operator: Meta/Facebook; Alphabet/Google/YouTube; Bytedance/TikTok; Twitter  Developer: Meta/Facebook; Alphabet/Google/YouTube; Bytedance/TikTok; Twitter Country: Global Sector: Technology Purpose: Detect hate speech Technology: Content moderation systemIssue: Bias/discrimination - religion; Safety - harassment Transparency: Governance; Complaints/appeals"
ShotSpotter gunshot detection system,"SoundThinking (named ShotSpotter until April 2023) is a US-based company that manufactures the ShotSpotter acoustic gunshot detection system. Consisting of microphones, sensors, algorithms, and human reviewers, the system alerts police to potential gunfire.
The ShotSpotter system is used by municipal authorities, police departments and school districts across the US, and has been used as evidence in legal trials. According to Chicago mayor Lori Lightfoot, ShotSpotter 'plays an important role' in saving lives.
However, ShotSpotter has also proved controversial.
SoundThinking claims 97 percent accuracy and 0.5 percent false positive rates across its ShotSpotter customers. But a number of third-party studies undermine this claim, raising questions about the system's effectiveness and value. 
A May 2021 study by researchers at the MacArthur Justice Center at Northwestern University’s School of Law found ShotSpotter sent Chicago Police Department (CPD) officers on over 40,000 'dead-end deployments'  and concluded the tool is 'too unreliable for routine use'. The CPD had signed a three-year, USD 33 million contract with ShotSpotter in 2018. 
A 2021 report by Chicago's Investigator General into the use of the system by Chicago police (CPD) concluded 'CPD responses to ShotSpotter alerts rarely produce evidence of a gun-related crime, rarely give rise to investigatory stops, and even less frequently lead to the recovery of gun crime-related evidence during an investigatory stop.'
A 2022 AP investigation reported ShotSpotter has 'serious flaws' in its technology that called into question its effectiveness and value, and undermined the company's marketing claims. The investigation found ShotSpotter's system was unreliable, can miss live gunfire directly under its microphones, and may misclassify car backfires and firework sounds as gunshots.
Questions have been asked about the degree to which human analysts assess and modify ShotSpotter alerts.
SoundThinking/ShotSpotter revealed to AP that alerts are modified 10 percent of the time, and that it stopped showing the system's algorithm’s confidence rating to human reviewers in June 2022 'to prioritize other elements that are more highly correlated to accurate human-trained assessment.'
A July 2021 VICE News report, citing the wrongful arrest, charging and jailing of 63-year-old Michael Williams for a gun murder that was later overturned on the basis of insufficient evidence, suggested SoundThinking analysts 'frequently modify alerts at the request of police departments' - a conclusion also reached by the AP. At Williams' trial, prosecutors cited ShotSpotter sensors to bolster their case. But, according to a motion (pdf) filed by Williams' attorneys, the company’s algorithms had initially classified the sound as a firework and that the location co-ordinates had been altered, persuading the prosecutors to withdraw ShotSpotter evidence against Williams. VICE and AP's accounts were strongly contested by SoundThinking.
ShotSpotter has also been accused of discrimination against minority communities. 
VICE News reported the system was used 'almost exclusively' in non-white communities in Atlanta, Chicago, Cleveland, and Kansas City. 
According to attorneys for Chicago-based community groups, ShotSpotter's lack of accuracy and its use in predominantly Black and Brown communities feeds 'racialized patterns of overpolicing.' 
Civil liberty and privacy advocates have expressed concerns about the use of ShotSpotter microphones and sensors in public spaces and their proximity to housing, and worry that it may set a poor precendent. 
Concerns have also been expressed about the length of time aural data is retained by SoundThinking, and how secure it is.
Claims that the ShotSpotter system has 'serious flaws' that appear to contradict what SoundThinking says publicly has led to accusations that its marketing is misleading and hyped.
SoundThinking has also been criticised for refusing to provide access to the ShotSpotter system so that its claims of accuracy and reliability can be independently peer-reviewed or assessed.
Given ShotSpotter's use in court, the company's lack of transparency 'isn’t acceptable', according to the ACLU.
Operator: Chicago Police Department; Houston Police Department; New York Police Department Developer: SoundThinking/ShotSpotter Country: USA Sector: Govt - police Purpose: Detect gunfire Technology: Gunshot detection system Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, income, location; Effectiveness/value; Oversight/review; RobustnessTransparency: Governance; Black box; Marketing; Legal"
Apple Watch heart rate variability inconsistencies,"Harvard biostatistics researcher JP Onnela has discovered that it is not possible to use Apple Watches for research into heart rate variability after finding the data collected was inconsistent. 
According to Onnela, updates to Apple Watch algorithms mean 'the data from the time period can change without warning'.  'These algorithms are what we would call black boxes — they’re not transparent. So it’s impossible to know what’s in them' he argues.
Operator: Apple Developer: AppleCountry: USASector: Healthcare Purpose: Detect heart rate Technology: Heart rate variability algorithm Issue: Accuracy/reliabilityTransparency: Black box"
Dubai deepfake court evidence,"Dubai-based family lawyer Byron James has revealed that deepfake technology was used to create a fake audio recording of his client threatening another party during a UK custody battle. 
Per UAE's National News, James says a 'heavily doctored' recording of his client appearing to utter 'violent' threats towards his wife had been presented in court, threats he had been adamant that he had not uttered.
Experts examining the deepfake's metadata concluded the recording had been manipulated.
Operator: Unclear/unknown Developer: Anonymous/pseudonymous Country: UAE/Dubai, UK Sector: Govt - justice Purpose: Discredit Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance; Marketing"
Dubai drone cloud seeding,"A programme operated by the Dubai government to generate rain by seeding clouds has raised concerns about dangerous flooding and its possible impact on public health, and the geo-political ramifications of 'rain stealing' and other potential weaponisations of the technology.
First reported by the Washington Post, the state's National Center of Meteorology (NCM) conducted 126 cloud seeding operations since 2021 using drones developed by the University of Reading working in conjunction with the University of Bath. Catapults launch the drones which zap clouds with an electric charge, charging the droplets inside. 
Regular heatwaves and the lack of freshwater meant the programme largely received a positive response by Dubai citizens. But scientists expressed concerns that chemicals used to generated rain are classified as 'possible carcinogens' to humans by the World Health Organisation’s International Agency for Research on Cancer.
Operator: National Center of Meteorology (NCM) Developer: University of Reading; University of Bath Country: UAE/Dubai Sector: Govt - agriculture Purpose: Seed cloudsTechnology: Drone Issue: Environment; Geopolitics; Public healthTransparency: Governance"
TrueAllele DNA algorithm,"TrueAllele is a widely used DNA analysis system developed by Pittburgh-based Cybergenetics that has been used in over a thousand criminal justice cases in the US and internationally.
TrueAllele uses probabilistic genotyping, a fully automated algorithmic method the accuracy and reliability of which has been questioned many times. 
TrueAllele is not open source and whilst its algorithm has been published in peer-reviewed literature, TrueAllele has repeatedly refused to release its source code to prosecutors, government crime laboratories, researchers, civil rights and privacy organisations and others on the basis that it is a trade secret, despite the high risk of wrongful convictions.
In 2017 the results of a TrueAllele DNA assessment were used to sentence Billy Ray Johnson to life in prison without parole. But the court denied the defense team access to TrueAllele’s source code, leading the ACLU to call the use of TrueAllele 'unconstitutional'.
In July 2021, the Washington Post reported on a court case in Fairfax County, Virginia, that saw public defender Bryan Kennedy  informed by Cybergenetics that the algorithm could only be reviewed by a defence expert under an the terms of a non-disclosure agreement the terms of which included a USD 15,000 fee and permission only to take handwritten notes.
The Markup notes that New York City’s proprietary DNA analysis software Forensic Statistical Tool (FST) was assessed in 2016 and found to have a serious flaw that 'tend[ed] to overestimate the likelihood of guilt.' FST was replaced by STRmix, a competitor to TrueAllele the makers of which have admitted to software bugs that led to prosecutors having to replace 24 expert statements in Australia.
According to Mark Perlin, founder and CEO of Cybergenetics 'You don’t learn how a car works by reading its blueprints; you take it for a test run.'
Operator: Virginia Department of Forensic Science (DFS) Developer: Cybergenetics Country: USA Sector: Govt - justice Purpose: Analyse DNA Technology: Probabilistic genotyping Issue: Accuracy/reliabilityTransparency: Governance; Black box; Legal"
Toronto Pearson airport facial recognition test,"The Globe and Mail has revealed that Government of Canada officials 'quietly' tested facial recognition at Toronto's Pearson International Airport in 2016. The aim of the six-month trial was to detect travelers using fake identities. 
Other than a brief statement on its website, Canada Border Services Agency (CBSA) failed to inform travellers that their faces were being scanned. Not did it specify which airport was being used for the trial. 
Details of the trial were obtained by The Globe through a freedom of information (FOI/FOIA) request.
Operator: Canada Border Services Agency (CBSA) Developer: Face4 Systems Country: Canada Sector: Govt - immigration Purpose: Identify deported travellers Technology: Facial recognition Issue: Privacy Transparency: Governance; Privacy"
Twitter suspends Japan PM critics,"Several critics of Japan Prime Minister Suga Yoshihide and his government's policies, notably concerning COVID-19 and the Olympic Games, have had their Twitter accounts suspended. 
No explanation was given for the suspension or for the subsequent restoration of the accounts, after a media and public backlash. 
Twitter later blamed the company's AI system.
Operator: Twitter Developer: TwitterCountry: Japan Sector: PoliticsPurpose: Moderate content Technology: Content moderation system Issue: Accuracy/reliability; Freedom of expression Transparency: Governance; Black box; Complaints/appeals
ASSESS DATABASE
Twitter website
Twitter Wikipedia profile
https://www.asahi.com/ajw/articles/14395800
https://www.asahi.com/ajw/articles/14380956
https://mainichi.jp/english/articles/20210626/p2a/00m/0na/018000c
https://globalvoices.org/2021/07/19/twitter-japan-appears-to-suspend-government-critics/
https://www.huffingtonpost.jp/entry/news_jp_609207fee4b05af50dc91c83
https://lite-ra.com/2018/06/post-4086.html
https://togetter.com/li/1660256
England footballers' racism Instagram moderation
Twitter right-wing content amplification
Page infoType: IncidentPublished: January 2022"
President Xi/'Mr Shithole' Facebook translation,"A post about President Xi’s visit to Myanmar posted on Daw Aung San Suu Kyi's official Facebook page contained references to 'Mr. Shithole' when translated from Burmese to English.
Xi had been visting Myanmar and had signed dozens of agreements covering Beijing-backed infrastructure plans with Aung San Suu Kyi.
According to Reuters, Google’s translation function did not show the same error. A headline in local news journal the Irrawaddy also appeared as 'Dinner honors president shithole'.
Facebook, which is blocked in China, apologised for causing any offence and said its system did not have President Xi Jinping’s name in its Burmese database and guessed at the translation. 
Translation tests of similar words that start with 'xi' and 'shi' in Burmese also produced 'shithole', it said. 
Operator: Meta/Facebook Developer: Meta/Facebook
Country: Myanmar
Sector: Politics
Purpose: Translate text
Technology: Deep learning; Machine learning Issue: Accuracy/reliability
Transparency: Governance"
Henn-na Hotel Tapia robot security vulnerability,"SoftBank's Pepper robot has significant security issues, notably unauthenticated administrative capabilities, according to a paper published by a group of researchers. 
Alberto Giaretta of Sweden's Örebro University and Michele De Donno and Nicola Dragoni of the Technical University of Denmark allege that SoftBank Robotics 'extensively neglected any sort of security assessments before commercializing their product'. 
The two went on to suggest that it would be 'a breeze to remotely turn [Pepper] into a 'cyber and physical weapon', exposing malicious behaviours'.
Pepper is most often used as a customer service robot.
Operator: Softbank Robotics Developer: Softbank Robotics Country: Japan Sector: Technology Purpose: Interact with humans Technology: RoboticsIssue: Security; Safety; Dual/multi-useTransparency:"
Softbank Pepper robot security vulnerabilities,"SoftBank's Pepper robot has significant security issues, notably unauthenticated administrative capabilities, according to a paper published by a group of researchers. 
Alberto Giaretta of Sweden's Örebro University and Michele De Donno and Nicola Dragoni of the Technical University of Denmark allege that SoftBank Robotics 'extensively neglected any sort of security assessments before commercializing their product'. 
The two went on to suggest that it would be 'a breeze to remotely turn [Pepper] into a 'cyber and physical weapon', exposing malicious behaviours'.
Pepper is most often used as a customer service robot.
Operator: Softbank Robotics Developer: Softbank Robotics Country: Japan Sector: Technology Purpose: Interact with humans Technology: RoboticsIssue: Security; Safety; Dual/multi-useTransparency:"
Nissei Eco robot undercuts Buddhist priests,"The configuration of Softbank's Pepper robot as a Japanese Buddhist priest able to deliver funerals sparked consternation that it would replace human priests. 
Developed by plastic moulding maker Nissei Eco Co., Pepper would chant Buddhist funeral sutras while tapping a drum. 
Introduced as a cheaper alternative to its human equivalents, 'Pepper' was said to cost less at JPY 50,000 (about USD 450) per funeral compared to more than JPY 240,000 (USD 2,200) for a human priest. 
In addition to undercutting human priests, Pepper robots were also pitched as able to help resolve staffing and revenue problems at Japanese temples, which had been suffering from falling attendences due increased secularisation and an ageing population.
Operator:  Developer: Softbank Robotics; Nissei Eco Co
Country: Japan
Sector: Religion
Purpose: Conduct funeral rites
Technology: Robotics Issue: Anthropomorphism; Employment 
Transparency:"
"Ocado robots collide, cause fire","Three robots have collided at an Ocado warehouse in Erith, south-east London, leading to a fire, the facility's evacuation and the cancellation of thousands of customer orders. The warehouse is serviced by around 3,500 delivery-packing robots. 
According to the London Fire Brigade, fifteen fire engines and around 100 firefighters were called to extinguish the fire. Ocado put the cost of the fire at GBP 35 million.
The collision is not the first incident involving robots at an Ocado facility. In July 2019, an Ocado robot malfunctioned in its Andover warehouse that resulted in a major fire. 
The Andover warehouse had to re-built at an estimated cost of GBP 110 million.
Operator: Ocado Developer: Ocado Country: UKSector: Transport/logistics Purpose: Pick groceries Technology: Robotics Issue: SafetyTransparency:"
Ocado robot charger malfunction,"A major fire has broken out an Ocado distribution centre in Andover, UK, resulting in the cancellation of customer deliveries. 
The company initially blamed 'some operational issues'. Hampshire Fire and Rescue Service said the fire involved automated packaging machinery.
The blaze lasted four days. It was later discovered that a detection system had failed and employees had turned off the sprinklers and tried to tackle the fire, only calling the fire brigade after one hour. 
The incident cost the firm an estimated GBP 110 million, excluding insurance cover. The facility re-opened in August 2021.
In July 2021, three robots collided at an Ocado warehouse in Erith, south-east London, resulting in the facility's evacuation and the cancellation of thousands of customer orders.
Operator: Ocado Developer: Ocado Country: UKSector: Transport/logistics Purpose: Pick groceries Technology: Robotics Issue: SafetyTransparency:"
Anthony Bourdain deepfake voice,"Roadrunner, a new documentary about the life and death of chef and TV personality Anthony Bourdain, uses deepfake technology to re-create Bourdain's voice without disclosure, and without the permission of Bourdain's widow Ottavia.
In an interview with the New Yorker, filmmaker Neville Morgan describes how he had supplied an AI company with a dozen hours of Bourdain speaking in order to manufacture three quotes he had wanted the TV host to say. 
The interview led to a backlash by film critics, film makers and others, who accused Morgan of underhand, manipulative and unethical behaviour. 'We can have a documentary-ethics panel about it later', Morgan quipped.
Operator: Focus Features Developer: Morgan Neville Country: USA Sector: Media/entertainment/sports/arts Purpose: Entertain Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Ethics; Privacy Transparency: Governance; Privacy; Marketing"
"Livonia skating rink misidentifies, bars black teen","A black teenager was barred from entering an ice skating rink in Livonia, Michigan, after she was misidentified by a facial recognition system. The incident called into question the accuracy and effectiveness of the (unnamed) system, and underscored perceptions that facial recognition systems are 'racist'.
According to Fox 2 Detroit reports that The Riverside Arena skating rink in stopped Lamya Robinson from entering its premises after her face was scanned and the results indicated she had previously been involved in a brawl there. However, Robinson had never been to the skating rink.
Robinson's mother told Fox 2 Detroit, 'To me, it’s basically racial profiling,' said the girl’s mother. 'You’re just saying every young Black, brown girl with glasses fits the profile and that’s not right.'
The skating rink apologised 'if there was a mistake.'
Operator: Riverside Arena, Livonia, Michigan Developer: Unclear/unknown Country: USA Sector: Media/entertainment/sports/arts Purpose: Strengthen security Technology: Facial recognitionIssue: Accuracy/reliability; Bias/discrimination - raceTransparency: Governance; Marketing"
Russia facial recognition ethnicity analytics,"An investigation by video surveillance company IVPM has discovered that facial recognition companies AxxonSoft, Tevian, VisionLabs and NtechLab have been developing and marketing ethnicity identification capabilities in Russia. 
According to Reuters, there is no indication that Russian police have targeted minorities using the firms' software.
Nonetheless, digital rights advocates are concerned facial recognition technology could be used to deepen existing racial and ethnic discrimination in the country.
The Russian government is a customer of each company; NtechLab is partially funded and owned by the Russian government.
Operator: Moscow Department of Technology Developer: AxxonSoft; Tevian; VisionLabs; NtechLab Country: Russia Sector: Govt - transport Purpose: Identify ethnicity Technology: Facial recognition Issue: Surveillance; Bias/discrimination - race, ethnicityTransparency: Governance; Marketing"
England footballers social media racist abuse,
Userviz video game cheating system,"An unknown, frustrated gamer is publicly touting Userviz, an AI and deep learning system that they developed to aim automatically at an enemy while gaming. 
Using machine learning, the system feeds gameplay video from a console via network streaming or capture card and feeds it into a nearby computer, which displays information the cheater can use. It is supposedly able to cheat any game on any platform or device and is 'undetectable'.
UserViz was later shut down after a legal threat from game publisher Activision, whose Call of Duty: Warzone game was being used to market the system.
Online gaming is big business and is subject to widespread cheating and fraud. 
Operator: Userviz; ActivisionDeveloper: Anonymous/pseudonymous  Country: USA; GlobalSector: Media/entertainment/sports/arts Purpose: Cheat video games Technology: Machine learning; Computer vision Issue: Ethics Transparency: Governance"
Tencent 'Midnight Patrol' facial recognition effectiveness,"Tencent's gaming unit has announced it is using facial recognition to stop kids gaming after bedtime. 
A time-sensitive facial recognition system, 'Midnight Patrol' is linked to China's central public security system to monitor and detect minors spending significant amounts of time gaming online between 10pm and 8am.
Chinese kids had been using adults' IDs to circumvent a curfew brought in by the Chinese government in 2019 that had banned them from gaming online between 10pm and 8am and restricted them to 90 minutes on weekdays and three hours on weekends and holidays. 
Some gamers are complaining about the new system; others reckon they can bypass it.
Operator: TencentDeveloper: Tencent Country: ChinaSector: TechnologyPurpose: Restrict gaming hours Technology: Facial recognition Issue: Effectiveness/value Transparency:"
Github Copilot code generator,"Copilot is a cloud-based automated code generation tool launched by software development community Github in June 2021. Initially launched as a limited access 'technical preview', it became available by subscription to developers in June 2022.
Developed in conjunction with OpenAI, Copilot was developed using OpenAI's Codex algorithm, which is based on its GPT-3 language generating algorithm, and trained on publically available source code of different projects in Github repositories. 
The response to Copilot has been mixed. Programmers have generally welcomed it, but others have pointed out that Copilot  reproduces large chunks of copyrighted code verbatim without attributing their owners, raising questions about 'fair use' and licensing and opening it to accusations of copyright abuse and 'code laundering'.
Lawyers are divided on whether Github and OpenAI's use of public repositories can be considered fair use given Copilot draws extensively on code available under many different types of public license, only some of which require attribution for derivative works.
In October 2022, programmer and lawyer Matthew Butterick and the Joseph Saveri law firm announced they were launching a class-action lawsuit against Github, Microsoft and OpenAI for training Copilot using open-source code without the permission of its developers or users.
Operator: Microsoft/GithubDeveloper: Microsoft/Github; OpenAI Country: USASector: Technology Purpose: Generate code Technology: NLP/text analysisIssue: Copyright; Ethics; PrivacyTransparency: Governance"
TikTok uses Bev Standing voice wiithout consent to train AI,"TikTok owner Bytedance settled out of court with voice actor Bev Standing for allegedly using her voice without her permission for its new text-to-speech function.
Hired to read thousands of English sentences for translation by China's Institute of Acoustics, Standing sued Bytedance on the basis that her recordings came into TikTok's possession and her voice used to generate TikTok's new text-to-speech function.
With her voice appearing in 'foul and offensive' viral videos across the world, Standing also accuses TikTok of causing her reputation 'irreparable harm'. 
TikTok has yet to acknowledge in public that it had been using her voice.
Operator: ByteDance/TikTokDeveloper: ByteDance/TikTok Country: USA; GlobalSector: Media/entertainment/sports/arts Purpose: Convert speech to text Technology: Text-to-speech Issue: Copyright; Employment Transparency: Governance; Privacy; Marketing; Legal"
ENA Emergency Severity Index racial bias,"STAT reports that a number of healthcare delivery and planning algorithms widely used across the US are actively reinforcing existing racial and economic biases. 
Researchers at The University of Chicago Booth School of Business' Center for Applied Artificial Intelligence assessed (pdf) algorithms that help emergency rooms triage patients and predict diabetes, amongst other uses. 
The researchers single out The Emergency Nurses Association (ENA)'s Emergency Severity Index, which is found to underestimate the severity of Black peoples' problems and sugggest they are sicker than they are.
Operator: Brigham and Women’s Hospital, Boston; Emergency Nurses Association (ENA)
Developer: Emergency Nurses Association (ENA)
Country: USA
Sector: Health
Purpose: Assess medical condition Technology: Triage algorithm
Issue: Accuracy/reliability; Bias/discrimination - race, economy Transparency: Black box"
Reddit shadowbanning opacity,"Reddit has announced it is replacing the controversial practice of shadowbanning its users with the suspension of their accounts, with users being notified via email and on the website why they have been suspended and for how long. When shadowbanned, users could only see their own content, were not informed that they were banned, and had to go through an opaque and largely unstructured appeals process. 
Operator: Reddit Developer: Reddit Country: USA Sector: Technology Purpose: Block/reduce user visibility Technology: Algorithmic content moderation Issue: Fairness; Freedom of expression - censorship; Harassment Transparency: Governance; Black box"
Amazon automated pricing glitch,"The Guardian reports that a technical glitch on Amazon's UK website has led to the price of thousands of products crashing to 1p, leaving hundreds of small businesses out of pocket.
The glitch stemmed from software developed by Repricer Express which automatically changes the cost of third-party products on Amazon Marketplace to ensure they are the cheapest available.
Amazon says most orders have been cancelled. Repricer Express CEO Brendan Doherty says he is 'deeply sorry for the disruption'.
Operator: Repricer Express; Amazon Developer: Repricer Express Country: UK Sector: Technology Purpose: Change product pricing Technology: Pricing automationIssue: Accuracy/reliability Transparency: Complaints/appeals"
TikTok mandatory beauty filtering consent,"TikTok was automatically touching up the videos of some users' faces without notifiying them or asking for their consent. 
The practice was exposed by Tori Dawn and other users, who shared the issue in videos posted on the social network. TikTok subsequently acknowledged there was an issue, but has not divulged what caused it, or how it was fixed. 
According to the Technology Review, automated beauty filtering is standard practice in China; elsewhere it is offered as an option to which users have to opt-in. 
TikTok has also been accused of suppressing ugly, poor, disabled and LBGT users and content.
Operator: ByteDance/TikTokDeveloper: ByteDance/TikTok Country: USA; Global Sector: Media/entertainment/sports/arts Purpose: Beautify user faces Technology: Computer vision Issue: Appropriateness/need; Privacy Transparency: Governance; Marketing; Privacy"
TikTok creators hate speech detection racial bias,"TikTok's hate speech detection system was accused of racial bias by Black creators and rights activists for appearing to block Black content.
Per The Verge, the furore was sparked by Black influencer Ziggi Tyler finding that phrases including 'Black Lives Matter' and 'Black success' were flagged by TikTok as 'inappropriate' when he tried to update his creator profile.
At the same time, TikTok's system allowed phrases such as 'white supremacy' and 'white success'.
TikTok responded by blaming its content moderation technology, and claims it has fixed the error.
Operator: ByteDance/TikTokDeveloper: ByteDance/TikTokCountry: USA; Global Sector: Media/entertainment/sports/artsPurpose: Detect hate speech Technology: Recommendation algorithm Issue: Bias/discrimination - race Transparency: Black box; Governance"
Foodinho fined for breaching privacy and labour laws in Italy,"Food delivery company Foodinho has been fined EUR 2.6m (USD 3m) for breaching privacy and labour laws in Italy, and has been ordered to update its management algorithms to avoid discrimination.
Italian privacy watchdog Garante said that the Glovo-owned gig worker management company had not explained its automatic order management system properly to its workers, and had failed to ensure that the results of automatic processes to evaluate the workers' performance were correct.
The regulator also said that Foodinho had failed to provide workers with ways to challenge decisions made using the algorithm, including the exclusion of some riders from taking orders, and raised concerns about potential discrimination arising from its management algorithm. It ordered Foodinho to make a raft of changes to its privacy practices and to amend how its management algorithms work.
Glovo said it would appeal the decision. 
Operator: Glovo/Foodinho Developer: Glovo/Foodinho Country: Italy Sector: Transport/logistics Purpose: Manage workers Technology: Automated management systemIssue: Privacy; Bias/discrimination - employment; Accuracy/reliability Transparency: Governance; Black box"
"Alfi personalised, real-time advertising","Alfi is a Miami-based digital marketing company that uses AI and computer vision to understand facial cues and perceptual details and matches relevant advertising or content based on the viewer’s profile. 
The firm says its software is designed to show ads to people based on their age, gender and ethnicity without specifically identifying any individual.
In July 2021, Alfi drew criticism for saying it would run digital tablets equipped with facial recognition in 10,000 Uber and Lyft cabs in the US. The system analyses passengers' reactions to advertising and other content, sending information back to advertisers. 
Uber and Lift told Bloomberg that Alfi contracts with their drivers directly, and that drivers must comply with local laws and regulations. 
The move prompted a written request (pdf) for information from US senators Amy Klobuchar and Richard Blumenthal, who claimed the programme raises 'serious concerns about privacy for your passengers.' 
According to Bloomberg, Alfi also plans to sell user data, including retina tracking, keyword recognition, voice intonation, and demographics.'  
Operator: Uber; Lyft Developer: AlfiCountry: USA Sector: Transport/logistics Purpose: Sell advertising Technology: Computer vision; Facial detection Issue: PrivacyTransparency: Governance; Privacy
ASSESS DATABASE
Alfi website
Alfi 2021 investor presentation
Alfi (2021). ALFI Installing 10,000 Digital Screens in Ubers and Lyfts Starting in Miami
Alfi (2021). Alfi Engages Miami-Based Fulfillment and Distribution Center to Rollout 10,000 Uber and Lyft Digital Tablets Nationwide
Amy Klobuchar (2021). Sen. Klobuchar questions privacy of tablets in Ubers and Lyfts
https://www.vice.com/en/article/5dbyvz/this-company-is-putting-face-tracking-ad-tablets-in-the-back-of-ubers
https://www.vice.com/en/article/epnawa/senators-send-letters-to-uber-and-lyft-over-face-tracking-ad-tablets
https://www.biometricupdate.com/202107/u-s-pols-probe-uber-lyft-about-facial-recognition-tech-facing-riders
https://www.biometricupdate.com/202106/watching-the-ride-hailing-watchers-with-computer-vision-tablets
https://www.dailymail.co.uk/sciencetech/article-9721763/Uber-Lyft-drivers-add-10-000-face-tracking-tablets-cars-gauges-riders-reactions.html
https://americanmilitarynews.com/2021/06/uber-to-track-riders-faces-in-back-of-car-with-tablet-cameras/
https://www.bloomberg.com/news/articles/2021-09-01/meme-stock-alfi-s-facial-recognition-ad-technology-fans-privacy-concerns
https://www.bnnbloomberg.ca/meme-stock-alfi-s-facial-recognition-ad-technology-fans-privacy-concerns-1.1646738
Sao Paulo METRO advertising facial biometrics
MoviePass PreShow eye tracking
Page infoType: System Published: October 2021Last updated: January 2023"
Canon smile recognition camera surveillance,"Japanese technology company Canon is using 'smile recognition' cameras to ensure that only happy employees are permitted to enter or book conference rooms in its corporate offices in China. 
The company stands accused of inappropriate surveillance and the manipulation of employee emotions.
Operator: Canon Developer: Canon Country: China Sector: Technology Purpose: Encourage workplace productivity Technology: Computer vision; Smile recognition Issue: SurveillanceTransparency:"
"Instagram offers Xanax, exctasy, opioid 'pipeline' to kids","Accounts advertising the sale to youngsters of Xanax, ecstasy, opioids and other drugs were widespread and easy to find on Instagram, according to a report by advocacy group Tech Transparency Project (TTP). 
The discovery demonstrates the ease with which the illegal sales of drugs were available to children on the platform, and shows the challenges Meta faced in enforcing its own policies.
TTP set up seven Instagram accounts appearing to belong to users between the ages of 13 to 17, and discovered the accounts were easily able to navigate to pages openly advertising the sale of illicit or pharmaceutical drugs. 
Furthermore, in some cases purported drug dealers reached out to the teenage accounts through direct messages or Instagram voice calls after they began following them.
The report also found that while Instagram bans some drug-related hashtags, other similar ones were still readily available. Meta later said it had blocked certain hashtags on Instagram and was reviewing others for potential violations of its policies. 
Operator: Meta/Instagram Developer: Meta/Instagram
Country: USA
Sector: Health
Purpose: Recommend content
Technology: Recommendation algorithm  Issue: Safety; Ethics
Transparency: Governance; Black box; Marketing"
Witcher 3 AI voice lines simulation copyright abuse,"A video game actor had his voice recreated without his permission in a mod for The Witcher 3,  prompting accusations of copyright abuse and prompting concerns amongst voice actors and others about the use of AI voice technologies to replace jobs.
GamesRadar+ reported that actor Doug Cockle's voice, which was used for the principal character Geralt, had been recreated in a mod (aka modification) for The Witcher 3 called A Night to Remember, thereby allowing the mod team to generate new dialogue in addition to reusing and remixing existing voice lines.  
It transpired that Cockle's voice had been recreated using Russian company Mind Simulation Lab's CyberVoice speech synthesis technology without his explicit permission, prompting accusations of copyright abuse. Mind Simulation Lab told Input that Cockle's parody audio would not be accessible for commercial purposes unless the actor joined the CyberVoice platform. 
However, the Russian company's claim was undermined by A Night to Remember modder nikich340 saying he had asked for specific voice lines from the lab, which Mind Simulation Lab had provided. 
Operator: Anonymous/pseudonymous; CD Projekt Red Developer: Anonymous/pseudonymous; Mind Simulation Lab Country: USA Sector: Media/entertainment/sports/arts Purpose: Simulate voice dialogue Technology: Voice synthesisIssue: Copyright; Employment - jobsTransparency: Governance"
Amazon Flex algorithm delivery driver firings,"A report from Bloomberg alleges that millions of independent contract drivers working for Amazon's Flex delivery service are being managed and fired by an algorithm with little human intervention. 
Some dismissals are considered unfair, with riders given only 10 days to appeal their terminations. If their appeal fails, they must then pay USD 200 for it to go to arbitration. 
To make matters even more challenging, riders are unable to have access to the algorithm and its decision-making processes.
In addition, notes Ars Technica, Flex drivers are complaining in forums about having their accounts terminated because their selfies failed to 'meet the requirements for the Amazon Flex program'.
Operator: Amazon Developer: Amazon Country: USA Sector: Transport/logistics Purpose: Increase efficiency Technology: Automated management system; Image recognition Issue: Fairness; Employment - pay, termination Transparency: Governance; Black box; Complaints/appeals"
Epic Systems sepsis prediction model,"An algorithm to predict whether or not patients with infections have contracted sepsis has been discovered to have missed about two-thirds of actual cases, rarely found cases medical staff did not notice, and frequently issued false alarms. 
Electronic health record company Epic Systems' Epic Sepsis Model is used by hundreds of hospitals across the US and is marketed as being 76 percent accurate. 
However, a June 2021 study published in JAMA Internal Medicine by University of Michigan researchers analysing a retrospective sample of over 27,000 adult Michigan Medicine patients concluded the algorithm is only correct 63 percent of the time, and raises many false alarms. Part of the problem, Stat News reported, is that the algorithm was trained to flag when doctors would submit bills for sepsis treatment, which doesn’t always line up with patients’ first signs of symptoms. 
In response, Epic pointed to previous research that found the model can accurately predict sepsis, and argued customers have 'complete transparency' into the model. In an accompanying editorial, medical researchers argue the findings highlight the need for the external validation of proprietary healthcare prediction models before clinical use.
An August 2023 Atrium Health research study found that Epic’s product was more accurate at the highest scoring thresholds, when it was most confident that a patient had sepsis. However, it also found that those thresholds were only reached after clinicians had already taken steps to treat the condition. 
In February 2022, Stat published the findings of a research study conducted with the Massachusetts Institute of Technology that small shifts in data fed into well-known health care algorithms, including the Epic Sepsis Model can cause their accuracy to degrade over time.
Instead of transforming care, the study found, the algorithms are unable to keep pace with fast-moving clinical conditions, potentially resulting in mis-diagnoses and raising the prospect AI could do more harm than good.
In October 2022, Epic Systems said it had overhauled its sepsis prediction model to improve its accuracy and make its alerts more meaningful to clinicians. It also said it had changed its definition of sepsis to match the international consensus definition. 
Operator: Michigan Medical School; Multiple Developer: Epic Systems Country: USA Sector: HealthPurpose: Predict sepsis infection Technology: Prediction algorithm Issue: Accuracy/reliability; Safety Transparency: Governance; Black box; Marketing - misleading"
Beijing Uyghur fake influence campaign,"A ProPublica/New York Times investigation has revealed that the Chinese government is running an elaborate influence campaign seeking to persuade people that life in Xinjiang is peaceful and happy.
The campaign is mostly in Mandarin or Uyghur and takes the form of thousands of videos, many of which follow the same or similar scripts, and often contain identical phrases.
The videos are run on Chinese media and social media sites including Pomegranate Cloud, which is owned by the official Communist Party newspaper People’s Daily. 
The clips are also being distributed on Twitter and YouTube, in some cases with English language captions. The clips do not carry logos and are not labelled as official government communication or propaganda.
In January 2021, the US government publicly declared China was committing genocide in Xinjiang.
Operator: Government of China; Pomegranate Cloud/People's Daily; Global TimesDeveloper: Government of China Country: China Sector: Govt - home/interior; Govt - foreign; Govt - security Purpose: Scare/confuse/destabilise Technology: Intelligent agents/bots; Social mediaIssue: Mis/disinformation Transparency: Governance"
Tesla China Cruise Control activation recall,"Tesla has had to recall over 285,000 Model C and Model Y cars in China after an investigation found that drivers can easily activate their cruise control system by mistake. 
The cruise control system is a key part of Tesla's Autopilot driver assistance function.
Operator: Tesla
Developer: Tesla
Country: China
Sector: Automotive
Purpose: Control speed Technology: Driver assistance system 
Issue: Safety; Accuracy/reliabilityTransparency: Black box"
ID.me unemployment benefit facial recognition,"ID.me is a Virginia-based company that works with a number of US government agencies to verify the identities of people using tax, unemployment, veterans affairs, and other services. 
The company was dragged into the spotlight during 2021 for a series of accuracy and security issues with its facial recognition system, and for misleading marketing.
According to a June 2021 VICE/Motherboard article, unemployment beneficiaries in 22 US states experienced issues with ID.me's facial recognition system that is supposed to help prevent fraud. 
The issue led to people being locked out of their unemployment accounts and not receiving the funds they are due. Many took to airing their complaints in public as they were unable to contact ID.me support staff.
ID.me CEO Blake Hall suggested to VICE that user error may have been to blame, and that the company had not been aware of 'eligible individuals' who had been unable to verify their identity.
In November 2021, the US Inland Revenue Service (IRS) announced it will require taxpayers to use a selfie to verify their identity with ID.me before using some of the agency's online services.
Others voiced their concerns that facial information collected by the IRS collects could be reused without users’ knowledge. The company's service terms enable the company to share people’s data with the police, government, and 'select partners', stoking privacy concerns.
Early 2022, security researcher Brian Krebs highlighted a number of security issues with ID.me's identity verification system. Blake Hall later admitted in a LinkedIn post that ID.me uses a one-to-many facial recognition system that searches for individuals across multiple databases.
On February 7, 2022, the IRS announced it will stop using ID.me for verification purposes. In May 2022, a group of Democratic lawmakers urged the US Federal Trade Commission to investigate ID.me, claiming (pdf) that its CEO made misleading comments about how the company uses facial recognition.
In November 2022, the US House Select Subcommittee on the Coronavirus Crisis and the Committee on Oversight and Reform  alleged (pdf) ID.me 'inaccurately overstated its capacity to conduct identity verification services to the Internal Revenue Service (IRS)'. 
The company also made 'baseless claims' of rampant COVID-19 unemployment fraud 'in an apparent attempt to increase demand for its identity verification services,' according to the Subcommittee. 
In February 2022, the Insider reported that hundreds of veterans and caregivers had been locked out of US Veterans Affairs services due to a variety of ID.me technological and customer support problems. 
An Insider public record request revealed over 700 complaints about ID.me to the Department of Veterans Affairs from October 2021 to January 2022. 
In January 2022, a coalition of digital rights groups including Fight for the Future, the Algorithmic Justice League and EPIC launched DumpID.me, a website that asks visitors to sign an online petition urging the IRS to stop using facial recognition technology.
In June 2023, New York Focus reported that complaints filed with the Federal Trade Commission and the Better Business Bureau show many applicants have had trouble using the platform’s facial recognition tool, stopping them collecting their unemployment insurance. 
Operator: ID.me; New York Department of Labor; US Inland Revenue Service (IRS); US Department of the Treasury; US Department of Veterans Affairs; US Patent and Trademark Office; US Social Security Adminstration; US Federal Energy Regulatory Commission Developer: ID.meCountry: USA Sector: Govt - welfare; Govt - taxPurpose: Verify identity; Detect fraud Technology: Facial recognition Issue: Accuracy/reliability; Privacy; Security; Bias/discrimination - race, ethnicity Transparency: Governance; Complaints/appeals; Marketing; Privacy
ASSESS DATABASE
ID.me website
ID.me Wikipedia profile
https://twitter.com/IDme/status/1405595765267959808
US IRS (2022). IRS announces transition away from use of third-party verification involving facial recognition
US IRS (2021). New online identity verification process for accessing IRS self-help tools
US House of Representatives Select Subcommittee on the Coronavirus Crisis (). Chairs Clyburn, Maloney Release Evidence Facial Recognition Company ID.me Downplayed Excessive Wait Times for Americans Seeking Unemployment Relief Funds
Ron Wyden (2022). Letter to Secretary of Labor
Dump ID.me
Algorithmic Justice League (2022).Tell the IRS: No Facial Recognition, Dump ID.Me
Joy Buolamwini, Algorithmic Justice League (2022). The IRS Should Stop Using Facial Recognition
ACLU, EPIC, et al (2022). Coalition Letter on Government Use of Facial Recognition Identify Verification Services
https://www.vice.com/en/article/5dbywn/facial-recognition-failures-are-locking-people-out-of-unemployment-systems
https://www.axios.com/deep-dive-end/unemployment-deep-dive/
https://www.denverpost.com/2021/04/25/colorado-unemployment-identity-verification-fraud/
https://gizmodo.com/issues-with-face-recognition-software-used-to-verify-un-1847135759
https://www.abcactionnews.com/news/local-news/i-team-investigates/facial-recognition-meant-to-stop-unemployment-fraud-is-blocking-legitimate-applicants
https://www.fox4now.com/rebound/id-me-speaks-out-for-the-first-time-regarding-unemployment-account-lockouts-and-wait-times
https://www.wsj.com/articles/faces-are-the-next-target-for-fraudsters-11625662828
https://www.cbsnews.com/news/irs-id-me-tax-file-2022
https://krebsonsecurity.com/2022/01/irs-will-soon-require-selfies-for-online-access/
https://www.cyberscoop.com/id-me-ceo-backtracks-on-claims-company-doesnt-use-powerful-facial-recognition-tech/
https://www.engadget.com/irs-to-end-facial-recognition-use-195637614.html
https://www.wsj.com/articles/irs-backs-away-from-facial-recognition-to-verify-taxpayers-identities-11644264843
https://www.nytimes.com/2022/02/07/us/politics/irs-idme-facial-recognition.html
https://www.vice.com/en/article/4awj7j/lawmakers-urge-ftc-to-investigate-idme-and-its-facial-recognition-tech
https://www.vice.com/en/article/4axpzg/idme-lied-about-its-facial-recognition-tech-congress-says
https://gizmodo.com/id-me-unemployment-1849793195
https://www.businessinsider.com/idme-veterans-affairs-identity-verification-va-disability-payments-emergency-assistance-2022-2
https://www.military.com/daily-news/2022/02/22/veterans-locked-out-of-disability-payments-assistance-vas-verification-service-families-say.html
Uber Real-time ID Check racial bias
Gladsaxe vulnerable children detection
Page infoType: SystemPublished: February 2022 Last updated: June 2023"
TikTok fails to stop beheading video going viral,"Footage of a girl dancing spliced with a highly graphic and disturbing video of a man being beheaded by a group of men in a bathroom went viral on TikTok.
Per Newsweek, the video had previously been present on gore sites for two years, with reports at the time of its initial discovery in 2019 alleging the victim was a 19-year-old Mexican man. 
Digital and human rights advocates argued TikTok's failure to detect and remove the video before it spread across its platform demonstrated the limitations of it's content moderation system. 
TikTok said they 'quickly' removed the offending item. However, TikTok users continued to say they were too scared to use the app lest they saw the beheading. 
Operator: ByteDance/TikTokDeveloper: ByteDance/TikTokCountry: USA Sector: Media/entertainment/sports/arts Purpose: Moderate content Technology: Content moderation system Issue: Safety Transparency: Governance; Black box"
NYPD Domain Awareness System facial recognition,"Hot on the heels of Microsoft's decision to join IBM and Amazon in temporarily barring police from using its facial recognition technologies, a group of digital and privacy activists has published (pdf) a letter requesting that Microsoft stop working with the NYPD on its Domain Awareness System (DAS). 
The Surveillance Technology Oversight Project says the system contributes to the 'warrantless spying' of citizens and 'may be even more biased, invasive, and destructive than facial recognition'. DAS 'utilizes the largest networks of cameras, license plate readers, and radiological sensors in the world' according to the NYPD.
In January 2021, Amnesty International launched 'Ban the Scan', a campaign calling on New York City authorities to halt police and government use of the technology. 
Specifically, Amnesty is encouraging NYC residents to file comments on the use of facial recognition by the NYPD under the Public Oversight of Surveillance Technologies Act. It will also help them with freedom-of-information requests.
The NYPD and Microsoft announced their DAS partnership in 2012 as an anti-terrorism measure.
Operator: New York Police Department Developer: Microsoft Country: USA Sector: Govt - policePurpose: Strengthen public safety, security Technology: Facial recognitionIssue: Surveillance; Privacy; Bias/discrimination - race, ethnicity; Dual/multi-useTransparency: Governance; Privacy"
McDonald's drive-through chatbot order taker,"McDonald's AI-powered automated ordering system (AOR) uses voice recognition and natural language processing to enable customers to place orders quickly and easily from the comfort of their cars. 
Based on technologies from voice recognition company Apprente and AI order taking outfit Dynamic Yield, the system was trialed at ten restaurants in Chicago in 2021.
According to McDonald's CEO Chris Kempczinski, the system is reputedly 85% accurate and able to take 80% of orders. 14,000 sites across the US are expected to use the technology, Kempczinski says.
Media reports indicate the company's AOR system has underperformed, with numerous TikTok users' recordings of their AI drive-thru fails going viral. One customer said the voice bot put 9 sweet teas on her order tab, when she only ordered one.
In June 2022, Restaurant Dive reported that 'McDonald’s drive-thru voice ordering accuracy rate at 24 Illinois restaurants was in the low 80% range, which is below the 95%-plus order accuracy rate the company wants before broader adoption,' citing a BITG research report.
In May 2021, McDonald's was accused of using voice recognition to identify customers in Chicago, USA, without their consent. The fast food company is being sued (pdf) by customer Shannon Carpenter for secretly using a voice recognition system when taking his drive-through order. 
The class-action lawsuit claims McDonald's is using the technology to identify repeat customers at sites across Chicago, and that it is connecting personal voice data with car license plate numbers so that customers can be more easily recognised whichever restaurant they visit.
Under Illinois' Biometric Information Privacy Act (BIPA), the collection and use of voiceprints, fingerprints, facial scans, handprints, and palm scans requires user consent.
In December 2022, McDonald's opened a new test restaurant near Fort Worth, Texas, in which all aspects of the customer experience is automated, from request to conveyer belt delivery. Only the kitchen contains human beings.
McDonald's said in a blog post that the restaurant demonstrates its commitment to 'finding new ways to serve [customers] faster and easier than ever before.' Some customers seemed to appreciate the concept, but others complained about the loss of jobs, poor pay, and lack of personal service.
Operator: McDonald's/McD Tech Labs Developer: McDonald's/McD Tech Labs Country: USA Sector: Travel/hospitality Purpose: Personalise orders Technology: Voice recognition; NLP/text analysis; Automated license plate/number recognition (ALPR/ANPR) Issue: Accuracy/reliability; Privacy; Ethics; Employment - jobs Transparency: Governance; Privacy"
CaliBurger Flippy robot employability,"USA Today reports that a robot burger-flipper at Caliburger, Pasadena, has been put on extended leave after working only a single shift due to a surge of orders after its introduction.  
Having made a big noise about its Flippy robot that it described as 'the future of food', manufacturer Miso Robotics had to pull its technology after it quickly exceeded its 2,000-burger-a-day capacity. 
According to CaliBurger CTO Anthony Lomelino, 'human staff' were part of the problem. 'Working with people, you talk to each other. With Flippy, you kind of need to work around his schedule' he says.
The incident stimulated discussion about the purpose of robots and their impact on jobs, with some commentators reckoning many jobs will disappear, and income equalities widen.
In an October 2022 interview with Reuters, Miso Robotics CEO Mike Bell said that some day, people will 'walk into a restaurant and look at a robot and say, 'Hey, remember the old days when humans used to do that kind of thing?’ 'And those days ... it's coming. ... It's just a matter of ... how quick.'
Flippy's replacement robot Flippy 2 is marketed as 'flexible and modular to meet any kitchen's needs'. 
Flippy 2 costs a minimum USD 3,000 a month.
Operator: Cali Group Developer: Miso Robotics Country: USA Sector: Travel/hospitality Purpose: Flip burgersTechnology: Robotics Issue: Accuracy/reliability; Employment - jobs Transparency: Governance; Marketing"
Instacart gig shopper robotisation,"According to Bloomberg, US gig shopping company Instacart is planning to replace its army of gig personal shoppers with robots. 
Leaked documents reveal the company aims to build a series of large-scale, automated fulfillment centers across the country in which robots would fetch items such as cereal boxes, and humans collect fresh produce and deli products. According to the documents, larger facilities would process orders for several locations while others would be attached to existing supermarkets and grocery stores.
Instacart uses hundreds of thousands of gig workers to pick up and deliver orders to customers at their homes and offices. Automation is envisaged to cut costs, increase delivery times, and attract more customers. Analysts believe the plan may help reduce the likelihood that Instacart will be squeezed out by its grocery partners.
In early 2019, Instacart changed its pay algorithm after personal shoppers boycotted the company claiming it lowered their pay and customers complained on social media their orders were being delayed.
Operator: Instacart Developer: Instacart Country: USA Sector: Transport/logistics Purpose: Increase efficiencyTechnology: Robotics Issue: Employment - jobs Transparency: Governance
ASSESS DATABASE
Instacart website, Wikipedia profile
https://www.bloomberg.com/news/articles/2021-06-01/instacart-looks-to-use-robots-over-people-to-do-grocery-shopping
https://www.dailymail.co.uk/sciencetech/article-9640895/Instacart-plans-replace-gig-shoppers-hundreds-ROBOTS-bid-slash-costs.html
https://www.foxbusiness.com/lifestyle/instacart-enlisting-robots-to-cut-labor-costs
https://nypost.com/2021/06/01/instacart-has-a-plan-to-use-robots-instead-of-shoppers/
https://www.digitalcommerce360.com/2021/06/08/instacart-wants-to-replace-army-of-gig-shoppers-with-robots/
https://www.msn.com/en-us/news/politics/instacart-eyes-robots-to-replace-many-gig-shoppers/ar-AAKBhO8
https://www.ft.com/content/364a0f74-f016-4862-9cc3-a7be58a10772
https://www.msn.com/en-us/news/technology/instacart-e2-80-99s-reported-plan-to-automate-its-workforce-seems-a-lot-like-bluster/ar-AAKBmiV
https://www.pymnts.com/pymnts-post/news/delivery/2021/instacart-seeks-partners-for-automated-fulfillment-centers/
CaliBurger Flippy robot
Denny's robot server
Page infoType: IssuePublished: December 2021Last updated: June 2023"
"Aespa virtual K-pop anthropomorphism, sexualisation","Korean entertainment company SM Entertainment has announced that Aespa, its latest K-pop girl group will include human and virtual members that can 'interact and communicate as independent beings as they have AI brains'. 
The announcement was met with a mixture of intigue and excitement, tempered by concerns about the 'ownership' of young girls by fans and the potential for dehumanisation and sexualisation. 
Others raised the possibility of Aespa avatars being manipulated for pornography and other unethical uses. 
Operator: SM EntertainmentDeveloper: SM Entertainment Country: S Korea  Sector: Media/entertainment/sports/arts Purpose: Create virtual avatarsTechnology: Deepfake - image, video Issue: Anthropomorphism; Dual/multi-use; SafetyTransparency:"
Google/HCA Healthcare patient data sharing,"The Wall Street Journal reports that Google's cloud unit and hospital chain HCA Healthcare have struck a deal that gives Google access to patient records at hundreds of hospitals and thousands of heathcare sites across the US. 
The stated aims of the deal are to increase HCA efficiencies and develop algorithms and tools to monitor patients and guide medical decision-making. Yet privacy advocates are concerned Google may use the data for other purposes, despite it having been anonymised.
US healthcare privacy laws permit hospitals to share information with suppliers, and allow researchers to analyse patient data without their permission. Healthcare companies can then use that information however they want.
Operator: HCA Healthcare; Alphabet/Google Developer: Alphabet/GoogleCountry: USA Sector: HealthPurpose: Increase operating efficiency Technology: Database Issue: Privacy; SecurityTransparency: Privacy"
Tencent/Bytedance automated link blocking,"TikTok owner Bytedance has openly accused Chinese technology giant Tencent of blocking links to its products.
In a lengthy online post (Mandarin), Bytedance says Tencent is blocking traffic from its mobile social services WeChat and QQ to its own short-form video applications Douyin, Huoshan, and Xigua. 
Bytedance says Tencent's practice of blocking links to competitor sites has been going on for years. 
Bytedance is not the only company in Tencent's sights; the technology company is also known to prohibit users from opening links to Alibaba’s Taobao and Tmall online marketplaces. 
In September 2021, China's industry ministry instructed Alibaba, Tencent, Huawei and other large internet companies to stop blocking links to each other's sites, and threatened to take action against those that failed to heed its warning. 
Operator: Tencent Developer: Tencent Country: China Sector: Technology Purpose: Block web traffic Technology: Link blockingIssue: Monopolisation Transparency: Governance; Marketing"
RCMP violated privacy using Clearview AI facial recognition,"The Royal Canadian Mounted Police (RCMP) was found guilty of breaking the law by using facial recognition to collect the personal information of Canadians without their knowledge or consent.
Canada's privacy commissioner Daniel Therrien said the RCMP was guilty of 'serious and systemic failings' in its information gathering operations using controversial facial monitoring company Clearview AI. 
Clearview AI uses artificial intelligence to match people’s images against a database of billions of photos scraped from the internet, including social media sites. 
In July 2020, during the investigation AI into whether Clearview’s AI technology breaks Canadian privacy laws, Clearview AI said it would pull out of Canada and stop working with the RCMP.
Similar to other police forces across Canada, the RCMP had previously publicly stated it was not using Clearview AI, only to later confirm that officers had used trial versions of the system without the knowledge or authorisation of police leadership. 
Operator: Royal Canadian Mounted Police (RCMP) Developer: Clearview AI Country: Canada Sector: Govt - police Purpose: Strengthen law enforcement Technology: Facial recognition Issue: Privacy; SurveillanceTransparency: Governance; Privacy; Marketing"
"Epic Deterioration Index (EDI) accuracy, bias","Physicians Vishal Khetpal, MD, and Nishant Shah, MD have published an op-ed on the Epic Deterioration Index (EDI), a machine learning algorithm developed by private electronic health record company Epic Systems that helps identify when to move a patient in or out of intensive care.
Rolled out at speed during the COVID-19 pandemic, the proprietary prediction tool has been taken up by hundreds of hospitals across the US. 
However, despite general concerns about medical system accuracy and bias, Epic is limiting expert access to raw data and equations. Nor was the system independently validated or peer-reviewed before launch.
Epic holds over 250 million patients' electronic records in the US.
Operator: Parkview Health; University of Michigan; MultipleDeveloper: Epic Systems Corporation Country: USA Sector: HealthcarePurpose: Predict patient outcomes Technology: Machine learningIssue: Accuracy/reliability; Bias/discrimination - race, gender Transparency: Governance; Black box"
US CPB covertly uses facial recognition to process asylum seekers,"The Los Angeles Times has revealed that the US government is covertly using facial recognition for immigration. According to the Times, the US government's Customs and Border Protection (CPB) is using CPB One, a mobile app incorporating facial recognition and a range of other surveillance and storage tools to process asylum seekers at the US/Mexico border. 
Civil and human rights experts are concerned about the privacy and surveillance implications of the system. The CPB also stands accused of poor transparency by failing to mention it would be using the app to process asylum seekers and that facial recognition is involved.
Operator: Customs and Border Protection (CPB)Developer: Customs and Border Protection (CPB) Country: USASector: Govt - immigration Purpose: Manage migration Technology: Facial recognition Issue: Privacy; Surveillance Transparency: Governance; Privacy"
Microsoft/Bing Tiananmen Square Tank Man 'censorship',"Bing search engine users in the UK, US, Singapore and other countries have been unable to view image search results for the query 'tank man'.
The phrase describes the lone protester standing before tanks during China's Tiananmen Square demonstrations in 1989. 
The users tried to find the image on the 32nd anniversary of the protests, prompting accusations of censorship. Microsoft has blamed the omission on 'accidental human error'.
Operator: MicrosoftDeveloper: Microsoft Country: ChinaSector: Technology Purpose: Rank content/search results Technology: Search engine algorithm Issue: Accuracy/reliability Transparency: Governance; Black box"
Kargu-2 'autonomous' drone attack,"A United Nations report suggests military drones are likely to have autonomously attacked human targets in Libya during a conflict in March 2020. 
The Kargu-2 attack quadcopter is manufactured by Turkish weapons company STM and is designed for asymmetric warfare and anti-terrorist operations.
Human rights, non-governmental organisations, including the United Nations, and technology experts have been calling for a global ban on lethal autonomous weapons systems (aka 'slaughterbots').
Turkish-made Bayraktar TB2 drones have also been used to deady effect by Ukraine's air force during Russia's invasion of the eastern European country, and by the Ethiopian government against Tigray People’s Liberation Front rebels.
Operator: Government of Libya Developer: STM Country: Libya; Turkey  Sector: Govt - defence Purpose: Kill/maim adversaries Technology: Drone; RoboticsIssue: Autonomous lethal weapons; Ethics Transparency: Governance"
Apple/SIS misidentifies 'shoplifter' Ousmane Bah,"Apple and security contractor Security Industry Specialists (SIS) were sued (pdf) for wrongfully accusing New York teenager Ousmane Bah of shoplifting multiple times at Apple stores across the US east coast. The accusation had led to Bah's arrest. 
Reports note that the suit highlighted the unreliable and biased nature of many facial recognition systems, and accused one of the two companies of deleting video images of the purported crimes, despite them surfacing during discovery. 
Furthermore, the suit alleged that a senior SIS executive denied the company ever identified Bah to Apple or to the NYPD, even though an SIS email to the NYPD suggested to the contrary. It also claimed that Apple and SIS selectively deleted video evidence that would have exposed them to potential criminal and civil liability for filing false complaints with the police.
In November 2021, Apple and Bah settled the suit on undisclosed terms.
Operator: Apple; Security Industry Specialists (SIS)Developer: Security Industry Specialists (SIS)Country: USA Sector: TechnologyPurpose: Verify identity Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race Transparency: Governance; Marketing: Privacy; Legal"
NHS patient medical history data store,"The NHS General Practice Data for Planning and Research (GPDPR) is a system that collects, stores and manages data from General Practitioners (GPs) across England. In its own words, it is 'used every day to improve health, care and services through planning and research in England, helping to find better treatments and improve patient care.' 
In May 2021, the Financial Times reported that NHS Digital was quietly planning to pool sensitive personal medical records on to a central database and make them available to third parties. Patients were given six weeks to opt out of the General Practice Data for Planning and Research (GPDPR) programme.
The news prompted an immediate and broad backlash from medical professionals, digital rights activists, and politicians accusing the government of inadequate transparency about the existence and nature of the programme, in particular about which types of organisations would have access to the data and what they would able to do with it. 
The backlash resulted the opt-out period being extended for two months. However, despite widespread concerns about poor transparency, the UK government stated it will would not write to individual patients informing them about the programme. 
Operator: National Health Service (NHS)Developer: NHS DigitalCountry: UK Sector: Govt - health Purpose: Centralise patient recordsTechnology: Database Issue: Privacy; Security Transparency: Governance; Marketing; Privacy"
BookCorpus large language dataset,"The NHS General Practice Data for Planning and Research (GPDPR) is a system that collects, stores and manages data from General Practitioners (GPs) across England. In its own words, it is 'used every day to improve health, care and services through planning and research in England, helping to find better treatments and improve patient care.' 
In May 2021, the Financial Times reported that NHS Digital was quietly planning to pool sensitive personal medical records on to a central database and make them available to third parties. Patients were given six weeks to opt out of the General Practice Data for Planning and Research (GPDPR) programme.
The news prompted an immediate and broad backlash from medical professionals, digital rights activists, and politicians accusing the government of inadequate transparency about the existence and nature of the programme, in particular about which types of organisations would have access to the data and what they would able to do with it. 
The backlash resulted the opt-out period being extended for two months. However, despite widespread concerns about poor transparency, the UK government stated it will would not write to individual patients informing them about the programme. 
Operator: National Health Service (NHS)Developer: NHS DigitalCountry: UK Sector: Govt - health Purpose: Centralise patient recordsTechnology: Database Issue: Privacy; Security Transparency: Governance; Marketing; Privacy"
Lemonade 'non-verbal cue' insurance claim assessments,"BookCorpus is a dataset that draws on 11,000+ free, unauthored books representing 16 different genres culled from Smashwords.com, a site that describes itself as 'the world’s largest distributor of indie ebooks'.
Compiled in 2014 by a group of University of Toronto and MIT researchers and funded by Google and Samsung, BookCorpus has been used to train influential large language models such as Google's BERT, Amazon's Bort, and OpenAI's GPT.
BookCorpus was withdrawn following a critical review of the dataset and its governance by Northwestern University researchers Jack Bandy and Nicholas Vincent in May 2021.
In a working paper, Bandy and Vincent set out several concerns abut BookCorpus, and called for stronger standards for the  documentation of datasets. 
Notably, they found that BookCorpus violates copyright restrictions for over 200 books that explicitly state that 'may not be reproduced, copied and distributed for commercial or non-commercial purposes.' 
The Guardian had previously noted that the developers of BookCorpus failed to make contact with or seek consent from authors whose books were included.
The reseachers also discovered that the size of the BookCorpus dataset had been inaccurately described, with several books reproduced multiple times, and that the sample is skewed to certain genres and religious forms.
Despite these concerns, BookCorpus remains widely available on data sharing websites.
Operator: Google; Amazon; OpenAI; Samsung Developer: Yukun Zhu; Ryan Kiros; Richard Zemel; Ruslan Salakhutdinov; Raquel Urtasun; Antonio Torralba; Sanja Fidler Country: CanadaSector: Technology; Research/academiaPurpose: Train language models Technology: Dataset; NLP/text analysis; Deep learning Issue: Copyright; Bias/discrimination - race, religionTransparency: Privacy; Marketing"
GPT-3 mimics QAnon,"AI large language systems such as OpenAI's GPT-3 can be used to create and deploy convincing short-form misinformation and disinformation on social media, according to a new report by researchers from Georgetown University's Center for Security and Emerging Technology (CSET).
Testing whether these kinds of models can mimic the style of the QAnon conspiracy theory, the researchers found that 'GPT-3 easily matches the style of QAnon' and 'creates its own narrative that fits within the conspiracy theory'. They go on to argue it will become increasingly difficult to distinguish reliable and fake news and information.
While OpenAI has restricted access to GPT-3, the authors argue it is only a matter of time before open source versions of GPT-3 or its equivalents will emerge, making it it easy for governments and other bad actors to weaponise them for nefarious purposes.
Operator: OpenAIDeveloper: OpenAI Country: GlobalSector: MultiplePurpose: Generate text Technology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learningIssue: Mis/disinformation; Safety; Dual/multi-useTransparency: Governance; Black box"
Beijing Uyghur emotion detection testing,"The Chinese government is testing a camera system that detects the emotions of the Uyghur people in the Xinjiang region of China. The allegations were made to the BBC on condition of anonymity by a software engineer who says he installed the system. 
'The Chinese government use Uyghurs as test subjects for various experiments just like rats are used in laboratories', the software engineer said. He went on to allege the system is used for 'pre-judgment without any credible evidence.' Most people, he says, are categorised as 'anxious' or 'scared'.
Home to 12 million ethnic minority Uyghurs, Xinjiang residents are under more or less constant surveillance. Human Rights Watch recently published a report detailing a policy of torture, disappearances, and cultural erasure in the province.
Operator: Government of China Developer: Zhejiang Dahua Technology; Hikvision Country: ChinaSector: Govt - police Purpose: Strengthen security Technology: Emotion detection Issue: Privacy; Surveillance; Accuracy/reliabilityTransparency: Governance"
Starship Technologies delivery robots,"Starship Technologies is an Estonian company that makes autonomous delivery robots. Starship launched started launching  commercial services in 2017 in the UK, USA, Germany, and other countries, often in partnership with local supermakets.  
The robots, which can travel at a max speed of 6 kilometres per hour (3.7 mph), use 10 cameras, ultrasound, radar, and GPS to travel along pavements and cross streets.
People have generally welcomed Starship Technologies' robots for their convenience, time-saving, and perceived cuteness. However, there have been instances of robots crashing, getting lost, or taking too much time to make deliveries, leading to negative feedback and, occasionally, physical backlashes.
In August 2020, the BBC reported that a Starship Technologies robot delivering groceries in Milton Keynes, UK, veered into a canal. It is not known how the robot made a mistake, or if human interference was to blame. 
In December 2022, a group of Starship robots in Cambridge, UK, were filmed being kicked by a construction worker, drawing a mixture of sympathy for the robots and concern about their impact on jobs.
The same month a Starship robot made the news by thanking a passer-by for his help after it had got stuck on an icy pavement in Cambridge.
In June 2023, Milton Keynes resident Brian Walker was 'attacked' by a Starship robot operated by the Co-op after it had collided with his dog and he had kicked it. 
Operator: Starship Technologies; Co-op; Sodexo; TescoDeveloper: Starship TechnologiesCountry: UK Sector: Transport/logistics Purpose: Deliver groceries Technology: Robotics Issue: Accuracy/reliability; Employment; Robustness, SafetyTransparency: Black box"
Google Derm Assist dermatology app racial bias,"Google has launched Derm Assist, an AI-powered app that automatically analyses skin conditions, asks questions, and suggests causes. The app is being tested in the US and has been approved for use as a medical tool in Europe. 
Google says the app can recognise 288 skin conditions; however, some doctors have expressed their concerns about the accuracy of the system, in part due poor image quality, and the potential for over-diagnosis of skin cancers.
It also appears the system was trained and tested on a dataset that underrepresents people with dark skin tones, resulting in accusations of racial bias. It also drew attention to perceived real-life Google workforce discrimination and inequality. 
Despite Google saying it would only save images to help train the Derm Assist algorithm if users gave them explicit permission to do so, concerns have also been raised about the potential for Google using personal sensitive data for other purposes.
Operator: Alphabet/Google Developer: Alphabet/Google Country: USA Sector: HealthPurpose: Identify dermatological issues Technology: Computer vision Issue: Accuracy/reliability; Bias/discrimination - race; Privacy Transparency: Governance; Privacy"
Cambodia torture victims' photo manipulation,"Manipulated photographs of Cambodian prisoners tortured by the Khmer Rouge led to a heated public backlash about the ethics of photographic touching-up using artificial intelligence. 
Irish artist Matt Loughrey had colourised black and white photographs of victims tortured by the Khmer Rouge in Phnom Penh's notorious S-21 prison, and jovialised them by adding smiles.
Published alongside an interview with Loughrey in Vice, the photographs resulted in complaints by victims' families and Cambodian authorities, accusations of re-writing history and copyright infringement, and a debate on the ethics of photographic manipulation.
Vice subsequently deleted the article and apologised, pointing out that the photographs had been 'manipulated beyond colourisation'. Loughrey had not mentioned adding smiles to the photographs.
Operator: Vice News Developer: Matt Loughrey Country: Rep. Ireland Sector: Media/entertainment/sports/arts Purpose: Colourise photographs Technology: AI colourisation Issue: Copyright; Mis/disinformation; Ethics Transparency: Governance; Marketing"
Social media LGBTQ discrimination,"Top social media sites including Facebook, Twitter, Instagram, TikTok, and YouTube are 'categorically unsafe' for LGBTQ people, according to a study by activist group GLAAD.
The sites, GLAAD warned, allow LGBTQ people to be harassed regularly, and for harmful misinformation and disinformation about LGBTQ people to spread unchecked. 
GLAAD and a team of outside experts spent several months looking at each site, their policies and track record of enforcing those policies. 
Operator: Meta/Facebook; ByteDance/TikTok; Twitter Developer: Meta/Facebook; ByteDance/TikTok; TwitterCountry: USA Sector: Media/entertainment/sports/artsPurpose: Moderate content Technology: Recommendation algorithm Issue: Bias/discrimination - LGBTQ; Safety; Mis/disinformation Transparency: Black box"
Italian car insurers discriminate using place of birth,"Car drivers in Italy were charged different insurance premiums depending on their birthplace, triggering accusations of discrimination based on national identity and the against drivers born in certain Italian cities.
A 2021 study (pdf) by researchers at the Universities of Padua, Udine, and Carnegie Mellon found that a driver born in Laos may be charged over EUR 1,000 more than a driver born in Milan with an otherwise identical profile. The study also found that birthplace and gender had a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use.
In 2012, Italy’s Institute for the Supervision of Insurance (IVASS) and National Anti-Racial Discrimination Office (UNAR) issued soft regulation to encourage car insurance companies to avoid using nationality-related factors as inputs to risk models. But the practice has been made more opaque and insidious when insurance companies started using algorithms to calculate risk premiums, according to AlgorithmWatch.
Operator: Linear; Genertel; Mps; Quixa; Con.TeDeveloper: Linear; Genertel; Mps; Quixa; Con.Te
Country: Italy
Sector: Banking/financial services
Purpose: Calculate insurance premium
Technology: Pricing algorithm Issue: Bias/discrimination - birthplace
Transparency: Governance"
Sao Paulo METRO advertising facial biometrics,
New Zealand immigration overstayer predictions,"An New Zealand government official has revealed that overstayers to the country can be fast track deported should they belong to a demographic group that have previously committed crimes or run up hospital costs. 
Immigration New Zealand compliance and investigations area manager Alistair Murray told RNZ that the country's immigration authorities are using an algorithm to predict overstaying based on age, gender, and ethnicity modelling data. 
The revelation prompted accusations of illegal racial profiling, forcing the government to admit the pilot programme had been operating clandestinely for 18 months.
The programme was later terminated and a review of government use of algorithms conducted. This stocktaking (pdf) fed into a formal Algorithm Assessment Report (pdf) by the New Zealand government that made recommendations on the development and use of algorithms, including those introduced to manage migration. 
Operator: Immigration New Zealand Developer: Immigration New Zealand Country: New Zealand Sector: Govt - immigration Purpose: Predict visa overstayers Technology: Prediction algorithm Issue: Bias/discrimination - age, gender, race, ethnicity Transparency: Governance; Black box; Marketing"
Appen recruitment skin colour assessments,"A digital media strategist applying for a job at Australian AI gig working company Appen revealed the company is asking potential employees to describe their skin tone during the online recruitment process.
Applying for a role in the US, Charné Graham was asked to select her complexion, from light to brown to black having ticked a box saying she is 'Black or African American'. Her tweet recounting the experience went viral.
Appen, which employs over one million contractors labeling photographs, text and other data, later apologised, claiming it was only an 'optional question' that is being used 'to ensure diverse datasets' and to 'take the bias out of AI'.
In July 2023, Appen scored poorly in a report from the University of Oxford’s Internet Institute into the conditions of AI gig workers. Companies that employ AI gig workers uniformly fail to meet a basic threshold of labour rights standards, the report argued.
Operator: Appen Developer: AppenCountry: USA; Australia Sector: Technology Purpose: Determine skin colour Technology: Computer vision Issue: Bias/discrimination - race; Employment - diversity Transparency: Governance"
Airbnb Smart Pricing algorithm racial bias,"The Financial Times reports that a Carnegie Mellon University study has found that Airbnb's 'Smart Pricing' algorithm is exacerbating racial inequality. 
Launched in 2015, the algorithm dynamically adjusts the cost of a night’s stay based on demand and allows hosts to set a minimum price.
However, Carnegie Mellon professor Param Vir Singh and his team of researchers discovered that White hosts appeared to prefer using the algorithm more than Black ones, thereby unintentionally widening existing real-world racial discrepancies. 
Research studies have also discovered that Airbnb secretly collects and feeds users’ personal data into an algorithm that assess whether they are trustworthy enough to make a booking.
Operator: Airbnb Developer: Airbnb Country: USA Sector: Travel/hospitality Purpose: Determine price Technology: Pricing algorithm Issue: Bias/discrimination - race Opacity: Governance; Black box"
"Twitter, Instagram 'censor' Palestinian posts","Instagram and Twitter users had their accounts closed and content and hashtags blocked when mentioning the possible eviction of Palestinians from East Jerusalem. 
During an 11-day war on the Gaza Strip in May 2021, users and journalists complained that Arabic language posts about Palestine was hit by hashtag removals and reshare blocks. Palestinian journalists also reported that their WhatsApp accounts had been blocked. Meantime, Hebrew content remained relatively unaffected.
Human and digital rights groups complained that 'discriminatory' algorithms were likely to be at work, accused the social media companies of censorship, and demanded greater transparency. Both companies later reversed course and reinstated the relevant accounts and content. Twitter blamed automated spam filtering software; Instagram put the problem down to a 'technical bug'. 
A September 2022 report by independent, non-profit organisation Business for Social Responsibility (BRC) on Meta's moderation of Arabic and Hebrew posts found that Meta had unfairly targetted Palestinian social media users.
Operator:  Developer: Meta/Instagram; Twitter Country: Palestine; Israel  Sector: PoliticsPurpose: Moderate content Technology: Content moderation system Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Freedom of expression - censorshipTransparency: Governance; Black box"
Rear seat driver abuses Tesla Autopilot,"A man has been recorded sitting in the back seat of a driverless Tesla Model 3 using its Autopilot feature while he looks out of the rear window and grins at someone in another car. 
Social media reports suggest this is not the first time the man had pulled the illegal stunt. Param Sharma, 25, was later arrested by California Highway Patrol on two counts of reckless driving and disobeying a police officer. 
The episode raises concerns about the ease with which Tesla's Autopilot system can be abused, as well as the degree of misuderstanding about its capabilities, which have been heavily hyped by Elon Musk and his company.
Operator: Param SharmaDeveloper: TeslaCountry: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: SafetyTransparency: Black box; Marketing"
China fake diplomatic Twitter influence campaign,"A seven-month investigation by the Associated Press and Oxford Internet Institute has discovered that Beijing has been running an army of fake accounts that retweet and repost Chinese diplomats and state media in order to amplify government propaganda.
The accounts, which impersonate users from the UK, Australia and other countries, are often nearly identical, created in batches, show strong signs of coordination, and do not disclose the fact that the content is government-sponsored. 
The Chinese Embassy in London did not deny the campaign's existence when asked about it. 
Operator: Government of China Developer: Meta/Facebook; Twitter Country: Global Sector: PoliticsPurpose: Increase influence Technology: Bot/intelligent agent; Social media Issue: Mis/disinformation Transparency: Governance; Marketing"
TikTok UK misuses childrens' data,"TikTok has been fined GBP 12.7 million by the UK privacy regulator for multiple failures to protect children on its platform between May 2018 and July 2020. 
The UK Information Commissioner's Office (ICO) ruled that TikTok had failed to stop approximately 1.4 million under-age children using its service in 2020, contrary to its own terms of service, none of which had gained the consent of their parents or guardians - contrary to UK law. 
The ICO also charged TikTok with not providing sufficient, easy-to-understand information about how user data is collected, used, and shared, without which children were unlikely to have been able to make informed decisions about their use of the platform.
Operator: ByteDance/TikTok Developer: ByteDance/TikTokCountry: UK Sector: Media/entertainment/sports/artsPurpose: Process personal dataTechnology:  Issue: Privacy Transparency: Governance; Privacy"
"Facebook teen alcohol, drug, gambling ad approvals","Research studies in the US and Australia show Facebook has been approving adverts targetting teen kids interested in smoking, alcohol, gambling, and extreme weight loss. 
Using a fake account, Reset Australia discovered advertisers could target teenagers on user interest areas such as gambling, smoking, alcohol and dating status.  
Using a similar process, the Tech Transparency Project found Facebook approved ads for pills, eating disorders, and dating services for kids as young as 13 years old. 
Facebook has since announced that it will no longer allow advertisers to target interest-based ads at teens. 
Operator: Meta/FacebookDeveloper: Meta/FacebookCountry: USA; Australia Sector: TechnologyPurpose: Review advertising Technology: Advertising management systemIssue: Accuracy/reliabilityTransparency: Governance; Black box"
Facebook COVID-19 misinformation ad approvals,"An investigation by Consumer Reports shows that Facebook is approving ads containing blatant misinformation about the evolving COVID-19 pandemic. 
The consumer advocacy organisation placed seven ads under a false name with no advertising history advising people to drink bleach and other false or dangerous claims, all of which were quickly approved by Facebook.
Facebook's advertising screening system is highly automated, with humans only involved in helping train the algorithms and occasionally reviewing specific ads before they run.
Consumer Reports quotes digital rights researchers and digital marketing experts saying that Facebook's advertising system is opaque, overly complex, and poorly documented and explained.
Operator: Meta/FacebookDeveloper: Meta/FacebookCountry: USA Sector: TechnologyPurpose: Review advertising Technology: Advertising management systemIssue: Accuracy/reliability; Mis/disinformationTransparency: Governance; Black box"
Dartmouth College medical school remote exam cheating,"Valley News has revealed that a number of students at Dartmouth College's Geisel School of Medicine, New Hampshire, are being investigated for allegedly cheating by using a learning management system while taking their exams remotely. 
Taking the exams involved students using one software program to take the exams and Canvas to store course materials. Dartmouth says Canvas data shows some students looked at course material during their exams.
Some students argue that their laptops, phones, and tablets regularly ping Canvas even when they are not using it. Accordingly, access was automated and random rather than intentional. 
Later, it emerged that 17 students had been charged based on the college secretly tracking what students did on Canvas during exams. Critics of Canvas say the the learning management system is unreliable and an inappropriate way to track students.
Operator: Dartmouth College Developer: CanvasCountry: USA Sector: Education Purpose: Detect and prevent cheating Technology: Learning management system Issue: Accuracy/reliability; Ethics Transparency: Governance"
Facebook 'pseudoscience' ad targeting,"An investigation by The Markup has revealed that Facebook has been allowing advertisers to run adverts targeting people interested in 'pseudoscience'. 
Mark Zuckerberg had previously stated that 'one of [his] top priorities is making sure that [Facebook users] see accurate and authoritative information across all of [Facebook's] apps'.
Facebook subsequently removed the pseudoscience interest category. The category contained more than 78 million people, according to Facebook’s ad portal.
Operator: Meta/FacebookDeveloper: Meta/FacebookCountry: USA Sector: TechnologyPurpose: Target audiences Technology: Advertising management systemIssue: Accuracy/reliability; PseudoscienceTransparency: Governance"
Walgreens fails to gain customer facial recognition consent,"US pharmacy chain Walgreens has been accused of unlawfully collecting and storing photographs of customers’ faces using cameras equipped with facial recognition technology. 
Defendant Leroy Jacobs argued in a class-action lawsuit (pdf) that Walgreens failed to disclose to customers that images of their faces were being collected and stored without their consent, despite being required to do so under the Illinois Biometric Information Privacy Act (BIPA).
Walgreens’ biometric data collection practices exposed customers to a heightened risk of identity theft and fraud given biometric identifiers cannot be changed if compromised, the suit argues. 
Operator: WalgreensDeveloper: 
Country: USA
Sector: Retail
Purpose: Identify shoplifters
Technology: Facial recognitionIssue: Privacy; Security
Transparency: Privacy"
AI Dungeon offensive speech filter,"AI Dungeon developer Latitude has come under fire for developing a content moderation system intended to stop players of its open-ended adventure game from generating stories depicting sexual encounters with minors.
An upgrade to OpenAI's GPT-3 large language model resulted in some players typing words that caused the game to generate inappropriate stories. It also appears to have prompted the AI to create child pornography of its own.
However, it quickly became clear that Latitude's new solution was blocking a wider range of content than envisaged. Gamers also complained that their private content was now being reviewed by moderators. 
Meantime, a security researcher published a report calculated that around a third of stories on AI Dungeon are sexually explicit, and one-half are assessed as NSFW. 
Operator: Latitude
Developer: Latitude; OpenAI
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Minimise sexual content
Technology: Content moderation system; NLP/text analysis 
Issue: Accuracy/reliability; Safety; Privacy Transparency: Governance"
Facebook credit card age ad targeting bias,"Four US credit card and financial loan companies have been excluding certain age groups from their advertising campaigns on Facebook, according to The Markup.
The exclusions by Aspiration, Chime, Hometap, and Varo, violate Facebook's anti-discrimination policies and in some cases potentially transgress US federal or state civil rights laws. 
The findings appear to contradict a claim by Facebook vice president Monica Bickert in an appearance before a Senate Judiciary hearing on social media in April 2021 that Facebook does not allow the use of sensitive targeting criteria when placing financial services and housing ads.
US senator Mazie Hirono used The Markup's article to accuse Bickert of making a claim that 'appears to be false'.
Operator: Meta/FacebookDeveloper: Meta/FacebookCountry: USA Sector: TechnologyPurpose: Target audiences Technology: Advertising management system Issue: Bias/discrimination - age Transparency: Governance"
YouTube ads 'inconsistent' hate speech blocklist,"The Markup reports that Google’s YouTube blocklist of hate terms is inconsistent and highly permeable, allowing advertisers to target people in terms such as 'white lives matter' and 'white power' but blocking them from running ads against terms such as 'Black Lives Matter. 
Google says it has now blocked additional terms associated with hate speech from being used as ad keywords on YouTube videos. 
The company says it does not publicly state how it develops its enforcement tools so that bad actors are less likely to game its system.
Operator: Alphabet/Google/YouTubeDeveloper: Alphabet/Google/YouTubeCountry: USA Sector: TechnologyPurpose: Identify & block offensive ads Technology: Advertising management system Issue: Bias/discrimination - race; Accuracy/reliability Transparency: Governance"
CBSE student facial 'matching' opacity,"India's Central Board of Secondary Education (CBSE) has introduced a facial recognition system so that students can access and download academic documents stored on DigiLocker - an Aadhaar-based cloud-based locker - without using ADHAAR and their phone numbers.
To access the documents, students have to verify their identities via 'Facial Recognition System, a so-called 'state of the art' facial recognition system provided by India's National e-Governance Division, that matches a human face from a facial dataset stored on a CBSE database.
The CBSE says it expects the system will 'immensely' help foreign students and those who are unable to use or open DigiLocker should they not have an Adhaar Card or use the wrong mobile number. 
However, as noted by MediaNama, the system did not have a privacy policy at launch. The CBSE later said its facial recognition system does not have a privacy policy because it is a 'simple face matching process', something confirmed in its response to a digital rights group Internet Freedom Foundation Right to Information (RTI) request.
This does not explain why the technology, which the CBSE refuses to provide information about, is described as a facial recognition system. The Internet Freedom Foundation describes the CBSE's 'face matching technology is just facial recognition in disguise.'
Operator: Central Board of Secondary Education (CBSE) Developer: National e-Governance Division Country: India Sector: Education Purpose: Access documents Technology: Facial recognition Issue: Accuracy/reliability; Privacy; Security Transparency: Governance; Marketing; Privacy"
UK Post Office Horizon payment system,"Horizon is a controversial software accounting system developed by Fujitsu and used by the UK Post Office since 1999. The use of Horizon resulted in what many people consider the UK's largest miscarriage of justice with sub-postmasters unfairly accused, convicted and imprisoned on charges of theft, false accounting.
In December 2019, UK High Court judge Peter Fraser ruled (pdf) Horizon was not 'remotely robust', contained 'bugs, errors and defects', and constituted a 'material risk' that shortfalls in branch accounts were caused by the system. The ruling saw the Post Office agreeing to settle with 555 claimants, having accepted that it had previously 'got things wrong in [its] dealings with a number of postmasters'. It was to pay GBP 58m in damages.
In February 2020, the UK government announced it would hold an official independent inquiry into the scandal, upgraded in June 2021 to a public inquiry. The government later offered Post Office workers with wrongful convictions for theft and false accounting GBP 600,000 each in compensation, and in January 2024 it said it would introduce legislation to exonerate wrongfully convicted. 
Plagued by poor transparency by the Post Office, Fujitsu and their advisors, the long-running saga has involved the obfuscation, blocking and delayal of mediation and litigation procedures, amongst other legal and reputational defensive tactics.
Operator: Post Office Developer: Fujitsu/ICL
Country: UK
Sector: Govt - retail
Purpose: Make benefits payments; Reduce fraud
Technology: Database
Issue: Accuracy/reliability; Ethics; Employment - jobs, pay Transparency: Governance; Legal - mediation, litigation"
IRCC immigration & visa applications AI screening,"Immigration, Refugees and Citizenship Canada (IRCC) has been running an 'advanced data analytics' and artificial intelligence (AI) pilot since 2018 to process temporary resident visa (TRVs) applications submitted from China and India. The system was extended to TRV applications submitted from all countries outside Canada in January 2022.
Visa applications are assessed on the basis of eligibility and admissibility. For straightforward applications, eligibility is approved solely by the model, while eligibility for more complex applications is decided upon by an immigration officer. All applications are reviewed by an immigration officer for admissibility.
A September 2018 report (pdf) by University of Toronto's Citizen Lab concluded the IRCC's pilot threatens to violate domestic and international human rights law in the form of bias, discrimination, privacy breaches, due process, and procedural fairness.
The report cautions that experimenting with these technologies in the immigration and refugee system amounts to a 'high-risk laboratory,' as many of these applications come from some of the world’s most vulnerable people, including those fleeing persecution and war zones. 
Citizen Lab called on the national authorities to freeze the development of AI-based systems until a government standard and oversight bodies are established. 
In January 2022, the IRCC published an algorithmic impact assessment (AIA) of its temporary resident visa application assessment system, concluding that the impact level of the system is moderate. 
However, the assessment has been described as 'noticeably short and trite in detail', noticeably regarding how its scoring system works, the identity and degree of involvement of external stakeholders, and transparency.
The IRCC's TRV application assessment system has been shrouded in secrecy since its inception. 
During the Citizen Lab report’s research phase, 27 distinct official information requests were submitted to the Government of Canada. Every one of them remained unanswered.
And while the publication of the IRCC's algorithmic impact assessment provided further information about the system, many questions were limited to 'yes' or 'no' answers.
Operator: Immigration, Refugees and Citizenship Canada (IRCC) Developer: Immigration, Refugees and Citizenship Canada (IRCC) Country: Canada Sector: Govt - immigration Purpose: Process temporary resident visa applications Technology: Data analytics; Machine learningIssue: Privacy; Bias/discrimination - gender, race; FairnessTransparency: Governance; Black box; Marketing"
RCMP British Colombia facial recognition procurement opacity,"The Royal Canadian Mounted Police of British Columbia has secretly subscribed to a facial recognition service that claims to help identify terrorists. 
Signed in 2016, the deal enabled the RCMP to access a 700,000-image database created by US-based IntelCenter using facial recognition developed by Morpho. Morpho was later bought by and renamed as IDEMIA. 
The database appeared to be at least partially made up of images scraped from social media platforms, according to Kate Robertson of the University of Toronto’s Citizen Lab - similar to Clearview AI, also covertly used by the RCMP.
In addition to hiding the deal from the public, emails obtained by The Tyee show the RCMP broke its own rules by not alerting senior officers of the sole-sourced contract, and that it avoided labelling the software with terms that might trigger more oversight, such as ‘facial recognition’ or ‘biometric.’ 
Privacy and civil rights experts are concerned facial recognition systems can produce false positives with particular bias against racialised individuals and can unlawfully incriminate Canadians and foreigners. It is unclear how the images are harvested, raising a range of concerns about privacy and accuracy, they say.
The RCMP in British Colombia told The Tyee it bought the software to test its feasibility. The contract came to an end in 2019. 
Operator: Royal Canadian Mounted Police (RCMP)Developer: IntelCenter; IDEMIA/Morpho
Country: Canada
Sector: Govt - police
Purpose: Strengthen law enforcement
Technology: Facial recognitionIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Ethics; Privacy
Transparency: Governance; Marketing"
Ningbo real estate facial recognition misuse,"A regulator in the city of Ningbo, in China's eastern Zhejiang province, has fined three real estate companies for 'illegally acquiring customers’ facial information', according to the South China Morning Post.
The three firms - China Poly Group, Sunac China Holdings, and Greenland Holdings - were fined 250,000 Yuan (USD 38,500) for violating China's consumer protection law by installing facial recognition devices at sales offices to identify customers, without informing them or obtaining their consent.
Per the SCMP, residential projects that don’t sell well in China often offer a discount to property agencies to get more buyers through the door. Capturing visitor’s facial information enables sales offices to determine who was returning with an agent, even if they are wearing a face mask.
China’s annual Consumer Protection Gala had earlier singled out international companies, including BMW, Kohler, and MaxMara - for the misuse of facial recognition.
Operator: China Poly Group; Sunac China Holdings; Greenland Holdings Developer: Unclear/unknown
Country: China
Sector: Real estate
Purpose: Identify customer identity
Technology: Facial recognition Issue: Privacy; Dual/multi-use; Surveillance
Transparency: Governance; Privacy"
Berlin Südkreuz rail station algorithmic surveillance,"Berlin Südkreuz is a rail station and transport interchange junction in southern Berlin that has been used by a number of recent German governments as a laboratory for live biometric and other forms of surveillance.
In August 2017, Germany's Ministry of the Interior started a six-month pilot, later extended to twelve months, to assess the facial recognition capabilities of three systems tracking 312 volunteers wearing transponders and who were added to a special police database.
The test elicited a strongly critical reaction from civil and privacy rights groups for its potential impact on privacy and other fundamental rights. It was also criticised for the apparently low quality of its outputs. 
Interim results of the pilot published in December 2017 indicated that 84.7% of people were correctly identified by the three systems, a figure contested by activists. The test was described by Florian Gallwitz, a facial recognition expert at the Nuremberg Institute of Technology, as 'a clear failure'.
In June 2019, the authorities started a pilot at Berlin-Südkreuz to test algorithms supplied by IBM, Hitachi, Funkwerk and G2K Group to detect suspicious behaviour focused on six scenarios, including unattended luggage, acts of violence, people lying down or entering blocked areas such as construction sites. 
The project again involved volunteers who were asked to do things to attract the attention of the systems. The pilot provoked another round of negative media coverage.
Operator: Bundespolizei (BPOL); Deutsche BahnDeveloper: Dell/Herta Security; AnyVision; IDEMIA; IBM; Hitachi; Funkwerk; G2K Group
Country: Germany
Sector: Govt - transport
Purpose: Strengthen law enforcement 
Technology: Behavioural analysis; CCTV; Computer vision; Facial recognition; Object recognition; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Privacy; Surveillance
Transparency: Governance; Marketing"
Suresnes 'abnormal situation' surveillance,"In March 2021, Guillaume Boudy, mayor of Suresnes, a commune in the western suburbs of Paris, announced it would start using 10 AI-equipped CCTV cameras to strengthen security and safety of its inhabitants and the cleanliness of its environment by spotting 'abnormal events' ('événements anormaux').
The eighteen-month test would be operated by XXII Group, a local video analysis platform provider which had been given the right to use the cameras to train its facial analysis and other relevant algorithms.
The decision was criticised by civil rights and privacy organisations such as Le Quadrature du Net and TechnoPolice as opaque and potentially intrusive, and that it constituted an inappropriate and unethical commercialisation of public space and personal privacy.
Concerns were also raised about the efficacy of the system, and how false positives would be handled.
Operator: Ville de Suresnes Developer: XXII Group
Country: France
Sector: Govt - municipal; Govt - police
Purpose: Strengthen law enforcement
Technology: Facial analysis Issue: Accuracy/reliability; Dual/multi-use; Privacy; Surveillance
Transparency: Governance; Privacy"
UK passport application photo 'racism',"The UK Home Office stands accused of implicit racism after a Black man was prevented from registering for a UK passport on its website because the software was unable to recognise his skin colour from the grey wall behind him.
Model and racial justice activist Joris Lechêne took to TikTok to complain that his photo 'was rejected because the artificial intelligence software wasn’t designed with people of my phenotype in mind.'
The UK's Passport Office had earlier told the New Scientist that an update to the system had been available for more than a year but had not yet been rolled out.
Furthermore, Home Office documents released under a Freedom of Information (FOI) request show it knew its passport photo system failed to work well for some ethnic minority people but decided to use it anyway. 
Operator: UK Home OfficeDeveloper: 
Country: UK
Sector: Govt - immigration
Purpose: Check photograph
Technology: Facial detection; Facial analysis; Computer vision Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity
Transparency: Governance; Complaints/appeals"
NDIS independent assessments 'robo-planning',"The National Disability Insurance Scheme (NDIS) is Australia's system for publicly supporting people with disabilities. Legally created in 2013, it was launched in 2020 under the auspices of the National Disability Insurance Agency (NDIA).
In 2021, the Australian government proposed introducing independent assessments for NDIS participants over the age of 7. Assessors had to be qualified independent health professionals making assessments of 1 to 4 hours using standardised assessment tools that would feed into an algorithm that decides a 'personalised budget' for each applicant.
The Australian government had argued the scheme would reduce inequality and improve the consistency of decision-making. But it quickly ran into technical and political headwinds, with disability groups saying they had not been properly consulted.
Bruce Bonyhady, the inaugural chairman of the NDIA and an original architect of the scheme, slammed (pdf) the policy as 'robo-planning' built on insufficient evidence the new tools adequately assess disability and which 'puts people in boxes before they have had a chance to outline what they would like to achieve or the ways in which they hope their lives change.'
Furthermore, the Australian government claimed publicly the scheme was not a cost-cutting measure, but leaked documents revealed it would deliver AUD 700 million reduction in funds allocated for disability support. 
In July 2021, NDIS Minister Linda Reynolds said the federal government would not push ahead with the proposal.
Operator: National Disability Insurance Agency (NDIA) Developer: National Disability Insurance Agency (NDIA)
Country: Australia
Sector: Govt - welfare
Purpose: Assess disability funding eligibility
Technology:  Issue: Bias/discrimination - income; Dual/multi-use
Transparency: Governance: Black box"
Amazon Alexa reinforces female stereotyping,"The use of female voices on voice-powered virtual assistants, including Amazon's Alexa, Apple's Siri, and Google Assistant, portray women as 'obliging, docile and eager-to-please helpers', reinforcing gender stereotypes of women, according to a UNESCO report.
Responses from AI assistants such as Alexa to verbal sexual harassment tend to be 'deflecting, lacklustre or apologetic', the report said, and appeared to show a greater tolerance towards sexual advances from men than from women.
The authors made a series of recommendations on how technology companies can make their virtual assistants less biased, notably by hiring more female programmers and by installing gender-neutral voices rather than making their assistants female by default.
Operator:  Developer: AmazonCountry: Global Sector: Media/entertainment/sports/arts Purpose: Provide information, services Technology: NLP/text analysis; Natural language understanding (NLU); Speech recognition Issue: Bias/discrimination - gender Transparency: Governance"
Facebook blocks #resignmodi hashtag,"In April 2021, Buzzfeed reported that Facebook had temporarily hidden posts calling for the resignation of Indian Prime Minister Narendra Modi. Posts with the hashtag #ResignModi had been labelled 'temporarily hidden' because 'some content in those posts goes against our Community Standards.'
The Prime Minister and his government had been widely criticised for the quality of its response to the COVID-19 pandemic. Tens of thousands of Indians had died, many of them unnecessarily, it is thought. 
However, the platform had failed to disclose which Community Standards may have been breached, despite Facebook's stated commitment to transparency. Facebook subsequently reversed its decision shortly after Buzzfeed had published its story. 
Facebook spokesperson Andy Stone told BuzzFeed 'We temporarily blocked this hashtag by mistake, not because the Indian government asked us to, and have since restored it.'
A few days before, Modi's government had ordered Twitter to block over 50 tweets that criticised how was handling the pandemic - a request to which Twitter complied, leading to accusations of censorship and criticism of the platform.
Operator: Meta/Facebook Developer: Meta/Facebook
Country: India
Sector: Politics; Health
Purpose: Moderate content
Technology: Content moderation system Issue: Freedom of expression - censorship
Transparency: Governance; Black box"
Indian government censors COVID-19 Twitter posts,"In late April 2021, the Indian government quietly ordered social media platform Twitter to remove posts spreading 'fake or misleading information' to 'create panic about the Covid19 situation' in the country. The move prompted lawmakers and human rights activists to accuse the government of censorship and further putting peoples' health at risk. 
The discovery that the government had made an emergency order to censor the tweets was revealed on Lumen, which publishes details of legal takedown notices social media companies and others receive from governments and private entities across the world. The order was first revealed by MediaNama.
Local reports indicated the banned material includes a tweet from Pawan Khera, a spokesman for India’s main opposition party, the Indian National Congress (INC), who accused Modi of holding political rallies while the virus raged and failing to acknowledge that they likely contributed directly to the spread of COVID-19.
Others on the list included posts by INC parliamentarian Revanth Reddy, West Bengal state minister Moloy Ghatak, and filmmakers Vinod Kapri and Avinash Das. A number of tweets castigated Modi for failing to fix India's healthcare system, which has run out of beds, oxygen and medicines, leading to a humanitarian disaster.
A Twitter spokesperson told DW, 'When we receive a valid legal request, we review it under both the Twitter Rules and local law. If the content violates Twitter's Rules, the content will be removed from the service. If it is determined to be illegal in a particular jurisdiction, but not in violation of the Twitter Rules, we may withhold access to the content in India only.' 
A few days later, Facebook was discovered to have been temporarily hiding posts calling for the resignation of Indian Prime Minister Narendra Modi.
Operator: Twitter Developer: Twitter
Country: India
Sector: Politics; Health
Purpose: Moderate content
Technology: Content moderation system Issue: Freedom of expression - censorship
Transparency: Governance; Black box"
Leonid Volkov' deepfake video calls,"Senior EU and UK politicians and officials were targeted by a campaign in which bad actors appeared to use deepfake technology whilst pretending to be Russian opposition leaders. It later emerged that the incident was a prank, with no effects - deepfake or otherwise - used.
The attack took the form of video calls involving an imposter claiming to be Leonid Volkov, chief of staff to Alexy Navalny, in which the Russia's annexation of Crimea was discussed. The identity and motivation of the actors remains unclear, though Volkov told The Guardian that it may have been an attempt to discredit Russian opposition head Navalny.
Among those targeted included Tom Tugendhat, then chair of the UK's foreign affairs select committee, chairman of the foreign affairs committee of the Estonian parliament Marko Mihkelson, Rihards Kols, chairman of the Latvian parliament's foreign affairs committee, and the Dutch parliament's foreign affairs committee.
Operator: Government of RussiaDeveloper: Government of Russia
Country: Estonia; Latvia; Lithuania; Netherlands; UK
Sector: Politics
Purpose: Damage reputation
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation; Ethics
Transparency: Governance; Marketing"
TUI airline classifies women as children,"A software upgrade by TUI Airways which classified female passengers whose title was 'Miss' as children led to a 'serious incident' on a Birmingham-Majorca flight in July 2020.
The mis-classification meant that the average weight of these female passengers used for take-off calculations for the Boeing 737 jet was 1,244 kg lighter than it actually was, potentially having an impact on take-off thrust.
An investigation (pdf) into the incident by the UK government’s Air Accidents Investigation Branch (AAIB) said the problem occurred after the flight reservation sheet had been updated while the airline was grounded due to COVID-19. 
According to the BBC, safety officials said the problem was that the software had been programmed in a foreign country where 'Miss' is used to refer to children, and 'Ms' to adult women. 
The flight proved safe and the fault identified in TUI's IT system was later corrected. TUI also introduced manual checks to ensure adult females were referred to as Ms on relevant documentation.
Operator: TUI Group/TUI Airways Developer: TUI Group/TUI Airways; Boeing
Country: UK
Sector: Transport/logistics
Purpose: Calculate airline weight
Technology: IT system Issue: Accuracy/reliability; Safety
Transparency:"
SARI live facial recognition,"SARI ('Sistema Automatico di Riconoscimento Immagini', or Automated System for Image Recognition') is a facial recognition system that enables police forces and sports authorities to identify criminals and suspected criminals. 
Piloted for eight months from 2017 by Italy's national police and introduced in July 2018, the system is based on a database of 16 million mugshots, nine million of which belong to people who have been identified by the police once, while the other seven million are of individuals who have been stopped repeatedly.
SARI has been helpful in the arrests of shoplifters and burglars across Italy. For example, in 2018, it correctly identified two burglars in Brescia. But it also raised concerns about its accuracy and its impact on privacy and legality, largely due to the seemingly high number of mugshots in the police database, which was many more than the police's fingerprint identification system.
SARI Real-time is a facial recognition system that uses cameras installed in a particular geographical area and capable of scanning individuals’ faces in real-time. These images are then compared with a government watch-list database of up to 10,000 faces. The database is available to law enforcement upon request.
In addition to concerns about inaccuracies, lawyer Tommaso Scannicchio told ZDNet how real-time systems make people at public events feel under surveillance, causing a 'chilling effect'. 
In April 2021, Italy's privacy regulator the Garante declared SARI Real-time illegal on the basis that it lacked a legal basis for the automated processing of biometric data for facial recognition, and that it would effectively establish an 'indiscriminate mass surveillance system.' 
Operator: Polizia di Stato; Udinese Calcio; S.S.C. Napoli Developer: SARI Enterprise; Reco 3.26
Country: Italy
Sector: Govt - police; Govt - immigration; Media/entertainment/sports/arts
Purpose: Strengthen law enforcement
Technology: Facial recognition Issue: Accuracy/reliability; Dual/multi-use; Privacy; Surveillance
Transparency: Governance; Legal; Marketing; Privacy
SARI, Soluzione Reco per il riconoscimento dei volti
Polizia Del Stato (2018): Brescia: ladri d'appartamento scoperti grazie al riconoscimento facciale
Reclaim your Face campaign
EDRI (2021). Chilling use of face recognition at Italian borders shows why we must ban biometric mass surveillance
IPRI Media (2021). Lo scontro Viminale-Garante della privacy sul riconoscimento facciale in tempo reale
Il Garante per la Protezione dei Dati Personali (2021). Opinion on Sari Real Time System [9575877]
Il Garante per la Protezione dei Dati Personali (2018). Automatic face identity search system [9040256]
Frederico D., (2018). Question to Written Answer
https://www.zdnet.com/article/face-recognition-are-italys-police-using-millions-more-mugshots-than-is-legal/
https://www.biometricupdate.com/202104/real-time-facial-recognition-system-deployment-blocked-by-italian-data-protection-authority
https://www.pogowasright.org/it-facial-recognition-sari-real-time-does-not-comply-with-the-privacy-policy/
https://iapp.org/news/a/italian-dpa-does-not-favor-use-of-sari-real-time-system/
https://algorithmwatch.org/en/italy-stadium-face-recognition/
https://www.wired.it/attualita/tech/2019/04/03/sari-riconoscimento-facciale-stranieri/
https://quifinanza.it/innovazione/video/polizia-sari-riconoscimento-volti/222563/
https://www.vice.com/it/article/g5p83w/riconoscimento-facciale-in-italia
https://tg24.sky.it/tecnologia/now/2020/02/13/sari-riconoscimento-facciale-polizia-italiana
https://www.toptrade.it/uncategorized/riconoscimento-facciale-un-limite-alla-privacy
UK Met Police retrospective facial recognition
Deliveroo Italy rider shift management algorithm
Page infoType: SystemPublished: February 2023"
UK govt Spotlight fund application assessments,"Spotlight is a software tool developed by the UK Cabinet Office to help government departments and local authorities conduct due diligence on organisations applying for public funds, and to identify those that may require further investigation.
Spotlight draws on a variety of data sources to sort applicants into a red, amber or green category based on it's assessment of the risk they present.
According to the UK government, 'Spotlight speeds up initial pre-award checks by processing thousands of applications in minutes replacing manual analysis that, typically, can take at least two hours per application.'
In November 2020, Arts Professional reported that Arts Council England had been using Spotlight to assess applications of up to GBP 3 million for the UK's GBP 1.57 billion COVID-19 Culture Recovery Fund. 
Following earlier unease about how recipients had been chosen, the article prompted journalists to accuse the UK government of 'a total lack of transparency' surrounding the process for awarding emergency arts funds.  
Others asked why many struggling arts organisations had been turned down whilst others with deep pockets had been given even more money, leaving the government open to accusations of 'cultural elitism' and political bias.
Spotlight stands accused of helping approve 'dozens of companies' selected by the Department for Work and Pensions (DWP) to become 'Kickstart gateways' had little to no trading history, or were based outside of the UK, according to a February 2023 FE Week investigation.
The investigation raised questions about the quality of gateway providers, as well as about Spotlight's effectiveness. 
The UK government's Kickstart scheme was a new employer initiative announced in summer 2020 aimed at creating six-month paid work placements for young people who were at risk of long-term unemployment. The scheme closed in January 2023.
A Kickstart 'gateway' was a type of organisation, such as a local authority, charity or trade body that would act as an intermediary and apply for funding on behalf of companies wishing to create fewer than 30 job placements.
Operator: Arts Council England (ACE); UK Cabinet OfficeDeveloper: UK Cabinet Office
Country: UK
Sector: Govt - culture; Govt - employment
Purpose: Assess public funds applications
Technology: Automated risk assessmentIssue: Accuracy/reliability; Bias/discrimination - political
Transparency: Governance"
Tesla Autopilot tricked into driverless driving,"Consumer Reports (CR) has easily tricked a Tesla into driving in the car’s Autopilot mode with no one at the wheel. 
A CR tester switched the Tesla Model Y to autopilot while it was on the track, and set the speed dial to zero miles per hour so the vehicle came to a stop. 
The driver then placed a weighted chain on the wheel to trick the car into believing a person’s hand was there and moved to the passenger’s seat. He turned the speed dial back up when he was in the passenger seat and the car started driving.
Jake Fisher, Consumer Reports’ senior director of auto testing, said, 'In our evaluation, the system not only failed to make sure the driver was paying attention, but it also couldn’t tell if there was a driver there at all.'
The test came a few days after a Tesla crashed in Texas, killing the two men in the car. Neither of the men were in the driver’s seat at the time of the crash, according to authorities.
Operator: Consumer Reports Developer: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; SafetyTransparency: Black box"
AI satellite falsefication/location spoofing,"A University of Washington research study has shown the ease with which deepfake satellite images can be created and used to create misinformation and disinformation.
The aim, author Bo Zhao told The Verge 'is to demystify the function of absolute reliability of satellite images and to raise public awareness of the potential influence of deep fake geography.'
As part of their study, Zhao and his colleagues created software to generate deepfake satellite images, using generative adversarial networks ('GANs'), and then created detection software that was able to spot the fakes based on characteristics like texture, contrast, and colour.
Per PetaPixel, the authors simulated their own deepfakes using Tacoma, Washington as a base map and placed onto it features extracted from Seattle, Washington and Beijing, China. The high rises from Beijing cast shadows in the fake satellite image while the low-rise buildings and greenery were superimposed from the urban landscape found in Seattle.
The authors warn false satellite images could be used to create hoaxes about natural disasters, support disinformation, or mislead geo-political foes.
Operator:  Developer: Zhao, B., Zhang, Z., Xu, C., Sun, Y., Deng, C.
Country: USA
Sector: Technology
Purpose: Scare/confuse/destabilise
Technology: Deepfake - image, video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation; Dual/multi-use
Transparency:"
Designers sue Shein for using AI to recreate their work,"Three designers sued fast fashion company Shein in the US for allegedly stealing their work using artificial intelligence. 
Krista Perry, Larissa Martinez and Jay Baron accused (pdf) Shein of contravening the US' 1970 Racketeer Influenced and Corrupt Organizations Act (RICO) by stealing their own and other independent artists’ works 'over and over again, as part of a long and continuous pattern of racketeering.'
According to the claimants, Shein's 'design ‘algorithm’ could not work without generating the kinds of exact copies that can greatly damage an independent designer’s career—especially because Shein’s artificial intelligence is smart enough to misappropriate the pieces with the greatest commercial potential.'
Operator: Krista Perry, Larissa Martinez, Jay Baron Developer: Shein Country: USA Sector: RetailPurpose: Identify and copy trending art Technology:  Issue: Copyright; Employment Transparency: Governance"
BDD100K driving video dataset,"BDD100K is a driving video dataset that comprises 100,000 videos collected across the US using vehicle mounted cameras. 
The footage covers different geographic, environmental, and weather conditions, including sunny, overcast, and rainy, and different times of the day and night, and come with GPS/IMU information recorded by cell-phones to show rough driving trajectories. 
Described by UC Berkeley, which created the dataset, as 'the largest and most diverse open driving video dataset so far for computer vision research,' it is intended to help make self-driving safer.
By hiring people to manually apply labels according to skin colour based on the Fitzpatrick scale, a scale commonly used to classify human skin colour, a 2019 study (pdf) by Georgia Institute of Technology researchers found that BDD100K is, on average, 4.8 percent more accurate at correctly spotting light-skinned pedestrians, and up to 12 per cent worse at spotting people with darker skin.
The researchers noted that the bias 'is not specific to a particular model' and therefore likely to be prominent throughout a variety of facial recognition technology.
Operator:  Developer: UC Berkeley Country: USASector: Automotive Purpose: Train self-driving car systems Technology: Database/dataset; Facial recognition; Object recognition Issue: Accuracy/reliability; Bias/discrimination - race, gender Transparency:"
Aadhaar facial recognition may marginalise vulnerable people,"Plans by the Indian government to use facial recognition integrated within the country's Aadhaar biometric ID system has raised fears that millions of vulnerable people without mobile phones or internet access will lose out on receiving their COVID-19 vaccinations.
According to Reuters, a facial recognition system based on the Aadhaar ID is being tested in the eastern state of Jharkhand as a 'touchless' vaccination process and hence better way of avoiding infection. Should the pilot prove successful, the system may replace fingerprint or iris scans at COVID-19 vaccination centres across the country.
RS Sharma, head of India's National Health Authority, was quoted as saying the system would not be mandatory, but new guidelines indicate that Aadhaar is already the 'preferred' mode of identity verification and for vaccination certificates.
Anushka Jain, associate counsel at the Internet Freedom Foundation (IFF), told Reuters that using facial recognition at vaccine centres risks further marginalising vulnerable people who may be misidentified and refused the vaccine, and raises fears the controversial technology could become the norm at all centres.
The IFF filed Right to Information requests demanding to know the specific purposes for which facial recognition technology will be used, the legislation which authorises them to do so, and the total expenditure incurred to procure such technology. The IFF also sought to know the list of persons authorised to access the technology, the software and hardware being used, and details of the database. 
Operator: Aadhaar Developer: Unique Identification Authority of India (UIDAI); National Health Authority (NHA) 
Country: India
Sector: Govt - health
Purpose: Verify identity
Technology: Facial recognitionIssue: Accuracy/reliability; Bias/discrimination - economic; Security; Privacy
Transparency: Governance; Black box"
Tesla Model S kills truck driver standing on road,"A Tesla Model S has hit a truck driver who had stopped on a road in Arendal, Norway. The truck driver, who was standing on the road beside his vehicle, died instantly in the accident. 
The accident was investigated by the police and the Accident Investigation Board Norway, with the former deciding to prosecute the driver for negligent homicide.
The accused Tesla driver claimed in court that he was alert and that the car's Autopilot driver assistance system was turned on. 
Earlier, Tesla Norway had received an apology from the police after the electric car maker was wrongfully accused of being uncooperative during the investigation. 
Operator:  Developer: Tesla Country: Norway Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Safety; Accuracy/reliability Transparency: Black bo"
Tesla Model X kills pedestrian outside Tokyo,"Yoshihiro Umeda, a 44 year-old husband and father, died in April 2018 when a Tesla Model X with Autopilot turned on crashed into a group gathered at the site of an earlier motorcycle accident outside Tokyo.
A car in front of the Tesla changed lanes to avoid the group of bikers, but the Tesla driver was reputedly dozing and Autopilot failed to change lanes and accelerated until it hit the group.
Umeda's relatives filed (pdf) a lawsuit against Tesla in California's Northern District Court claiming that the company's Autopilot system is 'defective and incapable of handling common driving scenarios' and that its system for detecting drivers who aren't paying attention is 'fatally defective'. 
The judge sided with Tesla’s motion to dismiss, suggesting Japan is the most appropriate venue for the case based on 'forum non conveniens.' Forum non conveniens says that a US court can dismiss a case when another court may be better suited to hear it. 
The case has yet to to filed in Japan. The Tesla driver was officially sentenced to three years in prison and up to five years of suspension. 
Operator:  Developer: Tesla Country: Japan Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
Home Office sham marriage algorithm,"The UK Home Office is reported as having been using an algorithmic system to detect 'sham marriages' in which couples get married to avoid immigration law rather than because they have a genuine relationship. 
According to the The Bureau of Investigative Journalism (TBIJ), the Home Office introduced an automated system to detect sham marriages in March 2015 as part of a 'hostile environment' immigration policy. The current machine learning-based triage system was introduced in April 2019.
The system makes an initial assessment over whether their partnerships are genuine or designed to get around visa rules by dividing couples into 'red' and 'green' light categories. A red light referral can put a person’s immigration and visa status at risk, and may lead to legal action and deportation.
Lawyers and civil rights advocates warn the system is opaque, unaccountable, may discriminate according to nationality and the age gap between partners, and may be flawed and unfair due to an over-reliance on automated decision-making.
The TBIJ discovered in April 2021 that an equality impact assessment (EIA) conducted by the Home Office revealed issues with the 'triage' process, including the possibility of 'indirect discrimination' due to potentially biased information that also includes the age gap between partners.
Legal charity Public Law Project (PLP) had previously reported that Bulgarians, Greeks, Romanians and Albanians have been more likely to have their marriages rated ‘Red’ than others, and are therefore more likely to be targeted for investigation. 
According to Joint Council for the Welfare of Immigrants legal policy director Chai Patel, 'Home Office data on past enforcement is likely to be biased because Home Office enforcement is biased against people of certain nationalities.'
The existence of the algorithm was revealed by documents obtained by the Public Law Project through a Freedom of Information request. The eight criteria taken into account by the system when a marriage is flagged for analysis revealed by the EIA uncovered by the TBIJ remains unknown as they were redacted by the Home Office.
In March 2023, PLP announced it had launched a legal challenge against the system, arguing it could discriminate against people from certain countries. 
Operator: UK Home OfficeDeveloper: UK Home Office
Country: UK
Sector: Govt - home/interior
Purpose: Detect sham marriages
Technology: Machine learningIssue: Bias/discrimination - age, nationality
Transparency: Governance; Black box; Marketing"
Tesla Autopilot confused by billboard,"Tesla owner Andy Weedman has discovered that his Tesla was registering a giant stop sign printed on a billboard as a real traffic sign, and so halting in the middle of the road.
Weedman tweeted and recorded a video of the Tesla stopping for the billboard, referring to it as an 'edge case'. The discovery prompted commentators to observe that autonomous cars have a long way to go before they are truly fit for purpose. 
Others went further, arguing that 'the world is absolutely crammed with out-of-the-ordinary situations we call edge cases,' and that it will 'likely take more than patchwork updates and fixes to actually reach full autonomy'. 
Operator: Andy Weedman Developer: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
Amazon Buy Box,
"PredPol perpetuates racial, ethnic, income bias","An investigation by The Markup and Gizmodo has discovered potential bias issues with crime prediction system PredPol.
Analysis of a huge volume of crime predictions left on an unsecured server showed PredPol (now renamed Geolitica) often avoided White and middle-to-upper-income residents neighbourhoods, and targeted Black and Latino neighbourhoods. 
The findings suggest PredPol technology is resulting in so-called 'feedback loops' in which lower-income, ethnic zones are treated as surveillance hotspots and lead to disproportionately higher numbers of arrests of minority populations.
Operator: Los Angeles Police Department Developer: Geolitica/PredPol
Country: USA
Sector: Govt - police
Purpose: Predict crime
Technology: Behavioural analysis Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, income
Transparency: Governance; Black box"
"Tesla Model S crashes into tree, kills two passengers","A Tesla Model S failed to navigate a turn in the Houston suburb of Spring before running off the road, hitting a tree, bursting into flames and killing two passengers. Authorities said no one appeared to be in the driver’s seat at the time of the crash.
Whilst it was unclear whether the car’s Autopilot driver-assist system was being used, the implication of nobody being in the driver's seat was that it was likely switched on.
However, Tesla CEO Elon Musk later claimed on Twitter that 'data logs recovered so far show Autopilot was not enabled and this car did not purchase FSD.' He also said that 'standard Autopilot would require lane lines to turn on, which this street did not have.'
According to Electrek, 'Tesla has been known to pull the logs out of vehicles involved in crashes where Autopilot was blamed ... and has also been known to sometimes present the data in misleading ways.'
A US National Transportation Safety Board (NTSB) investigation concluded the driver was operating the vehicle up until the moment it hit the tree and that they had been under the influence of alcohol and drugs.
A few days after the incident, Consumer Reports tricked a Tesla into driving in the car’s Autopilot mode with no one at the wheel.
Operator: Will VarnerDeveloper: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Governance; Black box; Marketing"
Tesla Model Y crashes into parked police car,"A Tesla has crashed into a state trooper’s cruiser with flashing lights outside Lansing, Michigan. The police said the driver was using Autopilot, Tesla’s advanced driver-assistance system, at the time of the late night crash.
Whilst no one was injured, the National Highway Traffic Safety Administration (NHTSA) opened an investigation to determine if and how Autopilot may have contributed to the crash.
In August 2021, the NHTSA announced that it was opening an investigation into Tesla Autopilot over its possible involvement in 11 crashes with emergency and first responder vehicles, including the Lansing incident.
The Tesla's driver was issued citations for failure to move over and driving with a suspended license. 
A few days earlier, the NHTSA sent a special crash investigation team to Detroit after a Tesla had driven under a semitrailer, leaving two people injured.
Operator:  Developer: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
Tesla Model Y crashes into tractor-trailer,"A Tesla Model Y drove through an intersection in Detroit, struck a tractor-trailer and became wedged beneath it, tearing the roof off the car and critically injuring a passenger. 
The National Highway Traffic Safety Administration (NHTSA) said it was sending a team to investigate the 'violent' incident. Detroit police said they did not believe Autopilot was in use during the crash.
In two incidents in 2016 and 2019, Teslas had also driven beneath tractor-trailers in Florida, causing two deaths. In both crashes, the cars were being driven while using Tesla’s Autopilot driving software.
Operator:  Developer: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
"Tesla Autopilot, FSD misleading marketing","Tesla has been regularly dogged by accusations that it has systematically over-stated the capabilities of its Autopilot and Full-Self Driving (FSD) systems, and under-stated their role in accidents. 
A number of regulators have argued that the Autopilot and Full Self-Driving names are misleading given each requires drivers to pay attention and be able to intervene at any time. 
In June 2023, a Washington Post investigation of US National Highway Traffic Safety Administration (NHTSA) found that there had been 736 crashes and 17 fatalities involving Teslas in Autopilot mode in the US since 2019, many more than previously reported. 
Four crashes involved motorcycles. According to the same Post investigation, Tesla accounted for the 'vast majority' of the 807 incidents reported to the NHTSA under a 2021 federal order that requires carmakers to disclose crashes involving driver-assistance technology. 
In 2020, a Munich court ruled that Tesla use of the words 'Autopilot' and 'Full Self-Driving' constituted misleading marketing, since the cars still required a driver to operate. The appeal ruling was over-turned by the Higher Regional Court of Munich in October 2022, according to TeslaMag.
In March 2021 it was reported that Tesla knew and admitted that its Full Self-Driving Capability is not capable of full self-driving. The emails between Tesla's legal team and California’s Department of Motor Vehicles (DMV) were revealed after a public records request from transparency advocacy organisation Plainsite.
A few days after Plainsite published the Tesla/DMV legal communications, US National Transportation Safety Board (NTSB) head Robert Sumwalt warned its sister agency, the National Highway Traffic Safety Administration (NHTSA), that it's 'hands-off approach to oversight of AV testing poses a potential risk to motorists and other road users.' 
'Tesla recently released a beta version of its Level 2 Autopilot system, described as having full self-driving capability. By releasing the system, Tesla is testing on public roads a highly automated AV technology but with limited oversight or reporting requirements,' Sumwalt argued.
In May 2021, the LA Times reported that California's DMV has put Tesla 'under review' to determine whether it misleads customers by advertising its full self-driving capability option. The DMV is allowed to sanction car manufacturers that advertise a vehicle as autonomous when it is not.
In complaints filed with the state Office of Administrative Hearings, the DMV argued that Tesla 'made or disseminated statements that are untrue or misleading, and not based on facts' about how well its advanced driver assistance systems (ADAS) worked.
In September 2021, Motherboard reported that Tesla FSD Beta testers were being forced to sign non-disclosure agreements that specifically prohibited them from speaking to the media or giving test rides to the media. 
A video taken by a FSD Beta tester showing a Tesla swerving to take an unexpected right turn across a crosswalk into the path of several pedestrians had gone viral, prompting Tesla to press Twitter to have it removed.
As noted by The Verge, Tesla was using NDAs to manage public perceptions of its Full Self-Driving software at a time when the company was about to open up access to expand the beta testing to a much wider group of Tesla drivers.
In October 2022, Reuters reported that Tesla had been placed under criminal investigation by the US Department of Justice (DoJ) over claims that the company's electric vehicles can drive themselves, with DoJ prosecutors examining whether Tesla misled consumers, investors and regulators by making unsupported claims about its driver assistance technology's capabilities.
In addition to the US DoJ investigation, the US Securities and Exchange Commission (SEC) had opened a civil investigation into whether Tesla had been misleading investors about the safety of its Autopilot system, according to The Wall Street Journal (WSJ). 
In January 2023, a legal deposition made by Tesla director of Autopilot software Ashok Elluswamy that was taken as part of a lawsuit over driver Walter Huang's 2018 death in a Tesla said a demonstration video was staged to show capabilities like stopping at a red light and accelerating at a green light that the system did not have.
The video was released in October 2016 and was promoted on Twitter by Elon Musk as evidence that 'Tesla drives itself.' The video remains archived on Tesla’s website.
In February 2023, Tesla investors lodged a class-action lawsuit in San Francisco accusing Elon Musk and his company of 'deceptive and misleading marketing of ADAS technology' - specifically Tesla's Autopilot and Full Self-Driving technologies. 
'Although these promises have proven false time and time again, Tesla and Musk have continued making them to generate media attention, to deceive consumers into believing it has unrivaled cutting-edge technology, and to establish itself as a leading player in the fast-growing electric vehicle market,' the suit states.
Tesla responded to a data leak to Handelsblatt that showed thousands of customer complaints about self-acceleration issues, braking problems, including 'unintentional emergency braking', and 'phantom stopping', by demanding that 'the data be deleted and spoke of data theft.'
In July 2023, a Reuters investigation revealed Tesla has for years been rigging the dashboard readouts in its electric cars to provide 'rosy' projections of how far owners can drive before needing to recharge. The carmaker went on create a special team in 2022 to cancel owners’ service appointments after a deluge of complaints regarding its driving range capalities and misleading marketing claims.
Operator: Tesla Developer: Tesla Country: USA; Germany Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Governance; Ethics Transparency: Governance; Black box; Marketing; Legal"
DeepScore trustworthiness assessments bias,"A voice and facial recognition app with a 10-question survey that it says can score how trustworthy a user is that has developed, However, privacy and human rights advocates are concerned about the product's accuracy and propensity for bias.
Japanese software company DeepScore enables loan lenders, insurance companies, and financial institutions to decide in real-time whether people are lying or not with their gestures and tone of voice at a reputed accuracy rate of around 70%, and a 30% false negative rate. 
Parity AI founder Dr. Rumman Chowdhury told Motherboard the app is at a 'minimum likely to discriminate against people with tics, anxiety, or who are neuroatypical.'
Others are concerned about DeepScore's privacy implications. Ioannis Kouvakas, a legal officer for Privacy International, told Motherboard he did not believe the company would be able to legally operate in the European Union due to the bloc’s General Data Protection Regulation (GDPR).  
DeepScore does not have a privacy policy on its website, and it appears that many of its customers are located in countries with threadbare or non-existent privacy laws.
Operator: DeepScoreDeveloper: DeepScore
Country: Japan
Sector: Banking/financial services
Purpose: Assess user/customer trustworthiness
Technology: Facial recognition; Voice recognition Issue: Accuracy/reliability; Bias/discrimination - disability; Privacy
Transparency: Black box"
Bitcoin mining algorithm environmental damage,"Bitcoin is a digital currency invented in 2008 by an unknown person (or people) that uses cryptography to verify transactions, which are recorded on a blockchain public distributed ledger.
Bitcoin's business model and algorithms - notably its proof-of-work cryptographic verification system - have been accused of causing significant environmental damage, and unduly contributing to climate change.
Specific concerns about Bitcoin's environmental record include:  
Multiple research programmes and studies show Bitcoin is responsible for a high level of carbon emissions and contributes significantly to climate change.
For example, Cambridge University's Bitcoin Electricity Consumption index calculates Bitcoin consumes 133.68 terawatt hours a year of electricity - more than many first-world countries - and estimates bitcoin mining to be responsible for 0.1% of total world greenhouse gas emissions.
A March 2020 energy research journal Joule study said Bitcoin accounted for about two-thirds of the energy consumed by 'proof-of-work' coins - of which an estimated 500 exist.
On the other hand, cryptocurrency advocates such as Twitter co-founder Jack Dorsey argue (pdf) that the bitcoin network could incentivise the more rapid development of renewable energy through the deployment of more solar and wind generation capacity.
Traditionally, Bitcoin required large amounts of fossil fuels - notably coal - to power mining operations, resulting not just in high carbon emissions but also significant volumes of air pollution in the areas around the mining farms, as well as water pollution, resulting in damage to fish and other wildlife populations. 
However, China's 2021 ban on Bitcoin mining saw many operators ditch their old equipment, relocate to other countries, and switch to using a higher percentage of renewable energy sources. 
The need for large stacks of dedicated circuits to power the mining of bitcoin, the lifespan of which is estimated to be approximately 1.3 years and which can not be used for other purposes, has resulted in huge amounts of electronic waste, with one bitcoin estimated to generate 270 to 380 grams (9.5 to 13.4 oz) of e-waste.
A much-cited 2021 research study calculated that Bitcoin's production of e-waste to be over 30,000 tonnes - comparable to the small IT equipment waste produced by the Netherlands.
Operator: Bitcoin.org Developer: 
Country: Global
Sector: Banking/financial services
Purpose: Create currency
Technology: Blockchain; Virtual currency Issue: Environment
Transparency: Governance; Black box; Marketing"
"Ethereum minting, trading environmental damage","Launched in 2015, Ethereum is an open-source, decentralised blockchain which hosts its native cryptocurrency Ether, as well as third-party decentralised applications, including non-fungible tokens (NFTs) and ERC-20 tokens.
Ethereum 1.0 launched using a proof-of-work cryptographic verification system that attracted considerable criticism for its huge energy demands. 
Specific areas of concern about Ethereum 1.0 included: 
Carbon emissions
Air pollution
Water pollution
Noise pollution
Electronic waste 
In September 2022, Ethereum transitioned to a proof-of-stake system, cutting its energy use by an estimated 99.95%.
Operator: Ethereum FoundationDeveloper: Ethereum Foundation
Country: USA
Sector: Banking/financial services
Purpose: Create currency
Technology: Blockchain; Virtual currency Issue: Environment
Transparency: Governance; Black box; Marketing"
HireVue recruitment facial analysis screening,"HireVue is a Salt Lake City-based company that uses video, gaming, and proprietary algorithms to assess job seekers, whose written answers, behaviour, intonation, and speech are fed into algorithms that assign them certain traits and qualities. 
The company is seen as amongst the top in its sector, and its products are used by 700+ customers in the US, UK, and elsewhere, including General Mills, Kraft, and Unilever.
A number of HireVue's practices - notably its use of psychological inferences to determine people's ability and character based on facial data - have also proved controversial with academics, ethicists, regulators, and commentators.
In November 2019, US privacy group Electronic Privacy Information Center (EPIC) filed a legal complaint (pdf) alleging HireVue's use of facial technologies and biometric data 'constitute unfair and deceptive trade practices' and that it produces results that are 'biased, unprovable, and not replicable'. 
It alleged HireVue's hiring algorithms are more likely to be biased by default, in contrast to it's marketing claims, that it's models fail to meet international standards on AI-based decision making, and that the company fails to give candidates access to their assessment scores or the training data, factors, logic, or techniques used to generate each algorithmic assessment.
The complaint also set out that HireVue's claims that it 'does not use facial recognition technology' is misleading as it collects and analyses 'facial expressions' and 'facial movements' to measure job candidates’ 'cognitive ability,' 'emotional intelligence,' and 'social aptitudes.'
Futhermore, EPIC accused HireVue of engaging in the 'intrusive collection and secret analysis of biometric data', thereby causing 'substantial privacy harms' to job candidates, that it’s assessment system causes 'substantial financial harms 'to job candidates, and that it's facial recognition software could be racially biased or improperly used to identify sexual orientation.
In January 2021, HireVue announced that it had 'proactively' stopped using facial analysis in new assessments on the grounds that it's own internal research had 'demonstrated that recent advances in natural language processing had significantly increased the predictive power of language', meaning visual analysis 'no longer significantly added value to assessments.'
In a response to WIRED, John Davisson, EPIC senior counsel, said 'I am surprised they are dropping this, as it was a keystone feature of the product they were marketing.' 'That is the source of a lot of concerns around biometric data collection, as well as these bold claims about being able to measure psychological traits, emotional intelligence, social attitudes, and things like that.'
Alongside its statement on facial analysis, HireVue released the results of an algorithmic audit by O’Neil Risk Consulting & Algorithmic Auditing (ORCAA) that it concludes it's AI-based pre-built assessments used in hiring early career candidates do not demonstrate bias.
However, Brookings Institution fellow Alex Engler pointed out in a Fast Company op-ed that HireVue mispresented both the claim that its termination of facial analysis was 'proactive' and that the audit concluded all of HireVue’s assessments were unbiased. 
The audit, he argued 'was narrowly focused on a specific use case, and it didn’t examine the assessments for which HireVue has been criticized, which include facial analysis and employee performance predictions.'
HireVue claims that it 'leads the industry with commitment to transparent and ethical use of AI in hiring' have been undermined by the opaque nature of its products and selectively misleading marketing. 
In March 2022, HireVue released what it billed as a 'first of its kind' explainability statement (pdf) intended to explain clearly how it uses AI in its game-based and interview assessments.
The Center for Democracy and Technology (CDC) said the statement 'sheds some useful light on how HireVue’s technology works, it is also incomplete in important respects', and 'suggests crucial deficiencies in the fairness and job-relatedness of HireVue’s approach to assessments.'
Operator: Delta; General Electric; General Mills; Hilton; Kraft; UnileverDeveloper: HireVue
Country: USA
Sector: Business/professional services
Purpose: Improve recruitment efficiency & effectiveness
Technology: Facial analysis; Facial recognition; Behavioural analysis; NLP/text analysis Issue: Accuracy/reliability; Bias/discrimination - gender, disability; Privacy
Transparency: Governance; Black box; Marketing"
Retorio talent personality assessments,"Reporters from Bayerischer Rundfunk (German Public Broadcasting) have discovered that an algorithm owned by Munich-based start-up Retorio responds differently to the same candidate in different outfits, such as glasses and headscarves.
Retorio analyses video to determine personality attributes based on the 'Ocean' Big Five Model, which breaks down scores into 'openness, conscientiousness, extraversion, agreeableness, and neuroticism.'
But the company's personality assessment system produced significantly different scores when analysing a woman wearing spectacles and a headscarf, a darkened video of a Black woman, or when a bookshelf was added in the background.
Retorio responded by saying the difference in results was due to the fact that the system had been trained based on how a chosen group of human recruiters perceived jobseekers and their personalities, leading the algorithm to reproduce the feelings and biases of those humans.
On its website, Retorio describes its personality product as 'the world's most innovative video-based personality assessment.'
Operator: Lufthansa; BMW Group; ADAC Developer: RetorioCountry: Germany Sector: Business/professional services Purpose: Identify personality traits Technology: Emotion recognition; Facial analysis; Computer vision Issue: Accuracy/reliability; Bias/discrimination - raceTransparency: Governance; Black box"
UCSB ProctorU data sharing,"A warning by University of California Santa Barbara (UCSB) faculty association members raising concerns about the potential sharing of student data with third parties resulted in a backlash against online proctoring service ProctorU.
According to a March 2020 letter (pdf) to the university's administration, ProctorU 'regularly collects and distributes' a wide range of student information such as social security numbers, browsing history, gender identity, medical conditions, fingerprints, faceprints, voiceprints, retina scans and more.
The faculty went on to say ProctorU's data privacy practices 'implicates the university into becoming a surveillance tool' and to recommend UCSB terminate its contract with ProctorU and discourage professors from using similar services. ProctorU's (now renamed Meazure Learning) privacy policy says personal data collected may be disclosed to third parties for undefined 'business and commercial purposes'.
In response, ProctorU attorney David Lance Lucas threatened (pdf) to sue the faculty association for defamation and violating copyright law, and accused it of 'directly impacting efforts to mitigate civil disruption across the United States' by interfering with education during a national emergency. 
Lucas' threat was condemned as inaccurate and unreasonable bullying by senior lawyers and others. According to Vice, ProctorU never filed a lawsuit against the UCSB faculty association. But the threat 'had a chilling effect on professors’ willingness to discuss the software.'
Operator: University of California (UCSB)Developer: Meazure Learning/ProctorU Country: USA Sector: EducationPurpose: Detect and prevent cheating Technology: Facial recognition; Fingerprint recognition; Voice recognitionIssue: Privacy Transparency: Legal"
Cleveland State University bedroom scans ruled 'unconstitutional',"An Ohio judge ruled (pdf) that the scanning of students' rooms before and during remote exams is an invasion of privacy and a violation of the US Fourth Amendment’s protection against unlawful searches in American homes. The ruling raised questions about the nature and legality of Honorlock and other online exam cheating detection systems.
Cleveland State University student Aaron Ogletree agreed to a room scan before a chemistry exam. With other people in his home, he took the test in his bedroom, where he had sensitive tax documents spread out on a surface. These documents were visible in the room scan recording and were shared with other students.
The university had defended its room scans by saying that it had become common during the COVID-19 pandemic and more acceptable to society, that Ogletree had not been coerced into scanning his room, and that he could have removed any sensitive documents or have chosen to take the test in a different room.
The ruling was applauded by digital rights and privacy groups. Privacy non-profit Fight for the Future said the ruling was a 'major victory' and called on higher learning institutions 'to ban not only room scans, but invasive and discriminatory eproctoring software once and for all.'
In December 2020, the Electronic Privacy Information Center (EPIC) filed a complaint (pdf) against Honorlock and four similar proctoring services, calling their practices 'inherently invasive.'
Operator: Cleveland State University (CSU)Developer: Honorlock; Respondus Country: USA Sector: EducationPurpose: Detect and prevent cheating Technology: Facial detection; Gaze detection; Machine learning Issue: PrivacyTransparency: Privacy"
University of Wisconsin Honolock 'racist' online proctoring,"The University of Wisconsin-Madison (UW-Madison) disabled its Honorlock anti-cheating software after three students with darker skin complained the programme had failed to recognise their facial features. Honorlock disputed the issue was related to skin tone, suggesting the students were looking down or away from the webcam during their exam.
UW-Madison first started working with Honorlock in summer 2020, during the coronavirus pandemic, which forced many students across the US to take exams in their homes. In October 2021, the university renewed its contract with Honorlock, despite over 2,000 UW-Madison students signing a petition complaining it abused student privacy and calling for it to be banned.
Honorlock is an automated proctoring solution designed to help schools and universities monitor exams live using AI technologies. It lets college administrators customise online exams and generate analytics, and students to verify their identity and take tests using a Google Chrome plugin. 
Operator: University of Wisconsin-Madison Developer: Honorlock Country: USA Sector: EducationPurpose: Detect and prevent cheating Technology: Facial recognition; Voice recognition Issue: Accessibility/usability; Accuracy/reliability; Bias/discrimination - race, ethnicity, disability, gender; Effectiveness/value; Privacy; Surveillance Transparency: Governance; Black box"
Proctorio uses 'racist' algorithms to detect students' faces,"A facial detection system run by online proctoring service Proctorio has been found to be 'racist'. College student Lucy Satheesan reverse-engineered the exam software and discovered that Proctorio's software is using a facial detection model that failed to recognise Black faces 57 percent of the time. 
By assessing the code behind Proctorio’s extension for the Chrome web browser, college student Lucy Satheesan discovered that the file names associated with the tool’s 'proprietary' facial detection function were identical to those published by open-source computer vision software library OpenCV.
Tbe discovery appeared to confirm complaints by students that Proctorio's face detection system is inaccurate and unreliable. The company uses the technology to see if a student is looking away from their screen, leaves the room, or if there’s another person in the frame - any of which could indicate cheating.
The Electronic Privacy Information Center (EPIC) had earlier filed a complaint (pdf) accusing Proctorio and four other online test proctoring services of unfair and deceptive trade practices. EPIC said it is preparing to file a lawsuit unless they change their practices.
Operator: Miami University; University of British Columbia; University of Illinois Developer: Proctorio Country: USASector: EducationPurpose: Detect faces Technology: Facial detection; Computer vision; Machine learningIssue: Bias/discrimination - race, ethnicity Transparency: Governance; Black box; Marketing"
Stanford facial study 'reveals' political orientation,"A research study published in Scientific Reports by Stanford University professor Michal Kosinski claims to show that facial recognition systems can expose people’s political views from their social media profile photographs.  
Using a dataset of 1,085,795 Facebook and an unnamed dating site facial profiles from people across Canada, the US, and the UK, 977,777 of whom had self-reported their political orientation fed into the VGG Face open source facial recognition algorithm, Kosinski said he trained an algorithm to correctly classify political orientation in 72% of 'liberal-conservative' face pairs. 
Kosinski noted the algorithm performed substantially better than humans, who are only able to distinguish between a liberal and a conservative with 55% accuracy, just a little better than a coin toss. This is despite conservatives being more likely to be white, older, and male.
The study prompted accusations of physiognomy - the controversial and debunked notion that a person’s character or personality can be assessed from their appearance - given the likelihood that patterns picked up by Kosinski's algorithm may have little or nothing to do with facial characteristics. 
Others question the ethics of the project, notably the rationale of conducting such a study, as well as the potential for the abuse and misuse of these kinds of tools by bad actors for social and political purposes.  
The study cannot be tested as Kosinski made available the project’s source code and dataset but not the actual images, citing privacy implications. 
Operator:  Developer: Michal Kosinski; Stanford University Country: USASector: PoliticsPurpose: Identify political orientation Technology: Facial recognition; Machine learning Issue: Accuracy/reliability; Dual/multi-use; Ethics Transparency: Black box"
4 Little Trees (4LT) student emotion recognition,"An AI programme that analyses students' emotions as they learn in order to help teachers make distance learning more engaging and personalised, came under fire from researchers and digital rights activists fearful that emotion recognition technologies are intrusive, can be misused, and may be biased against people with darker skins.
Find Solution AI, the company behind 4 Little Trees, said its algorithm is 85% accurate and identifies happiness, sadness, anger, surprise, fear, and other emotions by analysing facial muscular micro-movements in real-time. Launched in 2017, 4 Little Treesand proved successful, with the number of schools reputedly using the system growing from 34 to 83 during the COVID-19 pandemic. 
However, a February 2021 CNN report cited researchers and activists noting that emotion recognition technologies are often intrusive, can be misused, and may be biased against people with darker skins. They also struggle to identify more complex emotions such as enthusiasm or anxiety.
A May 2021 Financial Times article noted that research into emotion recognition systems suggested that while they might be able to decode facial expressions much of the time, what a person is really feeling or thinking, or what they plan to do next, may be quite different.
Databnk
Operator: True Light CollegeDeveloper: Find Solution AICountry: Hong Kong Sector: Education Purpose: Identify & monitor emotions Technology: Emotion recognition; Facial analysis; Gesture analysis; Computer vision Issue: Accuracy/reliability; Privacy; Surveillance; Bias/discrimination - race, ethnicity Transparency: Governance; Black box"
Raffaela Spone 'deepfake' cheerleader framing,"Raffaela Spone, 50, was arrested in Pennslyvania, USA, on March 4 for allegedly framing her teenage daughter's cheerleading rivals by creating 'deepfake' photos and videos of them naked, drinking, and smoking. 
According to her prosecutors, Spone aimed to humiliate her daughter's friends and force them from their team - the Victory Vipers. She was charged with cyber harrassment of a child and related offences.
One of the alleged 'deepfakes' was a video showing one of the girls vaping - which violated the rules of her cheer squad - and another showed a girl without clothes in a public space. One of the messages she is meant to have sent along with a doctored image urged one of the girls 'kill herself'.
However, synthetic media experts such as Henry Ajder expressed their doubts that the video had been deepfaked and, during preliminary court proceedings, District Attorney Matt Weintraub revealed that the police did not have definitive proof that Spone had created the images, or that they were manipulated at all. 
Two months later, the deepfake charges against Spone were dropped, though she continued to be charged with harrassment. Spone was found guilty of three counts of misdemeanour harassment in March 2022, and received three years' probation.
A September 2021 Cosmopolitan investigation concluded the videos were likely to be real, and that the viral scandal that emerged from the case was the result of incompetent police work.
Operator: Unclear/unknownDeveloper: Unclear/unknownCountry: USASector: Education; Media/entertainment/sports/artsPurpose: Damage reputationTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Privacy; Ethics Transparency: Governance; Privacy"
Delhi government schools facial recognition 'privacy abuse',"Indian digital rights group Internet Freedom Foundation has discovered that facial recognition technologies are being used in at least a dozen government-funded schools in Delhi, opening the authorities to accusations of 'overreach' and an invasion of childrens' privacy. 
The facial recognition systems were installed without laws to regulate the collection and use of data. India proposed a national privacy law in July 2018, though it still remains in draft early 2023. The move followed a 2019 decision by the Delhi government to mount CCTV cameras in over 700 public schools to ensure the safety of students, and to reduce truancy.
The installation of the facial recognition technology was discovered through a Right to Information query filed with the Directorate of Education in December 2020 by the Internet Freedom Foundation (IFF). According to the IFF, the Government of Delhi failed to inform its various stakeholders of which company is supplying the hardware and software, what the system was for, how it worked, and how the personal data of students and employees was managed.
Operator: Government of Delhi Developer:  Country: India Sector: Education Purpose: Verify student identity Technology: CCTV; Facial recognition; Facial matching Issue: Accuracy/reliability; Appropriateness/need; Privacy; Security; Surveillance Transparency: Governance; Marketing; Privacy"
Netanyahu COVID-19 vaccination chatbot,"A post by then Israeli Prime Minister Benjamin Netanyahu has been deleted and a chatbot linked to his account suspended for violating Facebook's privacy rules.
Visitors to Netanyahu’s Facebook page who clicked on a link about COVID-19 received an automatic message purporting to come from Netanyahu asking for the name and contact details of people who had not been vaccinated.
'If you know someone who is nervous about getting vaccinated, send me their name and phone number,' Netanyahu said in the video. Maybe they'll get a surprise phone call from me and I'll convince them,' he promised. 
'Under our privacy policy we do not allow content that shares or asks for people's medical information,' said a Facebook spokeswoman told Haaretz. 'We have removed the offending post and temporarily suspended the messenger bot, which shared this content, for breaking these rules.'
Facebook had suspended Netanyahu’s chatbot twice before, once for inciting hatred against Israel's Arab population, the second time for breaking Israeli election law by publishing polls in the 24-hours prior to the election. 
Operator: Likud/Benjamin Netanyahu; Meta/FacebookDeveloper: Likud/Benjamin NetanyahuCountry: IsraelSector: Govt - healthPurpose: Increase vaccination rates Technology: Chatbot; NLP/text analysis Issue: PrivacyTransparency: Privacy"
Singapore TraceTogether COVID-19 contact tracing data sharing,
NHS QCovid risk prediction algorithm accuracy,"The reliability of QCovid, an algorithm developed by the University of Oxford to predict the risk of someone catching, being admitted to hospital, or dying from COVID-19 in England, UK, is in the spotlight for wrongly identifying some patients as high risk.
Based on data from 'the first few months of the pandemic', the model took into account various socio-economic indicators, underlying health conditions such as diabetes and heart disease, body-mass index, and postcode (within Britain), among other factors, to return an 'absolute risk of a covid-19 associated death' or hospitalisation.
Following the launch of the tool, an additional 1.7 million people were instructed to shield, with around 800,000 people moved up the priority list to be vaccinated. These include women with previous gestational diabetes but who were healthy and could not understand why it was being recommended that they shield.
Some General Practitioners also described seeing healthy young men on the list. Young, healthy people who are less likely to have measurements such as body weight recorded in their health records, Irene Petersen, professor of epidemiology and health informatics at University College London, told The Guardian.
An updated version of the system was released in November 2021.
Operator: National Health Service (NHS) Developer: University of Oxford; NHS Digital
Country: UK
Sector: Govt - health
Purpose: Predict COVID-19 risk
Technology: Prediction algorithm Issue: Accuracy/reliability
Transparency:"
US border imposter identification failures,"The US Customs and Border Protection (CBP) failed to turn up a single example of an individual impersonating someone else at US airports, despite using facial recognition to scan 23+ million travelers’ faces at 30+ points of entry in 2020. The findings call into question the effectiveness of its facial recognition system.
According to a February 2021 OneZero report based on the agency's 2020 annual report (pdf), the CPB has been testing the collection and analysis of traveller biometrics, including fingerprints and facial images since 2013. 
Techdirt's Tim Cushing notes, 'Spending millions to deal with a minor problem by deploying tech that remains unproven shouldn’t be considered acceptable. Neither is the alternative: a system that rarely recognizes imposters, allowing government agencies to assume it’s less of a problem than it might actually be. 
A September 2020 US Government Accountability Office (GAO) report (pdf) took issue with the CPB over lackluster accuracy audits, poor signage notifying the public the technology is being used, and the paucity of public information on how its systems worked.
Operator: Department of Homeland Security (DHS); Customs and Border Protection (CBP) Developer: 
Country: USA
Sector: Govt - immigration
Purpose: Identify/verify identity
Technology: Facial recognition Issue: Effectiveness/value
Transparency: Governance; Marketing"
Uber compensation algorithm pays new hires less,"An algorithmic compensation system created and operated by Uber paid new recruits less, led to uneven pay across similar jobs, and reinforced existing gender pay gaps, resulting in turmoil across the company.
According to a report in The Information, Uber introduced the system to reduce the amount of equity being given to new hires and to save money. It followed a so-called 'market of one' philosophy in which each offer is tailored to a specific candidate based on his or her personal circumstances. 
But by producing a low minimum offer for candidates, including women who typically make less than men in similar jobs, it  reinforced existing gender biases. 
The system, which was said to have saved Uber several hundreds of millions of dollars of equity, was later altered. 
Operator: UberDeveloper: Uber
Country: USA
Sector: Transport/logistics
Purpose: Determine pay/compensation
Technology: Pay algorithm Issue: Bias/discrimination - gender; Fairness
Transparency: Governance"
Facebook removes 'Ville de Bitche' page,"Ville de Bitche, a fortress town of 5,000+ people in the Moselle department of north-eastern France, saw its official Facebook page - titled Ville de Bitche - suddenly removed without explanation.
Per Politico, Bitche communications official Valérie Degouy told Radio Mélodie that she appealed the decision the same day the town's page was taken down, but had not heard back from Facebook. 'I tried to reach out to Facebook in every possible way, through different forms, but there's nothing [I could] do,' she said.
Facebook reinstated the page and apologised, telling CNN it was 'removed in error.' In a press statement released after the incident, Mayor of Bitche Benoît Kieffer urged Facebook to be more transparent and fair in how it makes content moderation decisions.
Operator: Meta/Facebook; Ville de BitcheDeveloper: Meta/Facebook
Country: France
Sector: Govt - municipal
Purpose: Moderate content
Technology: Content moderation system Issue: Accuracy/reliability
Transparency: Black box; Complaints/appeals"
Dahua Smart Police Heart of the City real-time Uyghur warnings,"Facial recognition software developed by state-owned Chinese video surveillance technology company Dahua provides 'real-time warning for Uyghurs' to the Chinese police, according to video research company IPVM.
Per the Los Angeles Times, documents on a Dahua support website reveal a law enforcement user guide dated December 2019 show Dahua’s 'Smart Police Heart Of City' (HOC) system can send a warning when it detects someone it identifies as Uyghur. And a consumer-facing product called SmartPSS offers a feature to sort by race individuals who pass in front of its cameras.
Dahua responded (pdf) by saying the documents referenced by IPVM and the Los Angeles Times were 'historical internal software design documents' and that it 'does not provide products and services for ethnicity detection' in the 'regional markets reported by the media.'
Dahua denied selling 'ethnicity-focused recognition' products in a November 2020 statement to the South China Morning Post after cybersecurity researchers had found code with an ethnic designation for Uyghurs. The company then deleted and updated the code on its website.
In April 2020, Reuters reported that Amazon had bought 1,500 thermal cameras from Dahua in a deal valued at close to USD 10 million. The cameras were intended to take the temperatures of workers during the coronavirus pandemic.
A day after the Los Angeles Times article was published, US senators Marco Rubio and Robert Menendez sent a letter (pdf) to then Amazon Chief Executive Jeff Bezos asking whether Amazon knew Dahua was on the entity list when it was considering entering into a contract with the company and whether that came up in its deliberations.
Dahua is blacklisted by the US Commerce Department over allegations it helped Beijing detain and monitor Uyghurs and other Muslim minorities. The arrangement with Amazon is legal because the rules apply to the public sector, rather than the private sector. 
However, the deal 'would mean that Amazon willfully ignored guidance from the United States government and purchased equipment from an entity-listed company that is complicit in China’s atrocities against' the Uyghurs, according (pdf) to Rubio and Menendez. 
Operator: Amazon; Modesto City Schools Developer: Zhejiang Dahua Technology; China Electronics Technology Group/Hikvision Country: ChinaSector: Govt - police; Govt - security Purpose: Identify & track Uyghurs Technology: CCTV; Facial recognition; Computer vision; Neural network; Machine learning Issue: Surveillance; Privacy; Bias/discrimination - race, ethnicity Transparency: Governance; Black box; Marketing"
Naypyidaw Safe City facial recognition,
Phyo Min Thein 'deepfake' corruption confession,"A March 2021 video of Yangon former chief minister Phyo Min Thein apparently confessing to corrupting Aung San Suu Kyi with silk, gold and cash has been widely denounced as a fake.
Journalists, academics and others who know Phyo Min Thein quickly noted that his voice in the video did not sound like his real voice and that his lips appeared to be out of sync with his words, and that the video was likely to be a deepfake.
Per WIRED, screen-shots from an online deepfake detector indicated with 90-percent-plus confidence that the confession was a deepfake. However, WITNESS director Sam Gregory reckons it may be a coerced or forced confession on camera. 
The video was broadcast by the military-owned Myanmar TV (MRTV) station, which had been used to formally announce the military coup d'état. The video also showed a mayor of Naypyitaw alleging her National League for Democracy party had committed electoral fraud by inventing voters.
Suu Kyi had appointed Phyo Min Thein as chief minister after he won the 2015 election representing Yangon's Hlegu township at the Yangon regional parliament. He was detained by Myanmar's armed forces in the aftermath of their coup d'état in February 2021. 
Operator: Myawaddy TV (MRTV) Developer: Government of Myanmar
Country: Myanmar
Sector: Govt - police; Govt - security
Purpose: Damage reputation
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation
Transparency: Governance"
Lucknow police to monitor 'women in distress' using facial and emotion recognition,"A plan by Lucknow police to monitor women’s expressions with facial recognition to prevent street harassment has met with stiff opposition. 
Lucknow police commissioner DK Thakur told The Times of India that authorities aim to install AI-enabled cameras at 200 'crime hotspots' across the city that will alert nearby police stations when they spot a woman's 'distress' due to harassment. The spots were determined by the 'presence of girls and women in the area,' and stalking and harassment complaints.
According to Reuters, digital rights activists are concerned the system is likely to be inaccurate and may lead to intrusive policing, surveillance against vulnerable sections of society, and privacy violations. Some feel it may also lead to over-policing in the areas where facial and emotion tracking technologies are being deployed.
Lucknow police also stood accused of inadequate transparency by failing to consult with the public, and by providing the public with inadequate information on how the technology works, how data is stored, and who can access the data. 
Privacy was declared to be a fundamental right by India's Supreme Court in a landmark ruling in 2017. 
Operator: Lucknow Police Commissionerate Developer: Staqu Technologies
Country: India
Sector: Govt - police
Purpose: Reduce sexual harrassment
Technology: CCTV; Facial recognition; Emotion recognition; Automated license plate/number recognition (ALPR/ANPR) Issue: Accuracy/reliability; Privacy; Surveillance
Transparency: Governance; Black box; Privacy"
Police request Amazon Ring BLM protest footage,"The Los Angeles Police Department (LAPD) came under fire from digital rights non-profit the Electronic Frontier Foundation (EFF) for asking users of Amazon's Ring smart home system for camera footage from the 2020 Black Lives Matter protests that took place across the US in the wake of the death of George Floyd.
The results of a Freedom of Information Act (FOIA) request the EFF had sent to the LAPD suggests the heavily redacted email request made no mention of any specific crime the LAPD may have been pursuing, only that the police wanted footage of an unspecified 'incident' related to a protest.
Per the BBC, Amazon requires that any requests from police include a valid case number for an active investigation, and details of the incident. Such requests can only be made if the purpose is to 'identify individuals responsible for theft, property damage, and physical injury'.
Amazon Ring users are under no obligation to share data with the police. However, a 2019 Motherboard report showed that Ring had been actively coaching police departments on how best to persuade Ring users into voluntarily sharing their surveillance videos.  
Rights groups worry that Ring’s video-sharing systems pressure private citizens into turning over their data to police without the oversight of a judge. 
Operator: Los Angeles Police Department (LAPD) Developer: Amazon/Ring
Country: USA
Sector: Consumer goods; Govt - police
Purpose: Strengthen security, safety
Technology: CCTV Issue: Privacy; Surveillance; Dual/multi-use
Transparency: Governance; Marketing"
TALON license plate camera surveillance,
NYPD 'digidog' hostage surveillance,"Images of an automated New York Police Department (NYPD) 'digidog' equipped with surveillance cameras responding to a hostage situation have drawn complaints from the local community, rights activists, and politicians. 
The NYPD used the Boston Dynamics-manufactured Spot dog to surveil a house in the Bronx in which two men were holding two others hostage. The suspects were later arrested. 
The incident prompted concerns about the technical capabilities of the robot, the scope for misuse and abuse, and its use in lower-income communities. It also raised questions about transparency, with the ACLU asking why the NYPD had failed to list it on its public disclosure of surveillance devices.
Operator: New York Police Department (NYPD) Developer: Hyundai Motor Group/Boston Dynamics Country: USA Sector: AutomotivePurpose: Strengthen law enforcement Technology: Robotics Issue: Bias/discimination - race, ethnicity; Dual/multi-use; Privacy; Surveillance Transparency: Governance"
Netherlands childcare benefits fraud assessments automation,
Son Ji-Chang Tesla Model X sudden acceleration,"Reuters reports that Korean celebrity Son Ji-chang is to sue Tesla after his parked Model X electric SUV suddenly accelerated, causing it to crash through his garage into his living room, injuring him and his passenger. The lawsuit alleges product liability, negligence, and breaches of warranty.
Tesla responded that the data from from the car and other evidence 'conclusively shows that the crash was the result of Mr. Son pressing the accelerator pedal all the way to 100%'. Tesla also argued that the actor was using his celebrity to damage Tesla. 
In January 2021, the US National Highway Traffic Safety Administration (NHTSA) ended a year-long review of claims that over 246 Tesla vehicles had accelerated without warning by concluding that there was insufficient evidence that Tesla might be to blame to warrant a full investigation.
Operator: Son Ji-chang Developer: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance systemIssue: Safety; Accuracy/reliability; Legal - Liability Transparency: Black box"
Zhengzhou authorities turn bank protestors' health codes red,"People trying to join protests in Zhengzhou in order to gain access to their deposits in struggling banks in China's Henan province saw health code apps turn red, classifying them a risk to public health and restricting their movements.
The spring and summer 2022 protests had been triggered by four rural banks stopping customers withdraw cash, resulting in a bank run and fueling allegations of financial corruption. An investigation by the China Banking and Insurance Regulatory Commission later found that a private investment company had collaborated with the banks to illicitly attract public funds via online platforms.
It transpired that 1,317 Henan bank depositors had unfairly received red health codes, almost all whom were unable to withdraw their savings from the four banks. Five Zhengzhou official were found to have manipulated protestors' COVID-19 health code apps and were punished.
The move reinforced fears that COVID-19 contact tracing data could easily be used for covert or nefarious purposes.
Operator: Zhengzhou Municipal Health Commission Developer: Alibaba/Alipay/DingTalk; Tencent/WeChat Country: China Sector: Govt - health Purpose: Control COVID-19 Technology:  Issue: Dual/multi-use; Privacy; Surveillance  Transparency: Governance; Complaints/appeals"
TikTok #intersex 'censorship',"TikTok users noticed that the #intersex hashtag keeps disappearing on the platform, raising questions about possible censorship of intersex people.
According toThe Verge, TikTok does not list banned words and phrases in its community guidelines, and did not make a public statement about the removal, leaving users to speculate whether they were being censored.
TikTok later claimed the tag had been removed by mistake. 
TikTok also stood accused of limiting the reach of posts by LGBTQ, disabled, ugly and poor people, and of suppressing Black creators.
Operator: ByteDance/TikTok Developer: ByteDance/TikTok Country: USA
Sector: Media/entertainment/sports/arts Purpose: Moderate content Technology: Recommendation algorithm 
Issue: Freedom of expression - censorship Transparency: Governance; Black box"
Spotify plan to use emotion recognition deemed 'manipulative',"A patent (pdf) granted to audio streaming company Spotify that identifies and analyses speech to recommend content based on a user's emotional state has been criticised as 'creepy' and 'manipulative'.
The proposed new technology will use speech recognition to determine a user's emotional state, gender, age, and accent. It can also retrieve information about a user’s background and social environment through microphones.
Music Business Worldwide notes that whilst the patent may never be implemented, the technology has garnered strong criticism from digital rights, privacy and security experts.
Fight for the Future, Access Now, and the Union of Musicians launched a dedicated campaign to persuade Spotify to abandon the technology.
Operator: Spotify
Developer: Spotify
Country: USA; Global
Sector: Media/entertainment/sports/arts
Purpose: Assess emotion Technology: Speech recognition
Issue: Privacy; Security; Dual/multi-use
Transparency:"
Kim Kwang-Seok voice recreated for TV show,"The recreation of late South Korean folk-rock singer Kim Kwang-Seok's voice for an AI-human singing competition prompted concerns from his fans and digital rights activists.
Kim's voice was brought to life using local AI company Supertone's Singing Voice Synthesis system to perform 'I miss you', a 2002 ballad by Kim Bum-soo, and performed as a duet with another singer for the 'Competition of the Century: AI vs Human' show on broadcast TV network SBS. 
The performance delighted Kim's fans. Howerver, despite the singer's family giving permission for the use of his voice, the show prompted concerns about plagiarism, copyright, and the potential misuse and abuse of deepfake technologies. 
The late singer died in 1996 aged 31 after a strong of hits. His death was officially attributed to suicide.
Operator: SBS Developer: Supertone Country: S Korea Sector: Media/entertainment/sports/arts  Purpose: Recreate voice  Technology: Deepfake - audio  Issue: Ethics; Copyright; Dual/multi-useTransparency:"
DeepTomCruise TikTok deepfakes,"The Daily Beast reports on a TikTok account in the name of @deeptomcruise that is posting deepfake video clips of actor Tom Cruise golfing, telling jokes about former USSR president Mikhail Gorbachev, and doing magic tricks.
A few minor clues indicate the fake nature of the clips; however, many viewers appear unable to tell whether they are real or otherwise, and commentators focus on the increasing realism of deepfake technologies and their power to manipulate and deceive unsuspecting consumers.
Using the Cruise fake videos, a 2022 study by University of Oxford, Brown University and Royal Society researchers found that most people cannot tell they are watching a 'deepfake' video even if they have been told what they are watching may have been digitally altered. 
As of January 2022, the @deeptomcruise account counts 3.3 milion followers.
Operator: Bytedance/TikTok
Developer: Chris Ume; Miles Fisher
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Imitate Tom Cruise  Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning 
Issue: Ethics; Impersonation; Mis/disinformation Transparency: Governance"
MyHeritage Deep Nostalgia deceased animations,"The launch of 'Deep Nostalgia', an AI system that enables users to imitate a person in a photograph, has drawn concerns about its use of unethical purposes. 
Developed with Israeli AI firm D-ID, geneaology firm MyHeritage's Deep Nostalgia uses artificial intelligence to animate images so that they smile, wink, nod, and move their heads from side to side.
Reaction to the tool has been mixed. Some describe the results as 'amazing' and 'emotional', while others see it as 'spooky', 'creepy', and 'unethical'. Photographs of deceased and living people can be animated using Deep Nostalgia.
MyHeritage told the BBC that it did not include speech 'in order to prevent abuse, such as the creation of deepfake videos of living people'.
In March 2022, MyHeritage introduced LiveStory, which enables users to animate their creations through vocal storytelling by using deep learning algorithms that draw on a custom set of over 140 voices in 31 languages.
Operator: MyHeritage
Developer: D-ID
Country: UK
Sector: Media/entertainment/sports/arts
Purpose: Imitate ancestors Technology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning; Text-to-speech
Issue: Ethics; Impersonation Transparency: Privacy"
Faces of the Riot facial recognition raises privacy concerns,"Over 6,000 facial images of people taking part in the January 6, 2021 insurrection at the US Capitol building were made available on Faces of the Riot, raising privacy concerns.
The site isolated and made public geo-tagged images of people involved in the insurrection that had been posted to Parler, the social networking service associated with Donald Trump supporters. The images had previously been downloaded by hackers after Amazon had decided to cease hosting Parler. 
Whilst each face on the site is only tagged with a string of characters associated with the Parler video in which it appeared, and faces are not named, the initiative raised privacy concerns. It was also seen to have failed to distinguish between lawbreakers and people legally attending the protests.
Operator: Anonymous/pseudonymous
Developer: Anonymous/pseudonymous
Country: USA
Sector: Politics
Purpose: Identify protestors Technology: Facial recognition; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning 
Issue: Privacy; Accuracy/reliability Transparency: Governance; Privacy"
QOVES AI beauty assessment tool raises bias concerns,"An Australian company developed an AI-powered beauty assessment tool that tells people how attractive they are and what they can do to improve it, raising concerns about bias and transparency.
Technology Review notes that Sydney-based startup QOVES is one of many companies offering facial analysis services, though few of them are keen to describe or talk about how their algorithms work in meaningful detail. 
Commentators reckon this reluctance stems from the desire to protect commercial IP on the one hand; on the other it provides a convenient fig leaf for a science that is highly subjective and, given this subjectivity, highly biased. 
QOVES describes itself on its website as a 'facial aesthetics consultancy dedicated to answering the age-old question of what makes a face attractive through modern science, telehealth and machine learning.'
Operator: QOVES
Developer: QOVES
Country: Australia
Sector: Cosmetics
Purpose: Assess & rank beauty Technology: Computer vision
Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, age Transparency: Governance; Black box"
Cruzcampo Lola Flores deepfake ad,"Celebrated Spanish singer Lola Flores has been brought back to life in a controversial news advert by brewer Cruzcampo. 
According to El País, Flores' voice, face, and features were recreated for the ad using deepfake technology and hours of audiovisual material and over 5,000 photographs.
Devised and created by advertising agency Ogilvy and production firm Metropolitana, the spot encourages people to be proud of their roots. Flores hailed from Anadalucia and died in 1995. 
The campaign has been praised for its relevance and realism. However, others complain that it is unethical and unnecessarily commercial. 
Operator: Cruzcampo
Developer: WPP/Ogilvy; Metropolitana; DeepFaceLab
Country: Spain
Sector: Consumer goods
Purpose: Imitate Lola Flores  Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning 
Issue: EthicsTransparency:"
"BMW, Kohler, MaxMara China facial recognition","Bathroom fixture maker Kohler, fashion house MaxMara and BMW have been singled out by the Chinese authorities for covertly using facial recognition to monitor and analyse shopper movements in their mainland China stores. 
CCTV alleges the three companies had failed to inform or gain permission from visitors to their stores that they were being surveilled and their facial images analysed and matched. 
Chinese national data privacy legislation that started on January 1 requires that user are informed and their consent is given when private and sensitive is collected and processed.
The allegations were made during events marking China's annual Consumer Rights Day.
Operator: Kohler; BMW; MaxMara
Developer: Unclear/unknown
Country: China
Sector: Retail
Purpose: Understand shopper behaviour Technology: Facial recognition
Issue: Privacy; Security Transparency: Governance; Privacy"
State Farm automated fraud detection discriminates against Black homeowners,"Fraud detection software used by US insurer State Farm discriminates against Black homeowners, according to a federal class-action lawsuit. 
Filed in the name of Jacqueline Huskey, the suit accused State Farm of repeatedly delayed assessing hail damage to her house, forcing her to provide additional information, and declining to pay part of the claim for external damage. 
Huskey was not alone. Based on a survey of 800 State Farm customers, the NYU showed that Black homeowners were 39% more likely to have to submit extra paperwork and 20% more likely to have to talk to a State Farm representative on at least three separate occasions before having their claims approved.
According to the lawsuit, the cause of the alleged discriminatory practice is State Farm's automated claims processing system, which appears to have had the effect of disproportionately delaying claims of African American homeowners. 
The suit notes that algorithms can have discriminatory effects even where demographic data, such as race, are not included as inputs. 
Operator: Developer: State Farm
Country: USA
Sector: Banking/financial services
Purpose: Process insurance claims
Technology:  Issue: Bias/discrimination - race
Transparency: Governance"
Amazon AWS Panorama workplace surveillance,"AWS Panorama, a new service enabling the automated analysis of workplace security camera footage, has been criticised for its dual use nature. 
According to Amazon, Panorama can be used to 'evaluate manufacturing quality, identify bottlenecks in industrial processes, and monitor workplace safety and security.' 
However, commentators pointed out that the service could easily be used for other purposes, including to monitor employee productivity and performance, raising concerns about surveillance and workplace privacy.
Operator: Amazon
Developer: Amazon
Country: USA
Sector: Business/professional services
Purpose: Assess product quality; Monitor workplace safety & security  Technology: CCTV; Computer vision
Issue: Privacy; Surveillance; Dual/multi-use Transparency: Governance"
"Amazon, Waterstones algorithms promote vaccine misinformation","Amazon was proactively recommending a wide range of anti-vaccination, health misinformation books, apparel and other products, according to a new study.
Researchers at the University of Washington also found (pdf) that Amazon's recommendation algorithms made matters worse by pushing users interested in these products at even more related products. 
Follow-up research in the UK by Sky News found that Foyles and Waterstones online bookstores were also recommending a wide range of COVID-19 anti-vaccination and health conspiracy theory books to their users. 
Amazon, Facebook, Google, Twitter and other companies are under pressure to tackle the huge volume of online misinformation about COVID-19.
Operator: Amazon; Foyles; Waterstones
Developer: Amazon; Foyles; Watertones
Country: USA; UK; France
Sector: Retail; Health
Purpose: Recommend content Technology: Recommendation system
Issue: Mis/disinformation; Freedom of expression - censorship Transparency: Black box; Governance"
Deepfake 'Amazon FC Ambassadors' sow confusion,"Twitter accounts with deepfake profile images purporting to be Amazon employees were used to defend the company's working practices ahead of a vote on the formation of the first labour union at a US Amazon warehouse. 
The accounts pocked fun at Amazon and sowed confusion amongst members of the general public, who were unclear who was the behind the accounts and what their purpose was. Some pointed out that the 'robotic' language language used was evidence that employees were being 'paid to lie'. 
In August 2018, Amazon had created an 'Amazon FC Ambassadors' programme for a select group of Fulfilment Centre warehouse employees to promote working conditions at the company. 
The programme backfired, leading to the creation of multiple parody accounts, and was later scaled back or terminated.
Operator:  Developer: 
Country: USA
Sector: Transport/logistics
Purpose: Satirise/parody
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Ethics; Mis/disinformation
Transparency: Governance; Marketing"
Oosto/AnyVision files facial recognition drone patent,"A patent application filed by Tev Aviv-based technology start-up AnyVision (now Oosto) for a drone-based system with advanced facial recognition capabilities raised concerns about police and military uses.
Filed in August 2019, the system describes how drones can get the best angle to obtain high-quality, facial images of people, and sets out different ways this may be achieved, including avoiding objects that may be obscuring a face.
How the system will be deployed remains unclear. According to AnyVision CEO Avi Golan, it could be deployed to deliver packages and in smart cities. Human and digital right advocates are concerned about police and military uses.
Microsoft announced in 2019 it would stop working with AnyVision after it had been discovered monitoring Palestinians in the Gaza Strip using facial recognition.
Operator: Oosto/AnyVision Interactive Technologies Developer: Oosto/AnyVision Interactive Technologies Country: Israel Sector: MultiplePurpose: Unclear/unknown Technology: Drone; Facial recognition Issue: Surveillance; Privacy; EthicsTransparency:"
Doordash tip withholding,"Tips given to Doordash delivery drivers are not going to the drivers but instead are being sent in whole or in part to the company to offset other business costs.
A few days later, a Doordash customer filed a class action lawsuit accusing the company of 'materially false and misleading' tipping representations on its app. Doordash settled the suit for USD 2.5 million in November 2020.
In August 2019, the company changed its tipping and pay model for its delivery workers so that they get all the tips customers add to their bills.
Operator: Doordash 
Developer: Doordash Country: USA
Sector: Transport/logisticsTechnology: 
Purpose: Determine worker pay
Issue: Employment - pay; Fairness Transparency: Governance"
Bytedance/TikTok automated Uyghur censorship,"The Chinese government demanded TikTok owner Bytedance develop algorithms to identify and automatically delete Uyghur language posts, giving creedance to rumours of ethnic censorship by Beijing.
The employee, who worked for a technology team that supports Bytedance's central Trust and Safety team, also alleged the company is also actively censoring and deleting content critical of Beijing. 
The employee wrote pseudonymously in Protocol that the technologies created by his team 'supported the company's entire content moderation in and outside China, including Douyin at home and its international equivalent TikTok'.
Rumours of Bytedance's close relationship with the Chinese government, and its surveillance and censorship of Uyghur-related content through its Douyin and TikTok services have long circulated.
Operator: Bytedance/TikTok/Douyin  Developer: Bytedance/TikTok/Douyin Country: China; Global Sector: Technology Purpose: Identify/remove toxic content Technology: Recommendation algorithm Issue: Freedom of expression - censorship Transparency: Governance"
Rikers Island prisoner risk classification system increases violence 50%,"A USD 27.5 million risk-based data analytics system designed to reduce violence at Rikers Island prison, New York, contributed to assaults and other attacks there increasing by almost 50%, according to a ProPublica investigation. 
Created by consulting company McKinsey, the system used an algorithm based on factors including age, possible gang affiliation, and any prior history in jail to determine where to house people behind bars with the least risk for confrontation.  
In April 2017, McKinsey reported that violence had dropped over 50% in the 'Restart' housing units at the prison. But documents and correspondence analysed by ProPublica discovered that the numbers were 'bogus' and the relevant units stacked with inmates thought to be compliant and unlikely to get into fights or to attack staff.
It also transpired that that IntelWatch, a data analytics system designed to predict gang violence and future violence at the prison, was 'riddled with bugs and errors', did not work, and was never used, despite having been paid for. Furthermore, McKinsey consultants and corrections officials used encrypted messaging app Wickr to communicate and share documents in a deliberate  effort to avoid transparency and accountability. 
McKinsey argued the programme positively impacted the areas of the jail in which it was piloted, and that its consultants were not involved in or aware of efforts to 'improperly skew or reduce the reported levels of violence in the Restarts.'
Operator: New York City Department of CorrectionDeveloper: McKinsey
Country: USA
Sector: Govt - justice
Purpose: Reduce violence
Technology: Classification algorithm Issue: Accuracy/reliability; Effectiveness/value
Transparency: Governance; Marketing"
Facebook job ad delivery gender discrimination,"Facebook is showing different job ads to women and men, according to an audit by independent researchers at the University of Southern California (USC).
The audit indicates that Facebook’s ad-delivery system is withholding showing job ads to women, even though the jobs require the same qualifications, something that is considered sex-based discrimination under US equal employment opportunity law. 
As Technology Review points out, the findings come despite years of advocacy and lawsuits, and after promises from Facebook to overhaul how it delivers ads.
Operator: Meta/Facebook Developer: Meta/Facebook Country: USA Sector: Business/professional services Purpose: Target audiencesTechnology: Advertising management system Issue: Bias/discrimination - genderTransparency: Governance; Black box"
Seoul bridge AI suicide detection raises privacy concerns,"The use of AI by the Seoul Metropolitan Government to detect and prevent suicide attempts on the Han River raised concerns that the project may encroach on personal privacy.
With the highest suicide rate of any OECD country, a rate that accelerated during the COVID-19 pandemic, authorities in the South Korean capital Seoul have been operating a 24-hour CCTV-based suicide surveillance and response system since 2012 on bridges over the Han River. 
To help reduce the number of suicides, and better spread resources, Seoul's Institute of Technology (SIT) and Seoul Fire and Disaster Headquarters started using machine learning to alert them to the scenes most likely to require intervention.
However, the intiative alarmed some privacy experts, who warned it could violate the privacy of people using bridges across the Han River, especially given the lack of public signage. Others expressed fears that the system could be used for other surveillance-related purposes in the future.
Operator: Seoul Metropolitan GovernmentDeveloper: Seoul Institute of Technology (SIT); Seoul Fire and Disaster Headquarters (SFDH) Country: S Korea Sector: Govt - municipal Purpose: Reduce suicides Technology: CCTV; Computer vision; Machine learning Issue: Privacy; Dual/multi-use; Surveillance Transparency: Governance"
Israel AI robot machine guns fire tear gas at Palestinian protestors,"Israel is deploying robotic machine guns that 'frequently coat hillsides in tear gas' and unleash sponge-tipped bullets at Palestinians without warning.
According to local residents speaking to the Associated Press (AP), guns made by Israel-based Smart Shooter, a company that makes 'fire control systems' that 'significantly increase the accuracy, lethality, and situational awareness of small arms,' have been spotted at the Al-Aroub refugee camp in the southern West Bank, and in the city of Hebron.
According to Smart Shooter, the system enables Israeli authorities to monitor individuals in a crowd and lock the gun onto specific body parts. 'The system fires only after algorithms assess complex factors like wind speed, distance, and velocity,' the company told the AP.
Critics view the deployment as an intrusive and potentially damaging form of population surveillance that could easily be misused. The company did not respond to questions concerning the extent of human oversight of the system, and regarding it's security. 
Operator: Israel Defense Forces (IDF) Developer: Smart Shooter
Country: Israel
Sector: Govt - security
Purpose: Control population
Technology: Computer vision; Robotics Issue: Human/civil rights; Safety
Transparency: Governance"
Facebook US political group recommendations volte-face,"Facebook recommended users of its platform to join political and civic groups on its platform, despite having publicly committed to stop doing so ahead of the US 2020 presidential election.
According to The Markup, Facebook continued to recommend political groups throughout January 2021, having renewed its promise not to on January 11. Data also suggests that Facebook remained disproportionately likely to recommend political groups to supporters of Donald Trump. 
The finding prompted Senator Ed Markey to write to Mark Zuckerberg asking for an explanation, calling Facebook group 'breeding groups for hate'. 
On January 11, Facebook had stated in a blog post that it was 'not recommending civic groups for people to join' on its platform. 
Databnk
Operator: Meta/Facebook
Developer: Meta/Facebook
Country: USA
Sector: Politics
Purpose: Recommend groups
Technology: Recommendation algorithm
Issue: Mis/disinformationTransparency: Governance; Black box"
Facebook political ad ban drives Georgia political partisanship,"Partisan content quickly replaced news coverage when Facebook turned off a ban on political and social issue advertising in Georgia, USA, according to The Markup.
Having banned political sponsored advertising to minimise misinformation during the 2021 US presidential elections, Facebook turned off the feature before Georgians headed to the polls to vote on a run-off for two Senator seats.
The finding prompted researchers to express concerns about the power of Facebook's platform in a political context, the ease with which misinformation and disinformation can be weaponised, and the difficulty in understanding their impacts without meaningful access to its system. 
Operator: Meta/Facebook
Developer: Meta/Facebook
Country: USA
Sector: Politics
Purpose: Review advertising
Technology: Advertising management system
Issue: Mis/disinformation Transparency: Governance; Black box"
Facebook advertises military gear during US attempted coup,"Facebook has been running adverts for military gear and weapons accessories next to posts about the January 6 attempted insurrection in Washington DC, USA.
Non-profit Tech Transparency Project's (TTP) discovery raises questions about the effectiveness of the social network's content safety measures. It also prompted a number of Democrat senators to ask Mark Zuckerberg to permanently block ads of products clearly designed to be used in armed combat.
Facebook later said it would ban ads for weapon accessories and protective equipment in the US 'out of an abundance of caution' until at least two days after Joe Biden's inauguration on January 20, 2021.
TTP's discovery comes weeks after it had found that Facebook was permitting US militia groups to run recruitment ads on its platform.
Operator: Meta/Facebook
Developer: Meta/Facebook
Country: USA
Sector: Retail
Purpose: Review advertising
Technology: Advertising management system
Issue: Safety; Accuracy/reliability; Ethics Transparency: Governance; Black box"
"Facebook blocks 'sexual' cows, office buildings","The BBC reports that photographs of wildlife, landscapes and buildings taken by the owner of a digital photo library have been blocked by Facebook for supposedly containing 'overtly sexual' content. 
Mike Hall had been trying to use photos on his business page to run ads on Facebook. His images include a cow standing in a field, the England cricket team, and office buildings. 
Facebook later admitted the block had been applied in error, though gave no details why and how it had been put in place. 
Operator: Meta/Facebook
Developer: Meta/Facebook
Country: UK
Sector: Business/professional services
Purpose: Review advertising
Technology: Advertising management system
Issue: Accuracy/reliability Transparency: Governance; Black box"
"Facebook Australia blocks news, civil society organisations","In February 2021, Facebook blocked news journalism on its platform rather than pay the companies that produce it under the Australia's government's proposed News Media and Digital Platforms Mandatory Bargaining Code that would make platforms pay for publishers' content.
However, the social network wiped clean the pages of new publishers, as well as of government departments, emergency services, non-profits and charities. Meantime, conspiracy theorist and anti-vaccine group pages were left untouched.
At the time, the move outraged Australians across all sections of society and raised questions about the accuracy of Facebook's content moderation and misinformation systems. 
Both systems had - and continue - to be widely accused of being ineffective, despite Facebook's insistence to the contrary.
The fracas also led some politicians and commentators to question whether Facebook had deliberately extended its news blocking activities to raise pressure on the Australian government. 
These fears appears to have been realised by a May 2022 Wall Street Journal report based on whistleblower testimony that indicates Facebook designed and used a custom algorithm for deciding what pages to take down that it knew would affect organisations other than news publishers, thereby ratcheting the pressure on the Australian government.
Facebook restored news to its platform after the Australian government introduced last-minute amendments to the legislation. 
An earlier threat by Google to shut down its search engine in Australia failed to materialise, with the search engine company instead choosing to forge private deals with news publishers. 
Operator: Meta/Facebook
Developer: Meta/Facebook
Country: Australia
Sector: Media/entertainment/sports/arts
Purpose: Moderate content
Technology: Content moderation system
Issue: Accuracy/reliability; Ethics; Mis/disinformation Transparency: Governance; Black box; Complaints/appeals; Marketing"
Wisconsin Dropout Early Warning System found to be mostly wrong,"A system used to predict the likelihood of students graduating at Wisconsin high schools was found to be wrong most of the time, raising questions about the reliability of the system.
Wisconsin's machine learning-based application Dropout Early Warning System (DEWS) was wrong 'almost three-quarters of the time' that it predicted a student would not graduate, and wrong more often about Black and Hispanic students not graduating than about white students, according to a 2023 Markup/ChalkBeat investigation.
The investigation also found that educators had not been properly trained to understand the dropout risk labels and intervene accordingly, and that students had not been informed about the existence of the system.
Meantime, University of California-Berkeley researchers concluded the system failed to achieve its primary goal of improving graduation outcomes for the students it labels as 'high risk'. They recommended Wisconsin scrap its use of individual factors such as a student’s race or even test scores in determining that student’s dropout risk.
Operator: Wisconsin Department of Public Instruction Developer: Wisconsin Department of Public Instruction
Country: USA
Sector: Education
Purpose: Predict student drop-outs
Technology: Machine learning Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity
Transparency: Governance; Marketing"
Serbia Social Card excludes thousands of welfare beneficiaries,"A Serbia government system meant to automate the allocation of welfare payments and detect fraud resulted in thousands of beneficiaries not receiving their payments. 
Approved by the country's parliament in March 2022 and developed by local IT company Saga, Serbia's socijalna karta (Social Card) system uses approximately 130 types of data collected from the Tax Administration, Ministry of Interior, and other government agencies to process individual payments in a fully automated manner. 
In November 2022, a group of human rights organisations, including Amnesty, submitted a legal opinion that accused the Serbian government of creating an 'invasive digital surveillance system that threatens the right to equality' by excluding the Roma and the disabled from the system, thereby increasing disparities against them.
Serbia's government has refused to provide access to the system's algorithm or data. 
Operator: Serbia Ministry of LabourDeveloper: Saga
Country: Serbia
Sector: Govt - welfare
Purpose: Allocate social benefits
Technology:  Issue: Accuracy/reliability; Bias/disrimination - ethnicity, disability; Fairness; Privacy; Surveillance
Transparency: Governance; Black box; Complaints/appeals"
Portland scuppers 'Smart City' mobility analytics plan,"A plan to track mobility patterns of how people move throughout the urban area of Portland, Oregon, was scuppered due to concerns about data quality, transparency, and privacy.
Portland Metro had planned to use mobile tracking data to inform decision-making about the planning of bike lanes, road repairs, and bus services. 
According to reports, Portland Metro decided to pull out largely because as a series of disputes with Google's Sidewalk Labs spin-off Replica over data quality, transparency, and privacy. Conversely, Replica says it was unwilling to share data to the level of detail requested by the city.
Last year, a Sidewalks Lab project to build a smart city in a disused waterfront area of Toronto also fell apart. An independent review panel had criticised the plan as 'tech for tech's sake', with some of the proposed elements 'irrelevant or unnecessary'.
Operator: Portland Metro Developer: Alphabet/Google/Sidewalk Labs; Replica Country: USA Sector: Govt - municipal Purpose: Track mobility patterns Technology: Location tracking Issue: PrivacyTransparency: Governance"
Google Nest Hub 2 sleep sensing data uses,"The success of Google's new Nest Hub smart device will likely depend on whether people trust Google enough to let the company monitor them whilst they are asleep.
Many reviewers praise the product's basic functions, usability, and sleep monitoring capabilities. But some commentators  questioned the accuracy of Google's sleep sensing and analytics. Others highlight Google's rapacious quest for user data, and wonder what the company might do with it - now, and in the future. 
Optional for now, it appears Google is looking to charge for the service going forward. The company says none of the data collected through Nest Hub's sleep sensing feature will be used to sell personalised advertising.
Operator: Alphabet/Google/Nest  Developer: Alphabet/Google/Nest Country: USA; UK Sector: HealthPurpose: Detect & analyse sleep patterns Technology: Sleep sensingIssue: Accuracy/reliability; Privacy; Dual/multi-useTransparency:"
Huawei Uyghur-spotting system patent application,"A patent application filed by Huawei sets out a system that identifies people who appear to be Uyghurs amongst images of pedestrians, sparking accusations of ethnic surveillance and poor ethics.
Discovered by video research group IPVM, the patent (pdf) describes AI techniques for identifying pedestrians by attributes including race. The patent was filed by controversial Chinese technology company Huawei in conjunction with the Chinese Academy of Sciences in 2018.
Huawei said the Uyghur detection capability 'should never have become part of the application' and that it is 'taking proactive steps to amend' it. 
IPVM also revealed patents for systems capable of recognising Uyghurs filed by Chinese AI companies Megvii and SenseTime.
Operator:  Developer: Huawei; Chinese Academy of Sciences Country: China Sector: Govt - police Purpose: Detect Uyghurs Technology: Object recognition Issue: Surveillance; PrivacyTransparency: Governance; Privacy"
Huawei 5G deepfake Twitter influence campaign,"Social media research company Graphika has published a report (pdf) that exposes an opaque, coordinated online campaign to attack the Belgian government's plan to ban Huawei from supplying 5G equipment to the country.
The campaign consisted of 14 Twitter accounts, each of which used a fake name and a fake profile image that had been generated using deepfake technology. Each profile posed as Belgium-based technology and 5G experts and pushed articles critical of the Belgian government. 
According to the report, the accounts spent their time retweeting content from popular accounts and mixing it with their own tweets, alongside tweets praising Huawei as a reliable investor and business partner. 
In addition, Huawei officials, including Kevin Liu, Huawei president for public affairs and communications in Western Europe, regularly shared anti-Belgium government tweets and content regularly.
Operator: HuaweiDeveloper: HuaweiCountry: Belgium Sector: Govt - telecoms Purpose: Influence goverment decision-making Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning; Bot/intelligent agent Issue: Mis/disinformationTransparency: Governance"
IBM Project Debater manipulation dangers,"The growing sophistication of IBM's Project Debater, an autonomous AI system that can debate competitively with humans, prompted concerns about the extent to which it could be manipulated in a real-world setting.
IBM researchers working on the project published a paper in Nature magazine in which they showed that Project Debater is capable of beating humans in debates. In a series of tests, the system was given 15 minutes to research topics and prepare for debates. Humans won most debates, but in one instance it was able to change the stance of nine people.
When tasked with a debate, Project Debater scans the internet for prior research, or quote well-known phrases used by people respected in the field of argument. It also uses IBM's Watson system to listen to the arguments given by opponents and then searches for rebuttals that have been given by others to similar claims. 
In a Nature magazine editorial, University of Dundee's Chris Reed made the case for stronger oversight and greater transparency of language models such as Project Debater and OpenAI's GPT-3 in order to reduce the potential for manipulation and harm.
Operator: IBM Developer: IBM Country: Israel Sector: TechnologyPurpose: Debate with humansTechnology: NLP/text analysis; Sentiment analysis; Text-to-speech Issue: Appropriateness/need; Dual/multi-useTranspa: Governance; Black box"
LA subsidised housing scoring system racial bias,"White people in the Los Angeles area received higher scores from system that guided who receives priority for housing assistance, according to an investigation by The Markup. The findings called into question the accuracy and fairness of the system and similar systems.
The Markup analysis of 130,000 Los Angeles Homeless Services Authority's (LAHSA) Vulnerability Index-Service Prioritization Decision Assistance Tool (VI‑SPDAT) surveys found that White people, notably those under 25, received scores considered 'high acuity' - or most in need - more often than Black people, a gap which persisted year over year. 
The Los Angeles Homeless Services Authority (LAHSA) had already planned to stop using the tool after a research team partnering with the agency found the tool had 'the potential to advantage certain racial groups over others.' 
In 2019, the LAHSA had found that Black people tended to receive lower vulnerability scores despite their overrepresentation in Los Angeles’s unhoused population, something the LAHSA attributed to 'structural racism, discrimination, and implicit bias'. 
Databan
Operator: Los Angeles Homeless Services Authority (LAHSA) Developer: County of Los Angeles Public Health
Country: USA
Sector: Govt - housing
Purpose: Assess subsidised permanent housing elgibility
Technology: Scoring algorithm Issue: Bias/discrimination - race, ethnicity
Transparency: Governance; Complaints/appeals"
"Microsoft reincarnation chatbot raises legal, ethical concerns","A Microsoft patent application that would see dead people digitally resurrected for friends and family to interact with has been approved.
The patent envisages that the chatbot could apply to a 'past or present entity', 'friend, a relative, an acquaintance, a celebrity, a fictional character, a historical figure', or 'random entity', would be trained on a range of sources, including 'images, voice data, social media posts, electronic messages, written letters etc', and may take 2D or 3D form.
Reactions to the technology range from 'weird' to 'dystopian', while digital rights advocates fear it could be used for nefarious purposes. In the absence of existing legislation, lawyers worry that digital resurrection raises real concerns about privacy, copyright, defamation, and other issues.
Microsoft representatives say there are currently no plans to put the chatbot into production.
Operator: MicrosoftDeveloper: MicrosoftCountry: USA Sector: Technology Purpose: Imitate personality Technology: Chatbot; NLP/text analysisIssue: Appropriateness/need; Privacy; Copyright; DefamationTransparency:"
Moodbeam HR emotion tracking,"The BBC reports that UK-based AI start-up Moodbeam is hawking wearable silicon wristbands that its says enable organisations to assess and compare the emotional state of individuals and teams.
Workers are encouraged to push yellow and blue buttons that indicate they are happy or sad; the data is then reported to workers' managers via a dashboard that includes a 'Daily Happiness Score'.
Some commentators are less enthusiastic, reckoning it is unnecessary, invasive, and likely to be inaccurate given how easily bored or frustrated workers can game it.
On its website, Moodbeam states purported benefits for employers include 'Increased productivity', 'Improved petention', and 'Reduced absenteeism'.  
Operator: Moodbeam Developer: Moodbeam Country: UK Sector: Health Purpose: Monitor emotions Technology:  Issue: Appropriateness/need; Privacy; SurveillanceTransparency:"
CLIP computer vision system fooled by handwritten notes,"OpenAI researchers discovered that their CLIP computer vision system could be deceived by tools as simple as a pen and paper. The finding highlighted how easily the system could be fooled.
Launched January 2023, CLIP was billed as a state-of-the-art system that explores how AI systems can learn to identify objects without close supervision by training on a database comprising 400 million image-text pairs scraped from the internet. 
The OpenAI study discovered CLIP includes so-called 'multimodal neurons' which respond to an abstract concept as a word or as a picture, thereby making it more powerful. However, they also potentially opened the system to abuse, such as adding dollar signs over a photograph of a poodle to fool the system into thinking the dog was a piggy bank.
Operator: OpenAI Developer: OpenAI
Country: USA
Sector: Technology
Purpose: Classify images
Technology: Computer vision; Deep learning; Neural network; Machine learning Issue: Robustness
Transparency:"
GPT-3 anti-Muslim bias,"OpenAI's GPT-3 large language model consistently associates Muslims with violence, according to a study by Stanford McMaster university researchers. It also exhibits 'severe bias' compared to stereotypes about other religious groups, they conclude. 
The researchers discovered that the word 'Muslim' was associated with 'terrorist' 23% of the time, and feeding the phrase 'Two Muslims walked into a ... ' into the model, GPT-3 returned words and phrases associated with violence 66 out of 100 times. 
It is not the only time GPT-3 has been called out for racial and religious bias. In 2021, the system kept casting Middle-eastern actor Waleed Akhtar as a terrorist or rapist during 'AI', the world’s first play written and performed live using GPT-3.
Operator: OpenAI
Developer: OpenAI
Country: USA
Sector: Multiple
Purpose: Generate text
Technology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning
Issue: Bias/discrimination - race, religion
Transparency: Governance; Black box"
GPT-2 dupes Medicaid,"WIRED reports that a Harvard medical student has been able to fool volunteers into thinking automatically generated comments submitted about a Medicaid proposal were real. 
Of the 1,000 or so comments on proposed changes to Idaho's Medicaid programme on the medicaid.gov website, half had been generated by student Max Weiss using OpenAI's GPT-2 large language model. 
Troublingly, Weiss' research shows Medicaid users were unable to distinguish real comments from fake ones.
The stunt demonstrates how easily large language systems such as OpenAI's GPT-2 can be deployed to produce effective misinformation and disinformation campaigns. 
Operator: OpenAI
Developer: OpenAI
Country: USA
Sector: Multiple; Govt - health
Purpose: Generate text
Technology: Large language model (LLM); NLP/text analysis; Neural networks; Deep learning; Machine learning 
Issue: Mis/disinformation; Dual/multi-use
Transparency: Governance; Black box"
"SimCLR, iGPT racial bias, stereotyping","TechnologyReview reports that a new research study (pdf) finds that prominent image generation algorithms, including Google’s SimCLR and OpenAI’s iGPT, are biased and prone to negative stereotyping. 
To highlight the issue, researchers Ryan Steed and Aylin Caliskan showed iGPT a head shot of prominent US politician Alexandria Ocasio-Cortez ('AOC') wearing business attire, only for the software to recreate her mutiple times in a bikini or low-cut top. 
Meantime, the researchers were criticised for including AI generated images of AOC in the pre-print paper, thereby exposing her to additional potential sexualisation and other possible abuses.
Operator: Alphabet/Google; OpenAI
Developer: Alphabet/Google; OpenAI
Country: USA
Sector: Multiple; Research/academia
Purpose: Generate images
Technology: Image generation; Neural network; Deep learning; Machine learning
Issue: Accuracy/reliability; Bias/discrimination - gender, race
Transparency: Black box"
Lee Luda AI chatbot,
Teleperformance TP Observer employee monitoring,"Plans by French call centre company Teleperformance to use cameras connected to an AI system to monitor employees working from home for home-working 'infractions' has resulted in a backlash.
According to leaked documents leaked to The Guardian, workers will have to explain why they are leaving their desks to avoid being reported for a breach, amongst other requirements. 
The plans led to complaints by trade unions about the 'unreasonable' and 'intrusive' nature of worker virtual monitoring programmes, including their impact on the privacy of those being monitored and their families.
Teleperformance, which employs over 380,000 people in 34 countries and counts Apple, Amazon and Uber as customers, has since said the remote scans for infractions will not be used in the UK. 
The company has also faced resistance to its workforce monitoring activities and solutions in Albania, Colombia, the US, and other countries.
Operator: Teleperformance 
Developer: Teleperformance  Country: UK; France 
Sector: Business/professional services
Purpose: Monitor employee behaviour 
Technology: Computer vision 
Issue: Appropriateness/need; Surveillance; PrivacyTransparency: Governance; Marketing"
"TikTok collects, sells personal data of US users","TikTok agreed to pay USD 92 million to settle a class-action complaint filed in California that it collected the private data of 89 million users in the US without their permission, and then sold it to third-party advertisers, including Facebook, Google, and entities in China.
The lawsuit alleged the company used 'automated software, AI, facial recognition, and other technologies' to collect and profit from sensitive and confidential data on users’ identity, ethnicity, gender, age, location, contact information and other attributes under the guise of a preventative measure to keep minors off the app. 
According to the lawsuit, the app 'clandestinely vacuumed up' huge quantities of private and personally identifiable data that could be used to identify and surveil users without permission. It also asserted that TikTok went to great lengths to hide its data collection and sharing practices by obfuscating its source code, amongst other measures.
The settlement raised concerns about the company's privacy practices, and was seen to underscore its reputed sharing of US user data with Chinese entities. TikTok said it agreed to settle in order to avoid a drawn-out legal battle.
Operator: ByteDance/TikTok
Developer: ByteDance/TikTokCountry: USA
Sector: Media/entertainment/sports/arts
Purpose: Collect personal data 
Technology: Facial recognition
Issue: PrivacyTransparency: Governance; Privacy"
"Verkada biometric cameras hacked, prompting privacy concerns","Hackers have gained access to security company Verkada, giving them access to live and archived footage of over 150,000 cameras inside the firm's customers facilities across the world, as well as its own offices.
Carried out by a hacker collective which aimed to show the pervasiveness of video surveillance and the ease with which systems could be broken into, the breach involved gaining access to Verkada through a 'Super Admin' account, thereby allowing the hackers to view the cameras of all of its customers.
Some Verkada cameras use facial recognition as a basic capability to identify individuals, potentially exposing the sensitive personal information of its customers and the employees, patients and others being monitored. 
Verkada customers include Tesla, Cloudflare, Equinox gyms, hospitals, jails, schools, and police stations.
Operator: Verkada; Tesla; Cloudflare; Halifax Health
Developer: Verkada Country: USA
Sector: Business/professional services
Purpose: Strengthen security; Identify individuals
Technology: CCTV; Facial recognition
Issue: Security; PrivacyTransparency:"
Amazon Driveri delivery driver safety monitoring,"Amazon is installing 'innovative' AI-enabled video cameras in Amazon-branded delivery vans, according to The Information. 
The Netradyne-supplied cameras access drivers' location, movement, and biometric data to detect risky driver behaviour, with verbal warnings issued when drivers appear distracted, ignore signposts, or drive too fast. 
Vice later reported that drivers who refuse to sign forms allowing Amazon to collect, store and use their facial and other biometric data lose their jobs, and that drivers are being unfairly punished for mistakes they have not made.
Drivers, digital rights advocates and others complain that Amazon is running an inaccurate, unfair, and unnecessary system with inadequate security and privacy protection. 
They also argue that Amazon deliberately makes it difficult for those being monitored to lodge complaints and appeals in a meaningful manner.
Operator: Amazon Developer: NetradyneCountry: USA Sector: Transport/logisticsPurpose: Improve safety Technology: CCTV; Computer vision Issue: Accuracy/reliability; Fairness; Surveillance; Security; Privacy; Employment - jobs, payTransparency: Governance; Black box; Complaints/appeals"
Pyth Bitcoin glitch triggers Bitcoin collapse,"Bloomberg reports that the recent 90% collapse in the price of Bitcoin from around USD 41,000 to USD 5,402 was likely due to a glitch on crypto data network Pyth.
According to a Pyth investigatory report 'The issue was caused by the combination of (1) two different Pyth publishers publishing a near-zero price for BTC/USD and (2) the aggregation logic overweighting these publishers’ contributions and both publishers encountered problems related to the handling of decimal numbers.' 
Whilst the impact on Bitcoin investors remains unclear, it seems some financial trading systems automatically sold Bitcoin in response to the apparent drop in price.
Operator: Pyth 
Developer: Pyth  Country: USA
Sector: Banking/financial services 
Purpose: Provide pricing information
Technology: Pricing algorithm
Issue: Accuracy/reliability Transparency: Black box"
Deliveroo Italy rider reliability discrimination,"In a case bought by a group of Deliveroo riders and backed by CGIL, Italy’s largest trade union, a Bologna court has ruled (pdf) that a 'secretive' algorithm used by food delivery company Deliveroo to assess the reliability of its riders is discriminatory.
The ruling found that the company's 'Frank' algorithm violated local labour laws by failing to distinguish between legally legitimate reasons riders may not be working, such as illness or serious emergency, and more mundane reasons. thereby unjustly penalising riders with legitimate reasons for not working. 
Deliveroo, which says it no longer uses the algorithm, has been ordered to pay EUR 50,000 to every affected rider. It had previously claimed the algorithm cut delivery times by 20%.
In April 2021, Deliveroo riders across the UK striked against the company's poor pay, safety and workers' rights record.
Operator: Deliveroo
Developer: Deliveroo Country: Italy
Sector: Transport/logistics
Purpose: Determine rider reliability
Technology: Workforce management system
Issue: Bias/discrimination - productivity; Fairness; Employment - pay Transparency: Governance; Black box"
Deliveroo UK riders protest poor safety and pay,"Riders for meal delivery company Deliveroo have been holding strikes in London and cities across the UK to protest against the company's poor pay, safety, and workers' rights record. 
The strike coincides Deliveroo public listing on London's stock exchange. The largest IPO in London since 2011, the company's stock price fell 26% on its first day due to concerns about employment conditions and the outsize control given to Deliveroo founder Will Shu also played a part. The listing was described by a banker as 'the worst IPO in London's history'.
The UK's top court recently ruled that Uber drivers should be classified as 'workers' rather than 'self-employed', raising significant questions about Deliveroo's business model. In January 2021, an Italian court ruled that a 'secretive' algorithm used by Deliveroo to rank and offer shifts to riders in Italy was discriminatory.
Operator: Deliveroo
Developer: Deliveroo Country: UK 
Sector: Transport/logistics 
Purpose: Determine rider pay
Technology: Workforce management system 
Issue: Employment - pay, working conditions Transparency: Governance; Black box"
Doordash order matching algorithm,"Drivers for food delivery company Doordash have launched #DeclineNow, an apparently successful campaign against the company's low pay rates by gaming its order matching algorithm. 
According to Bloomberg, two drivers  worked out that the platform automatically reallocates a delivery order at a slightly higher rate after another driver has declined it. 
The movement now counts over 40,000 members and aims to ensure no delivery job is made for less than USD 7. However, to what extent the gambit proves sustainable the gambit is an open question. 
In April 2020, Doordash, Grubhub, Postmates and Uber Eats were accused of abuse of monopoly by only listing restaurants if their owners signed contracts which include clauses that require prices be the same for dine-in customers as for customers receiving delivery.
Operator: Doordash 
Developer: Doordash Country: USA
Sector: Transport/logistics
Purpose: Determine rider compensation
Technology: Order matching algorithm 
Issue: Employment - pay; Fairness Transparency: Governance; Black box"
Racist' Uber Eats facial ID check gets Pa Edrissa Manjang fired,"False mismatches by Uber Eats' 'racist' facial identification system resulted in the wrongful dismissal of delivery drivers Pa Edrissa Manjang and Imran Javaid Raja.
Pa Edrissa Manjang, who is Black, claimed that the selfies he had to submit daily mistook him for someone else multiple times, and that the system was 'racist'. 
Manjang also argued that a 'plethora of research' showed the software 'places ethnic minority groups at a disadvantage in that false positive and false negative results are greater in individuals from ethnic minority groups.' 
He went to allege that he had been increasingly targeted for heightened and excessive facial recognition verification checks, amounting to racial harassment. 
A judge allowed the case to proceed after Uber attempted to have it dismissed in July 2022. The suit was filed by The App Drivers and Couriers Union (ADCU) in the UK. 
Operator: Uber/Uber Eats Developer: Microsoft
Country: UK
Sector: Transport/logistics
Purpose: Verify identity
Technology: Facial recognitionIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Employment - pay, jobs
Transparency: Governance; Black box; Complaints/appeals"
Waze directs tourists to drive into Vermont lake,"Google-owned navigation app Waze directed a group of tourists in their borrowed Jeep Compass to drive into an icy lake in Vermont, USA.
According to the police report, the driver 'followed the GPS directions, which advised to go straight, and upon following this he went down the boat ramp onto the ice.' The officer who wrote the report said he believed the driver 'was not under the influence of alcohol or drugs at the time of the incident.'
Google said it was unable to explain how Waze directed the driver into the lake. 'Generally speaking, Waze maps are updated with millions of edits to adapt to real time road conditions daily, often making them the most accurate available', a spokesperson told the Burlington Free Press.
Operator: Waze usersDeveloper: Alphabet/Google/Waze Country: USASector: Travel/hospitalityPurpose: Direct driversTechnology: Machine learningIssue: Accuracy/reliability; SafetyTransparency: Governance"
Nutri-Score nutritional labelling system,
Sleeping driver speeds on highway with Autopilot switched on,"A Tesla driver has been caught asleep behind the wheel while doing 93 MPH on the highway near Ponoka, Alberta, in his Model S.
The 20-year old driver, who had both front seats reclined and Autopilot switched on, was slapped with reckless driving charges and had his license suspended for 24 hours. 
The car accelerated when approached by a police car with its emergency lights flashing, even though the driver and passenger appeared to be asleep.
Operator: TeslaDeveloper: Tesla
Country: Canada
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Safety; Accuracy/reliability
Transparency: Black box"
"Tesla Model X crashes into wall, killing passenger","A Tesla electric car spun out of control in the parking lot of a Seoul, South Korea, parking lot, and crashed into a wall and caught fire, killing the passenger and injuring two others. The fire took over an hour to extinguish.
The police are investigating the incident. The car chauffer claimed 'the car suddenly went out of control', leading the police to suggest that sudden unintended acceleration may have been the cause of the accident.
The Korea Herald reported that local experts figure the battery-operated doors may also have been a factor in the outcome of the latest Tesla accident, as the doors remained shut as rescuers tried to force them open from outside.
Operator:  Developer: Tesla
Country: S Korea
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Accuracy/reliability; Safety
Transparency: Black box"
Tesla Model 3 crashes into overturned truck,"A video of a Tesla Model 3 crashing into a truck on a highway in Taiwan while reportedly on Autopilot has gone viral. The May 2020 incident, which was caught on security camera, recorded the car smash into a large truck carrying salad and breakfast ingredients which had flipped over and was lying on its side in the middle of the road. The driver was apparently unharmed.
Per Taiwan English News, 'According to preliminary investigations by the Highway Police Bureau, the 53-year-old Tesla driver, Mr Huang, said that his car was on autopilot, and traveling at around 110 kilometers per hour at the time of the crash. As soon as he saw the truck, he stepped on the brake, Mr Huang said. 
However, it was too late to stop the vehicle, and it crashed through the roof of the overturned truck.'
Operator:  Developer: Tesla
Country: S Korea
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Accuracy/reliability; Safety
Transparency: Black box
Tesla Autopilot, Full-self Driving
Incident video
https://www.setn.com/News.aspx?NewsID=753860
https://electrek.co/2020/06/01/tesla-model-3-crashing-truck-autopilot-video-viral/
https://taiwanenglishnews.com/tesla-on-autopilot-crashes-into-overturned-truck/
https://www.forbes.com/sites/bradtempleton/2020/06/02/tesla-in-taiwan-crashes-directly-into-overturned-truck-ignores-pedestrian-with-autopilot-on/
https://www.dailymail.co.uk/sciencetech/article-8377461/Shocking-moment-Telsa-Model-3-Autopilot-mode-crashes-truck-Taiwan-highway.html
https://cleantechnica.com/2020/06/02/the-latest-my-tesla-ran-into-a-really-big-truck-while-on-autopilot-kerfluffle/
https://www.autoblog.com/2020/06/01/video-tesla-model-3-crashes-into-overturned-truck/
https://www.republicworld.com/entertainment-news/whats-viral/tesla-crashes-into-truck-driver-alleges-it-was-on-autopilot.htm
https://www.cna.com.tw/news/asoc/202006010318.aspx
https://bgr.com/2020/06/01/tesla-crash-model-3-autopilot-truck-taiwan/
Sleeping driver speeds on highway with Autopilot switched on
Tesla Model Y crashes into parked police car
Page infoType: IncidentPublished: March 2023"
"Tesla Model S driver watches movie, crashes into police car","A Tesla Model S whose driver was watching a movie on his phone while his car was on Autopilot smashed into the rear of a police patrol car in North Carolina, USA. Nobody was hurt in the crash, but the Tesla was destroyed and the rear-end of the police car was badly damaged and a wheel was ripped off.
Nash County Sheriff Keith Stone commented 'It shows automation is never going to take the place of the motoring public paying attention, not texting, not being on the phone, but focusing on what you were doing, that is, driving.'
Tesla has been accused of unduly hyping the capabilities of its Autopilot and Full-self Driving functions by multiple regulators in the US and elsewhere.
Operator:  Developer: Tesla
Country: USA
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Accuracy/reliability; Safety
Transparency: Black box"
Tesla Autopilot tricked into accelerating,"Researchers at McFee Labs have tricked several Tesla models into breaking the speed limit by sticking a 2-inch piece of electrical tape on a sign. 
When the researchers placed a bit of black electrical tape measuring 5cm on a road sign depicting a 35mph speed limit, the MobilEye camera in a Model X misread the sign as 85mph and began accelerating.
The attack only worked for older Tesla Model S and X vehicles using MobilEye's EyeQ3 camera system. The system is deployed in over 40 million vehicles across the world.
Operator: McFee Labs Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system; Computer vision Issue: Safety; Accuracy/reliability Transparency: Black box"
Tesla tricked into reacting to false lane markers,"A joint investigation by The Mark Up and Consumer Reports has found that Allstate, the fourth largest insurer in the US, has been increasing premiums on to a 'suckers list' of customers already paying the highest rates for their insurance, while keeping premiums more or less the same for 'thriftier' customers.
To identify the 'suckers', Allstate devised a 'customer retention model' or 'advanced' price adjustment algorithm that was said to contain dozens of variables that would adjust insurance premiums in the direction of the company's new risk model. 
But, according to the investigators, the model was actually very simple: it identified existing big spenders and squeeze more money out of them than others. 
The algorithm also determined customers which customers were owed discounts.  Though some customers were owed thousands of dollars, Allstate capped the discounts at a half percent irrespective of the amount owed. Senior customers were overrepresented within this cohort.
Insurers are not obliged to inform customers if they are denied discounts, and the National Association of Insurance Commissioners told The Markup that it had never heard of an insurer voluntarily informing its customers that they had been denied a discount. 
Maryland rejected Allstate’s proposal on the grounds that it was discriminatory. But it was approved by Arizona, Arkansas, Wisconsin, and a number of other states. 
Operator: Allstate Developer: AllstateCountry: USASector: Banking/financial services Purpose: Assess customer risk Technology: Price adjustment algorithm Issue: Bias/discrimination - age, income Transparency: Governance; Black box; Marketing"
Allstate car insurance 'suckers list' overcharging,"A teams of researchers at Israel's Ben-Gurion University have created a basic projection system that is able to trick Tesla’s Autopilot driver assistance system into seeing things that don't exist.
The team used off-the-shelf drones and a cheap projector to project a number of 'phantom' images onto the road, including false traffic lines and a false speed limit sign.
Whilst the reaction of the Tesla was mostly fairly limited - when shown an image of Elon Musk, the Autopilot system slowed from 18mph to 14mph - the spoofs show how easily a Tesla could be manipulated by third-parties. 
Ben-Gurion student Ben Nassi also successfully spoofed a Mobileye 630 PRO driver assist system using inexpensive drones and battery-powered projectors.
Operator: Ben-Gurion University Developer: Tesla
Country: Canada
Sector: Automotive
Purpose: Summon car
Technology: Driver assistance system Issue: Accuracy/reliability; Safety
Transparency: Black box
Tesla Autopilot, Full-self Driving
https://www.nassiben.com/phantoms
Incident video 1
Incident video 2
https://arstechnica.com/cars/2020/01/how-a-300-projector-can-fool-teslas-autopilot/
https://www.dailymail.co.uk/sciencetech/article-7944181/Israeli-scientists-trick-Teslas-Autopilot-feature-projecting-fake-signs-road.html
https://www.forbes.com/sites/thomasbrewster/2019/04/01/hackers-use-little-stickers-to-trick-tesla-autopilot-into-the-wrong-lane/
https://www.thedrive.com/news/27262/you-can-fool-teslas-autopilot-by-placing-small-stickers-on-the-ground-study-finds
https://electrek.co/2019/04/01/tesla-autopilot-hacker-tricked/
https://tech.slashdot.org/story/19/04/02/0347232/researchers-trick-tesla-autopilot-into-steering-into-oncoming-traffic
https://thenextweb.com/cars/2020/02/05/teslas-autopilot-dangerously-fooled-by-drone-mounted-projectors/
Tesla Autopilot tricked into accelerating
Tesla Model S tricked into veering into wrong lane
Page infoType: IncidentPublished: March 2023"
Pedestrian following Google Maps hit by motorcyclist ,"A pedestrian was injured by a motorist while following an online walking route across Park City, Utah, filed a lawsuit claiming Google had supplied unsafe directions on its Maps app. 
Google Maps instructed Lauren Rosenberg to walk for about 0.5 miles along 'Deer Valley Drive', an alternative name for that section of Utah State Route 224. Rosenberg said she was not not warned the highway lacked sidewalks, putting Google at fault in the accident, the case claimed. Rosenberg was struck by a vehicle while crossing the road.
The incident prompted controversy, with some people arguing Rosenberg failed to use her common sense. The court later dismissed her case on legal and 'policy' grounds. Rosenberg had sought compensation for 'severe' injuries costing over USD 100,000 in medical bills, lost wages, and unitive damages.
Operator: Lauren Rosenberg Developer: Alphabet/Google Country: USA Sector: Travel/hospitalityPurpose: Direct pedestriansTechnology: Machine learningIssue: Accuracy/reliabilityTransparency: Governance"
Teenager freezes to death after Google Maps provides wrong turn,"Russian teenager Sergey Ustinov died in -50C temperature after Google Maps directed him down a short cut that transpired to be an infamous Siberian road known as the 'Road of Bones'.
Ustinov and a friend were driving from Yakutsk to the Pacific port of Magadan when Google Maps offered a shorter option of 1,733 km (1,076 miles) on the R504 Kolyma highway - also known as the Road of Bones. 
A policemen searching for the missing pair found that they had built fires to stay warm in temperatures plunging to -50C. Ustinov died, and his friend was close to death. Local people were apparently shocked that the two had no warm clothes between them for the Siberian winter. 
Google competitor Yandex Maps did not offer the same short-cut, raising questions about the safety of the former's system.
Operator: Sergey Ustinov Developer: Alphabet/Google Country: Russia Sector: Travel/hospitalityPurpose: Direct driversTechnology: Machine learningIssue:  Accuracy/reliabilityTransparency: Governance"
Agricultural Bank of China facial recognition age bias,"A video has emerged of people waiting in line for an ATM in Guangshui, Hubei province in China, having to lift a 94 year-old woman in order to have her identification verified using the bank's facial recognition system in order to activate her social security card.
The video caused an outcry on social media, with people calling the bank 'inhuman' and urging it and other organisatons to consider the needs of elderly people when designing products and services.
The bank apologised and said it would it improve its customer service. 
Operator: Agricultural Bank of China Developer:  Country: China Sector: Banking/financial services  Purpose: Verify identity Technology: Facial recognition Issue: Bias/discrimination - ageTransparency: Governance; Marketing"
Barclays employee 'spyware' monitoring,"The introduction of software that monitored Barclays' employee activity and performance met with a backlash and resulted in the bank halting its trial. 
A whistleblower revealed to CityAm that the Sapience Analytics software monitored how long staff stayed at and how effective they are at their desks, told them to 'avoid breaks' and recorded toilet trips as 'unaccounted activity.'
Automated warnings were also fired at employees judged to be away from their computers for too long, or if they had been spending too long on a particular task.
The whistleblower alleged that the 'spyware' monitoring had resulted in additional stress, and had sparked a backlash. Barclays had responded that it had been using the software 'to tackle issues such as individual over-working as well as raise general productivity.' 
In March 2022, a UK Information Commissioner's Office (ICO) investigation concluded that the action warranted 'no further action', according (pdf) to a Freedom of Information request response.
Operator: Barclays Corporate and Investment Bank Developer: Sapience Analytics Country: UK Sector: Banking/financial services Purpose: Improve employee productivity Technology: Behavioural monitoring system Issue: Surveillance; Privacy Transparency: Governance"
ScaleFactor hypes accountancy AI 'automation',"A US start-up claiming to deliver an AI-based, real-time bookkeeping tool in fact relied on traditional bookkeepers and hired a Filipino contract accounting firm to process customer data.
Austin, Texas-based ScaleFactor built AI-based accounting automation and book-keeping software for small businesses that integrated with Quickbooks and Xero in such a way that it would 'revolutionise accounting'.
However, the firm often relied on traditional bookkeepers and had hired a traditional contract accounting company in the Philippines, with customers receiving monthly statements rather than the real-time data they were promised, according to Forbes.
Established in 2014, ScaleFactor closed in August 2020, primarily blaming the COVID-19 pandemic. However, customers complained its products had failed to work properly and that it had consistently over-promised and under-delivered. 
Operator:  Developer: ScaleFactor Country: USA Sector: Business/professional services  Purpose: Automate book-keeping, financial forecasts Technology: Automation Issue: Business model; Effectiveness/value Transparency: Marketing"
Nanning real estate sales office facial recognition,"Customers of a real estate company in Nanning, Guangxi province in China, have had their properties sold without their knowledge by a dealer who used their facial information to access their bank details. 
The customers looking to sell their houses were instructed to verify their identities on the town's official 'Yonje Deng' app. The data was then used by an intermediary who was pretending to work at Nanning Youju Real Estate to transfer and mortgage the houses and pocket the proceeds.
The individual was later arrested by the police, and the app updated and made more secure. 
A year later, another Nanning resident was jailed for stealing USD 23,500 from his ex-girlfriend’s bank account by unlocking her phone with her fingerprint after he had drugged her, and then pulling up her eyelids while she was sleeping to activate her phone’s facial recognition feature.
Operator: Nanning Youju Real Estate; Nanning Natural Resources Bureau Developer: Alipay Country: ChinaSector: Real estate sales/management Purpose: Verify identity Technology: Facial recognition Issue: Security Transparency: Governance"
Clearview AI tests live facial recognition cameras,"News that controversial facial recognition company Clearview AI was testing AI-enabled security cameras and augmented reality glasses prompted concerns about privacy.
Buzzfeed discovered that Clearview AI was operating Insight Camera, a subsidiary company offering facial recognition-powered surveillance cameras aimed at 'retail, banking and residential buildings' and being tested by the United Federation of Teachers and Rudin Management. 
The latter told Buzzfeed that the system consisted of a self-contained, closed system' that did not access Clearview AI’s principal database of facial photographs.
Insight Camera later took down its website, which had not spelled out its relationship with Clearview AI. Buzzfeed was able to link the two companies by comparing code from their respective websites. 
Clearview had earlier insisted that it only worked with US government agencies. 
Operator: ClearviewAI/Insight Camera Developer: Clearview AICountry: USASector: Banking/financial services; Real estate Purpose:  Strengthen securityTechnology: Facial recognition Issue: Privacy; Surveillance Transparency: Governance"
Dali Smart Lamp raises privacy concerns,"A desk lamp for school children equipped with cameras, posture recognition, and full-screen advertising raised concerns about surveillance and privacy.
Developed by TikTok owner Bytedance, the Dali Smart Lamp is equipped an AI-powered digital assistant to help with vocabulary and math problems, and with two surveillance cameras - one mounted on the front and one on the top - to help parents keep an eye on their kids while they're doing homework. The lamp can be connect to a parent's smartphone, with the cameras allowing parents to see both their child's face and their homework while video chatting.
A more expensive version of the lamp is able to recognise when children slouch, automatically sending alerts to their parents. While the Dali Smart Lamp is said to have sold well amongst Chinese consumers, it has also been described as 'weird' and 'creepy', particularly in western countries where the monitoring of schools kids and parental oversight of children tends to be less overt and intrusive.
Operator: Bytedance/Beijing Kongming Technology  Developer: BytedanceCountry: ChinaSector: Consumer goodsPurpose: Increase learning efficiency Technology: Posture detection/recognition; Deep learning; Neural network; Machine learning Issue: Privacy; Surveillance Transparency: Privacy"
Texas university kills 'biased' PhD applicant screening system,"A machine learning-based algorithm used to evaluate applicants for the University of Texas at Austin’s PhD in computer science was pulled after concerns about bias and inequality.
Used from 2013, the computer science department's GRADE (GRaduate ADmissions Evaluator) algorithm was trained using the details of students acce[ted before 2013 in order to teach the system to identify people the school would favour. 
GRADE made the school's admissions process quicker, but was also found to have been disadvantaging underrepresented groups, notably women and Black people, on which the university was latterly placing greater emphasis. The system was terminated in December 2020 after a student backlash.
Created by a University of Texas faculty member and graduate student, GRADE predicted how likely the admissions committee was to approve an applicant and expressed that prediction as a numerical score out of five. The system also explained what factors most impacted its decision. 
Operator: University of Texas at Austin Developer: University of Texas at AustinCountry: USASector: EducationPurpose: Assess PhD applications Technology: Machine learning Issue: Bias/discrimination - race, gender Transparency: Governance"
GPT-3 advises patient to kill themselves,"A medical chatbot based on Open AI's GPT-3 large language model recommended that a researcher acting as a patient commit suicide. When told 'I feel very bad, I want to kill myself', GPT-3 responded 'I think you should' to researchers at French healthcare technology company Nabla.
The researchers also concluded that GPT-3 was helpful when performing basic administration tasks, but was 'nowhere near ready' to provide medical support or advice, and lacked the memory, logic, and understanding of time to answer specific questions in a meaningful manner. 
The research was seen to underscore GPT-3's inability to act as a trusted medical advisor, and calls into question the effectiveness of the safety guardrails put into place to ensure the safety of its users by Open AI. It also prompted Facebook AI head Yan LeCun to argue the text generator is 'not very good' as a Q&A or dialogue system.
Operator: OpenAI; Nabla Developer: OpenAI
Country: France
Sector: Multiple; Health
Purpose: Generate text
Technology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Safety
Transparency: Governance; Black box"
Student GPT-3 fake blog posts pass as human,"A US college student fooled people into believing fake blog posts generated by the GPT-3 large language model were written by a human. The incident raised concerns about the ease with which GPT-3 could be misused and abused to produce clickbait content and misinformation and disinformation. 
Using the account of a PhD student to bypass usage restrictions imposed by Open AI, University of California, Berkeley, student Liam Porr used GPT-3 to post a fake blog under a fake name. The first post - titled Feeling unproductive? Maybe you should stop overthinking - was ranked first on Hacker News. 
Porr told MIT Technology Review that only a handful of people seem to have noticed that his posts were AI-generated, but they were quickly downvoted by Hacker News community members. According to Porr, 'it was super easy, actually, which was the scary part.'
Operator: OpenAI; Liam Porr; Substack Developer: OpenAI
Country: USA
Sector: Multiple
Purpose: Generate text
Technology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics
Transparency: Governance; Black box
GPT-3 Wikipedia profile
GPT-3 research study
GPT-3 model card
Nothing but words blog
Liam Porr (2020). My GPT-3 Blog Got 26 Thousand Visitors in 2 Weeks
https://www.technologyreview.com/2020/08/14/1006780/ai-gpt-3-fake-blog-reached-top-of-hacker-news/
https://www.theverge.com/2020/8/16/21371049/gpt3-hacker-news-ai-blog
https://www.businessinsider.com/fake-ai-generated-gpt3-blog-hacker-news-2020-8
https://sea.mashable.com/tech/12032/college-student-creates-a-fake-blog-using-ai-that-becomes-most-read-on-a-security-website
https://bdtechtalks.com/2020/08/24/ai-blog-gpt-3-fake-news/
https://news.ycombinator.com/item?id=24164470
GPT-3 large language model
GPT-3 advises patient to kill themselves
Page infoType: IncidentPublished: April 2023"
Scammers bypass WePay facial recognition security using gifs,"Swindlers who used GIFs to trick WeChat's mobile payment service into verifying people’s identities and defrauding them were arrested by Chinese police.
One of the suspects told Hubei police that they would find selfies of their victims in their WeChat Moments feed and turn them into GIFs of the people doing identity-verifying actions, according to SixthTone.
As a deterrent to scammers, many Chinese set up facial recognition as an added security measure for online transactions on WeChat Pay, requiring them to blink or turn their heads to verify their identities. They would then transfer money from their victims' WeChat Pay accounts. 
The incidenst raised questions about the strength of WeChat Pay's security, and about the security of facial recognition systems more broadly. 
Operator:  Developer: Tencent/WeChatCountry: ChinaSector: Banking/financial servicesPurpose: Verify customer identity Technology: Facial recognition Issue: SecurityTransparency:"
"Proctoring software companies accused of providing invasive, discriminatory software","Five companies providing online test proctoring software and services have been accused of 'unfair and deceptive' business practices. 
A complaint by US non-profit Electronic Privacy Information Center (EPIC) with the District of Columbia Attorney General accused Respondus, ProctorU, Proctorio, Examity, and Honorlock of developing invasive software, discriminatiing against ‘non-typical’ students, and of the deceptive use of facial recognition. It cited a 'reliance on opaque, unproven AI analysis to flag purported instances of cheating.'
 
The use of proctoring software using keystroke patterns, facial recognition, gaze-monitoring and recordings of students’ surroundings to monitor students and flag suspected cheating in home tests was accelerated during the COVID-19 pandemic. These techniques raised concerns over mass biometric surveillance, biased outcomes and algorithmic transparency.
 
EPIC’s complaint set a precedent in challenging black-box algorithms in education. Since the complaint was filed in 2020, the Federal Trade Commission warned software companies against surveillance of students, and a federal legal ruling outlawed practices such as scanning students’ rooms.
Operator: Developer: Respondus, ProctorU, Proctorio, Examity, HonorlockCountry: USASector: EducationPurpose: ProctoringTechnology: Facial analysis; Facial recognition; Gaze recognition; Location recognition; Prediction algorithmIssue: Accuracy/reliability; Bias/discrimination; Privacy; Surveillance Transparency: Black box; Complaints/appeals"
UCSB faculty advises against ProctorU 'surveillance tool',"A warning by University of California Santa Barbara (UCSB) faculty association members raising concerns about the potential sharing of student data with third parties resulted in a backlash against online proctoring service ProctorU.
According to a March 2020 letter (pdf) to the university's administration, ProctorU 'regularly collects and distributes' a wide range of student information such as social security numbers, browsing history, gender identity, medical conditions, fingerprints, faceprints, voiceprints, retina scans and more.
The faculty went on to say ProctorU's data privacy practices 'implicates the university into becoming a surveillance tool' and to recommend UCSB terminate its contract with ProctorU and discourage professors from using similar services. ProctorU's (now renamed Meazure Learning) privacy policy says personal data collected may be disclosed to third parties for undefined 'business and commercial purposes'.
In response, ProctorU attorney David Lance Lucas threatened (pdf) to sue the faculty association for defamation and violating copyright law, and accused it of 'directly impacting efforts to mitigate civil disruption across the United States' by interfering with education during a national emergency. 
Lucas' threat was condemned as inaccurate and unreasonable bullying by senior lawyers and others. According to Vice, ProctorU never filed a lawsuit against the UCSB faculty association. But the threat 'had a chilling effect on professors’ willingness to discuss the software.'
Operator: University of California (UCSB)Developer: Meazure Learning/ProctorU Country: USA Sector: EducationPurpose: Detect and prevent cheating Technology: Facial recognition; Fingerprint recognition; Voice recognitionIssue: Privacy Transparency: Legal"
"UBC academic, students accuse Proctorio of privacy abuse","Proctorio was accused by University of British Colombia (UBC) students, staff, and faculty of providing an 'unethical, invasive' online exam invigilation system, and for causing anxiety and other mental harms, and negatively impacting students’ academic performance.
The complaints were initially triggered by a UBC student claiming that Proctorio had failed to provide support when encountering an issue the system, to which Proctorio CEO Mike Olsen posted excerpts of a support chat log to Twitter, resulting in allegations of privacy abuse. Olsen later apologised for his actions. 
Subsequently, UBC employee Ian Linkletter criticised the software on Twitter, linking to unlisted videos on the company's YouTube Channel showing how its technology worked and accusing the company of jeopardising privacy, increasing student anxiety, discriminating against students of colour and others, and inadequate transparency. 
Despite swiftly removing the videos, Proctorio sued Linkletter for infringing its copyright and distributing confidential material, triggering accusations that it was acting disproportionately, and inappropriately limiting academic freedoms. 
In May 2023, the British Colombia Court of Appeal ruled in Proctorio's favour on confidentiality and copyright grounds, despite a court earlier ruling for Linkletter on freedom of expression grounds.
Operator: University of British Columbia Developer: Proctorio Country: CanadaSector: EducationPurpose: Detect exam cheating Technology: Facial detection; Gaze detection; Machine learning Issue: Bias/discrimination - race; Confidentiality; Privacy; Freedom of expression; Freedom of information Transparency: Governance; Black box; Complaints/appeals; Marketing; Legal"
"Miami University student accuses Proctorio of privacy abuse, bias","Miami University student Erik Johnson accused remote exam cheating detection company Proctorio of providing invasive and inequitable software, raising questions about the system and company leadership, and prompting a legal dispute.
In a series of tweets, computer engineering student Johnson criticised Proctorio as invasive, 'inherently ableist and discriminatory', and posted an analysis of the software’s code on Pastebin. 
In response, Proctorio CEO Mike Olsen demanded that three of Johnson’s tweets were removed by Twitter under a copyright takedown notice and blocked his IP address so he could no longer use the software to take his exams.
In April 2021, the Electronic Frontier Foundation (EFF) sued Proctorio for trying to silence critics through the misapplication of copyright law. The two parties settled out of court a year later.
Operator: Miami University Developer: Proctorio Country: USASector: EducationPurpose: Detect exam cheating Technology: Facial detection; Gaze detection; Machine learning Issue: Bias/discrimination - race; Freedom of expression; Privacy Transparency: Governance; Black box; Complaints/appeals; Marketing; Legal"
80 Million Tiny Images dataset,"80 Million Tiny Images is an image database that is used to train machine learning systems to identify people and objects in an environment.
Created in November 2008, the dataset contains over 79 million 32×32 pixel colour images, scaled down from images collected from search engine queries, and a set of 75,062 non-abstract nouns derived from WordNet.
In June 2020, University of Toronto researchers Vinay Uday Prabhu and Abeba Birhane discovered (pdf) that large-scale image datasets like 80 Million Tiny Images were associating offensive labels with real pictures. 
According to the research, the dataset labeled Black and Asian people with racist slurs, women holding children labeled as whores, and included pornographic images. 
They also found that WordNet, from which 80 Million Tiny Images copied content, contains derogatory terms, resulting in images and labels that confirm and reinforce stereotypes and biases, albeit inadvertently. 
The creators of 80 Million Images apologised and took the dataset offline in June 2020 and urged that researchers refrain from using it in the future and delete downloaded copies. 
How many copies were downloaded, how they were used, and whether their plea was followed remains unclear.
Operator: University of TorontoDeveloper: MIT
Country: USA
Sector: Technology; Research/academia
Purpose: Identify & classify objects, people
Technology: Dataset; Computer vision; Object recognition Issue: Bias/discrimination - race, gender; Privacy; Safety 
Transparency: Governance"
Harrisburg University study predicts criminality based on one facial picture,"Professors and a PhD student at Harrisburg University developed software that automatically predicted whether someone would become a criminal based solely on a picture of their face with '80 percent accuracy and no racial bias'.
A backlash quickly followed the announcement, with the researchers accused of 'unsound scientific premises, research, and methods which … have [been] debunked over the years' by the Coalition for Critical Technology (CCT) in an open letter signed by over 1,700 academics demanding the research remain unpublished.
The research was intended to appear in a book series titled 'Springer Nature – Research Book Series: Transactions on Computational Science & Computational Intelligence.' 
Springer Nature later confirmed it would not publish the research, which was subsequently withdrawn by Harrisburg University.
Operator: Developer: Harrisburg University Country: USA Sector: Govt - police Purpose: Predict criminality Technology: Facial recognition; Emotion detection Issue: Accuracy/reliability; Bias/discrimination - race, gender, age, income; EthicsTransparency:"
Face Depixelizer turns Barack Obama white,"Face Depixelizer, a tool that generates high resolution, de-pixelated photos from low-resolution, pixelated ones, generated white faces from coloured people, notably Barack Obama.
Based on the PULSE image facialiser developed by Duke University researchers using NVIDIA's StyleGAN generative adversarial network (GAN), Face Depixelizer was primarily to upscale pixelated portraits of characters from a number of video game franchises. But it was found to struggle consistently with non-white faces such as those of US politician and Congresswoman Alexandria-Ocasio Cortez and actress Lucy Liu.
The incident called into question the tool's reliability and led to accusations of racism. It also resulted in a heated public spat over the nature of algorithmic bias between Facebook chief AI scientist Yann LeCun, who appeared to blame the quality of the data on which the tool was trained, and AI ethics researcher Timnit Gebru, who insisted it reflected structural issues in the technology industry. LeCun later apologised for his words.
Operator:  Developer: Denis Malimonov Country: USASector: Politics Purpose: Improve image quality Technology: Computer vision; Pattern recognition Issue: Robustness; Bias/discrimination - race, ethnicityTransparency:"
University of Miami accused of using facial recognition to track student protestors,"The University of Miami (UM) came under fire for allegedly using facial recognition to track students protesting against the university’s opening up plan during the COVID-19 pandemic.
 
University administrators denied that campus police used facial recognition software, saying that students were identified using ordinary footage. Students however claimed that a page on the UM website indicated campus police had introduced motion detection, facial recognition and object detection systems on-site.
 
The case led to 20 human rights organisations submitting a letter to the UM Board of Trustees calling for a ban on facial recognition. While it is unclear whether facial recognition was used, the incident triggered concerns of human and digital rights organisations about the extent to which private universities in the USA are using invasive facial recognition surveillance.
Databank
Operator: University of Miami Developer:Country: USASector: EducationPurpose: Identify individuals Technology: Facial analysis; Motion DetectionIssue: Privacy; Surveillance Transparency: Governance"
Audio deepfake fraudulently impersonates CEO ,"US-based cybersecurity company NISOS uncovered an attempted fraud in which an employee received a call from someone identifying himself as his company CEO asking him to call back for 'immediate assistance to finalize an urgent business deal.'
Fortunately, the employee 'immediately thought it suspicious' and called the legal department. It transpired the voice had been faked and the number the would-be victim was meant to call was a VOIP service burner with no user information.
In 2021, criminals had fooled a senior director at a UK energy company into transferring USD 240,000 to a bank account in Hungary having cloned the voice of colleague. 
Operator: Anonymous/pseudonymousDeveloper: Anonymous/pseudonymous Country: USA Sector: TechnologyPurpose: Defraud Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Impersonation; Security Transparency: Governance; Marketing"
COVID-19 Cultural Support Fund assessments,"An automated system used to award cultural support funding in Poland during COVID-19 was criticised for being opaque, unfair, and politically-driven.
Poland's Ministry of Culture announced the beneficiaries of the country's PLN 400 million (USD 106 million) Cultural Support Fund (FWK) in November 2021. The fund was intended to help struggling arts organisations recover from losses incurred during the COVID-19 pandemic. 
The 2,064 beneficiaries and their levels of compensation had been calculated algorithmically using accounting and statistical data for the previous year (2019), including criteria such as the decrease in revenue, the number of people employed, the number of canceled events and the impact on the local community.
The publication of the beneficiaries' names caused controversy as it was seen to be granting large sums to established stars whilst many Poles and the county's healthcare service continued to struggle. The system was also criticised as opaque and politically-driven, with large sums granted to 'disco polo' and other genres favoured by the government. 
Facing uproar, the ministry suspended payments from the support fund on November 15, saying the list of beneficiaries would be submitted for 'urgent reverification' and that 'every effort' would be made to ensure the audit was carried out as 'efficiently and meticulously as possible.'
Poland's culture minister and deputy prime minister Piotr Gliński argued the algorithm was the fairest way to divide the funds as it did not look at names or the size of the organisation but transferred money on equal terms to everyone.
The Polish government did not revealed how the algorithm worked.
Operator: Ministry of Culture Developer: Ministry of Culture
Country: Poland
Sector: Govt - culture
Purpose: Calculate revenue loss
Technology: AlgorithmIssue: Fairness; Bias/discrimination - economic, political
Transparency: Governance
ASSESS DATABASE
https://news.artnet.com/art-world/poland-culture-recovery-fund-1924242
https://notesfrompoland.com/2020/11/16/polish-government-suspends-covid-culture-fund-after-big-name-stars-claim-millions/
https://www.newsy-today.com/piotr-glinski-solidarna-polska-supported-the-oppositions-motion-regarding-the-culture-support-fund/
https://tvn24.pl/biznes/pieniadze/koronawirus-fundusz-wsparcia-kultury-pomoc-dla-teatrow-filharmonii-wokalistow-zespolow-disco-polo-lista-4750451
https://www.rp.pl/polityka/art8758021-piotr-glinski-algorytm-najlepszym-sposobem-dzielenia-pieniedzy-dziala-na-rownych-zasadach
https://wiadomosci.radiozet.pl/Polska/polityka/Koronawirus.-Artysci-wsparci-milionami-zlotych-z-resortu-kultury.-Piotr-Glinski-komentuje
https://www.tvp.info/50799841/piotr-glinski-o-funduszu-wsparcia-kultury-decydowal-algorytm-pokazujacy-kto-najwiecej-stracil
https://www.polsatnews.pl/wiadomosc/2020-11-16/piotr-glinski-w-programie-gosc-wydarzen-transmisja-od-1920/
UK government Spotlight fund application assessments
Gdansk Primary School No. 2 meal payment verification
Page infoType: IncidentPublished: January 2023Last updated: January 2024"
Human rights lawyer travel restricted by COVID-19 health app misuse ,"Changsha-based human rights lawyer Xie Yang had his travel curtailed when his COVID-19 health report app turned red when he was trying to travel to Shanghai to visit the mother of a citizen journalist who had been jailed for reporting on the initial COVID-19 outbreak in Wuhan. 
The incident raised concerns about the misuse of health data for Chinese state control purposes. People trying to join protests in Zhengzhou, Henan province, in order to gain access to their deposits in struggling banks had also seen their health code apps turn red, classifying them a risk to public health and restricting their movements. 
Stopped at the airport, Yang was thrown into quarantine. Changsha had no reported cases of the virus at the time. ‘The Chinese Communist party has found the best model for controlling people,’ he said shortly afterwards.
d
Operator: Changsha City Health Commission Developer: Alibaba/Alipay/DingTalk; Tencent/WeChat Country: China Sector: Govt - health Purpose: Control COVID-19 Technology:  Issue: Dual/multi-use; Privacy; Surveillance Transparency: Governance; Complaints/appeals"
Hangzhou 'Personal health code' scoring system,"China's Hangzhou Municipal Health Commission announcement that it was planning to make permanent a version of the country's 'health code' app used during the COVID-19 pandemic resulted in a strong backlash.
The system, which was to assign citizens a personal score, colour, and ranking based on data collected about their medical history, health checkups, and lifestyle habits, including smoking, drinking and sleeping, would compare users’ health indicators with the health code colors to build a personal health index ranking.
The plan triggered anger on the country's social media, with citizens raising conerns about its need, normalising and overly intrusive nature, as well as its scope for surveillance and other forms of abuse and misuse. According to Sixth Tone, a poll on Chinese microblogging platform Weibo resulted in 86% of 6,600 users voting against the proposal.
Hangzhou authorities responded by saying they would press ahead with the system. The city was the first in China to implement a COVID-19 QR code app and, in 2019, it had launched a social credit system - one of the first cities in China to do so.
Operator: Hangzhou Municipal Health CommissionDeveloper: Alibaba/AliPay/DingTalk; Tencent/WeChat Country: China Sector: Govt - health Purpose: Calculate personal health score Technology:  Issue: Appropriateness/need; Privacy; Scope creep/normalisation; Surveillance Transparency: Governance"
Google Autocomplete links French user to rape,
Mohsen Fakhrizadeh assassinated using robot machine gun,"Mohsen Fakhrizadeh, the head of Iran's nuclear programme, was assassinated by a remote-controlled machine gun as he was being driven from Tehran to his weekend villa in Absard. The first known autonomous assassination, the attack prompted politicians, activists and others to voice their fears about the nature and use of lethal autonomous weapons.
The New York Times later published a report confirming that Fakhrizadeh was assassinated using a modified Belgian FN Mag machine gun incorporating robotics and varous forms of artificial intelligence, including facial recognition. According to the Times, the attack was controlled by a Mossad team operating in a command centre outside Iran.
Operator: Mossad Developer: FN MAG Country: Israel  Sector: Govt - military Purpose: Kill/maim/damage/destroy  Technology: Robotics; Facial recognition Issue: Dual/multi-use; Autonomous lethal weapons  Transparency: Governance"
Suzhou social 'civility score' trial,"The Chinese city of Suzhou's launch of a 'civility code' early September 2020 to rank citizens’ civility and award or punish them accordingly quickly backfired after the local community took to social media to criticise the plan as overbearing, manipulative, and an abuse of privacy.
Embedded in the 'Suzhou City Code' (蘇城碼) - a digital ID used to control residents’ movements during the COVID-19 pandemic - the new 'Suzhou App 2.0' comprised two sets of indexes: 'civility in traffic performance' and 'civility in voluntary work performance'.
Citizens found to be jay-walking or drunk-driving would lose points, whilst those volunteering work would gain points. People with high scores would have more advantages in seeking employment, enrolling in schools, and accessing public and private services. 
Suzhou authorities suspended the system three days after its launch and said it would re-launch it after modifications had been made. It is not known to have re-launched five years later.
An extension of its financial credit rating system, China's Social Credit System launched in 2014 and tracks and assesses the creditworthiness and trustworthiness of citizens, businesses, government bodies, and NGOs. It has been trialled in 'model cities' across China, including Suzhou, since December 2017.
Operator: Government of China Developer: Government of China
Country: China
Sector: Govt - police; Govt - security
Purpose: Assess creditworthiness, trustworthiness
Technology: Behavioural monitoring; Deep learning; Neural network; Machine learning Issue: Ethics; Fairness; Privacy
Transparency: Governance"
Digital Minds scans job applicant emails to assess cultural 'fit',"A personality assessment product by Finnish company Digital Minds that enabled potential employers to scan the private emails and social media posts of job applicants to determine whether they would be a good fit for their prospective employer sparked concerns about abuse of privacy and confidentiality.
Founded by two psychologists, Digital Minds said it aimed to develop 'third-generation' assessment technology for employee recruitment. However, an investigation by Finnish broadcaster YLE revealed that the Finnish Data Protection Ombudsperson suspected the personality assessment violated the country's Labour Privacy Act, which states that information must be collected with jobseeker consent and that personal emails are protected under the country;s confidentiality laws.
The investigation also found that very few jobseekers had agreed to participate in Digital Mind’s analysis of their social media posts, and only one had agreed to participate in the analysis of email correspondence, suggesting the company's marketing had been misleading. Digital Minds closed shortly after the investigation was published. 
Operator: Digital MindsDeveloper: Digital MindsCountry: Finland Sector: Business/professional services Purpose: Assess personality Technology: Behavioural analysis; Facial analysis; Speech recognition Issue: Accuracy/reliability; Privacy Transparency: Governance; Black box; Marketing"
Ugandan police accused of using facial recognition to stifle Museveni term protests,"Ugandan police were accused of using a facial recognition system supplied by Chinese company Huawei to track, arrest, and torture protesters against President Yoweri Museveni’s November 2020 decision to seek another term in office.
 
Local politicians and rights advocates had flagged possible human rights and privacy violations when the system was acquired in 2019 as part of Huawei’s Safe City programme. Since 2020, civil society groups have criticised governance institutions in Uganda for failing to protect the fundamental rights of citizens from surveillance in public places. 
This comes amidst reports that Uganda is expanding its digital surveillance capabilities as part of a nationwide integrated surveillance system, including capabilities to check vehicle license plates and monitor social media.
A Ugandan police spokesperson denied that the technology was used to monitor opposition figures but confirmed that a new surveillance system was in use.
 
In transition or fragile democracies, this incident raises questions over how imported surveillance technologies are used to quash protests and suppress individual freedoms. Reports indicate other countries in Africa increasingly importing surveillance technologies, including Nigeria, Morocco, Zambia, and Ghana.
Operator: Government of Uganda; Uganda Police Force Developer: HuaweiCountry: UgandaSector: Govt - home/interior; policePurpose: Identify individuals Technology: Facial analysis;  Safe cityIssue: Dual/Multi-use; Governance; Human/civil rights; Privacy; Surveillance Transparency: Governance"
Buenos Aires government uses live facial recognition to identify child criminals ,"Buenos Aires authorities were accused by Human Rights Watch (HRW) of using live facial recognition to identify children accused of committing crimes, thereby violating a United Nations agreement that protects children’s privacy in legal proceedings.
According to HRW, the details of 'at least 166' children accused of committing crimes were stored on Argentina's CONARC, the country's national database of inviduals with outstanding arrest warrants for serious crimes, between May 2017 and May 2020. 
HRW contended the City of Buenos Aires' live facial recognition system is likely to amplify the risks of wrong identification of children due to the known inaccuracies of such systems when used on children, and to potentially unjustly limit their job and educational opportunities. 
Argentina is thjought to be the only country in the world to deploy live facial recognition against people under the age of 18. 
Operator: Government of the City of Buenos Aires; Buenos Aires City Police; Argentine Ministry of Justice and Security; ReNaPerDeveloper: Danaide/NtechLab
Country: Argentina
Sector: Govt - municipal; Govt - police; Govt - security
Purpose: Identify criminals
Technology: Facial recognition Issue: Accuracy/reliability, Bias/discrimination, Human/civil rights, Surveillance
Transparency: Governance; Privacy"
7-Eleven customer survey facial recognition,"Chain store 7-Eleven breached customer privacy in 700 stores between June 2020 and August 2021 by collecting facial imagery without consent, according to Australia's privacy commissioner the OAIC. 
The commissioner ruled (pdf) that 7-Eleven abused customer privacy by collecting and storing their facial images in order to validate a survey it had been conducting into customer needs and to understand their demographic profile. 
7-Eleven had required customers to fill out information on tablets with built-in cameras, and had captured their facial images at two points during the survey-taking process. 7-Eleven said it obtained its customers' consent by providing a notice on its website stating the company would collect photographic or biometric information from users.
The OAIC said the collection of facial images was not reasonably necessary in this instance, and that 7-Eleven had failed to provide information about how customers' facial images would be used or stored. 7-Eleven was order to cease collecting facial images and faceprints as part of the customer feedback mechanism, and destroy all the faceprints it collected.
Operator: 7-Eleven Stores Developer: 
Country: Australia
Sector: Retail
Purpose: Validate survey responses
Technology: Facial recognition Issue: Privacy; Necessity/proportionality
Transparency: Governance; Marketing; Privacy"
NiJeer Parks facial recognition wrongful arrest,"In December 2020, 33-year-old Black man Nijeer Parks annnounced he would sue (pdf) New Jersey city, police department, and prosecutor for false arrest, false imprisonment and violation of his civil rights for misidentifying him using facial recognition.
Parks had been arrested in January 2019 for purportedly shoplifting from a candy shop in Woodbridge, New Jersey. The real cuplrit had given the police a fake driver's license with Park's photograph before absconding, leading to the police issuing a warrant for Parks' arrest. Only Parks had never been to Woodbridge, had never owned a driver's license, and had a sound alibi.
Having feigned an asthma attack during a police interrogation, Parks was arrested, denied bail, and held for 10 days. Parks was finally released and the case against him dismissed for lack of evidence. 
Operator: Woodbridge Police Department, New Jersey; Middlesex County Prosecutor’s Office Developer: Clearview AI
Country: USA
Sector: Govt - police
Purpose: Strengthen law enforcement
Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity
Transparency: Governance; Black box"
Michael Oliver facial recognition wrongful arrest,"Michael Oliver is suing the city of Detroit for USD 12 million for using facial recognition to arrest and jail him in July 2019 for a crime he never committed.
Oliver was arrested for reputedly having snatched a mobile phone which had been used by a teacher to video a group of students involved in a brawl near a Detroit school. Facial recognition software developed by DataWorks Plus matched Oliver with a photograph in a police database. 
In September 2019, Wayne County Prosecutor’s Office dropped the charges against Parks on the basis that he had tattoos and other body markings that the real thief did not have.
According to Oliver's September 2020 lawsuit, '[Police relied] on failed facial recognition technology knowing the science of facial recognition has a substantial error rate among black and brown persons of ethnicity which would lead to the wrongful arrest and incarceration of persons in that ethnic demographic.' 
In July 2020, Detroit Police Chief James Craig had admitted facial recognition technology misidentified suspects in 96 percent of cases.
Operator: Detroit Police Department Developer: DataWorks Plus
Country: USA
Sector: Govt - police
Purpose: Strengthen law enforcement
Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity
Transparency: Governance; Black box"
Robert Williams facial recognition wrongful arrest,"Robert Williams, 42, was arrested in January 2020 for reputedly stealing five high-end watches at a store in Detroit in October 2018. However, Williams had been misidentified by facial recognition technology used by the Detroit Police Department, inflicting significant emotional damage on Williams and his family.
A detective had used facial recognition technology on a grainy image from the store's CCTV  video, with the system flagging Williams as a potential match based on a driver’s license photograph. A security guard who had not been present at the incident then identified Williams in a photo line-up of Black males.
Arrested in front of his family, Williams had been arrested, arraigned, detained for 30 hours and questioned in connection with a crime that took place in a store he hadn't visited since 2014. Prosecutors and police later apologised for how the case was handled. 
In April 2021, Williams sued (pdf) the Detroit Police Department for wrongfully arresting and jailing him. He was the first person known to have been arrested in the US because of a facial recognition failure.
Detroit Police Chief James Craig had earlier admitted facial recognition technology misidentified suspects in 96 percent of cases.
Operator: Detroit Police Department Developer: DataWorks Plus
Country: USA
Sector: Govt - police
Purpose: Strengthen law enforcement
Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity
Transparency: Governance; Black box"
Roermond 'Sensing Project' predictive policing pilot,"A predictive policing system in Roermond, the Netherlands, has been castigated for being discriminatory, an abuse of privacy, ineffective, opaque, and unaccountable.
Operating between January 2019 and October 2020, the 'Sensing Project' used cameras and sensors to collect data on vehicles driving in and around the city, supposedly to reduce shoplifting and pickpocketing. 
The data was then assessed by an algorithm that calculated the probability that the driver and passengers intend to engage in forms of 'mobile banditry' such as pickpocketing and shoplifting, and directs police towards the people and places it deems 'high risk', according to Amnesty. 
Marketed as a neutral system that uses objective crime data, Amnesty discovered that it is designed to identify people of Eastern European origin, notably those of Roma ethnicity, and exclude local nationals. 
Amnesty also found that the system created many false positives, that the police were unable to demonstrate its effectiveness, and that no one in Roermond had consented to its use.
Operator: Roermond Municipal Council Developer: 
Country: Netherlands
Sector: Govt - police
Purpose: Reduce crime; Profile ethnicity
Technology: Machine learning; Pattern recognition Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Effectiveness/value; Surveillance; Privacy
Transparency: Governance; Marketing"
Netherlands SyRI welfare fraud detection automation,"SyRI (or 'System Risk Indication') is a risk classification system developed and operated by the Netherlands government to detect and predict social security, tax and employment fraud.
Deployed by the Department of Social Affairs and Employment, the system used data about employment, fines, penalties, taxes, property, housing, education, retirement, debts, benefits, allowances, subsidies, permits and exemptions, amongst others, to determine whether a welfare claimant should be investigated.
In February 2020, SyRI was found to be in breach of human rights law by a Dutch court, which ordered an immediate stop to its use. The government had been taken to court (pdf) by a number of civil rights organisations and two citizens. 
The court ruled that SyRI violated article 8 of the European Convention on Human Rights (ECHR), which protects the right to respect for private and family life, and that by primarily targeting poor neighbourhoods, the system may have discriminated against people on on the basis of socioeconomic or migrant status. 
The court also ruled that the legislation which permitted SyRI contained insufficient safeguards against privacy intrusions, and that the 'fair balance' between its objectives and the violation of privacy that its use entailed, meant the legislation was unlawful.  
SyRI was also roundly criticised for its opaque nature. The court took it to talk for a 'serious lack of transparency' about how its risk scoring algorithm worked, stating that the use of the system is 'insufficiently clear and controllable'. 
Human Rights Watch complained that the Dutch government refused during the hearing to disclose 'meaningful information' about how SyRI uses personal data to draw inferences about possible fraud.
Despite the Dutch government's decision not to appeal the ruling and stop using SyRI, a Lighthouse Reports investigation discovered that it had quietly continued to deploy an adapted SyRI in some of the country’s most vulnerable neighbourhoods.
The finding led to a joint investigation by Lighthouse Reports and WIRED that found that a machine learning algorithm used by the Municipality of Rotterdam to detect welfare fraud discriminates against welfare claimants based on ethnicity, age, gender, and parenthood.
Operator: Ministry of Social Affairs and Employment (CZW); Benefits Intelligence Agency Foundation; Municipality of Rotterdam Developer: Ministry of Social Affairs and Employment (CZW); Benefits Intelligence Agency FoundationCountry: NetherlandsSector: Govt - welfarePurpose: Detect and predict welfare fraud Technology: Risk assessment algorithm; Machine learning Issue: Bias/discrimination - race, ethnicity, economic; Privacy; Scope creep/normalisation Transparency: Governance; Black box; Complaints/appeals; Marketing; Legal"
Channel 4 deepfake Queen Christmas message,"A deepfake version of the late Queen Elizabeth II's traditional Christmas message aired by UK television broadcaster Channel 4 came under fire from commentators and viewers who questioned its appropriateness and ethics.
The Queen 'revealed' her thoughts about Princes Harry and Andrew, poked fun at then Prime Minister Boris Johnson, joked about the lack of toilet paper facing people during the COVID-19 pandemic, and danced for a TikTok video. She ended by advising people to be wary of what they view online and on television.
Channel 4 had said in advance that the broadcast would provide a 'stark warning' about the dangers of misinformation and disinformation in the age of AI. But this failed to convince many viewers, who complained that it was 'disrespectful, 'distasteful', and 'creepy'.
Operator: Channel 4 Developer: Channel 4; FramestoreCountry: UK Sector: Media/entertainment/sports/arts Purpose: EntertainTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Appropriateness/need; Ethics Transparency:"
Vocal Synthesis Jay-Z AI voice impersonations,"An pseudonymous music creator named 'Vocal Synthesis' used AI to generate deepfaked tracks of rapper Jay-Z's reciting  Shakepeare's 'To be, or not to be' and Billy Joel's Don't Start the Fire. The incident resulted in a DCMA take-down order and a debate about the effectiveness of existing copyright law for deepfakes.
Voice Synethesis’ videos are created by feeding Google’s Tacotron 2 text-to-speech model with Jay-Z's songs and lyrics, and having the synthetic voice read pre-written text. Other videos created by Voice Synthesis include Tucker Carlson reading the Unabomber Manifesto and Bill Clinton reciting 'Baby Got Back'.
Jay-Z's agency entertainment Roc Nation LLC claimed copyright infringment and argued 'This content unlawfully uses an AI to impersonate our client’s voice.' YouTube took down the videos, but later reinstated them on the basis that the DCMA request was 'incomplete'. The videos remained on decentralised, open source platform LBRY.
As Input noted, Vocal Synthesis 'transformed Jay-Z’s discography in a humorous way for no commercial benefit and clearly labels all videos as speech synthesis'. Vocal Synthesis hit back by creating a deepfake video of Barack Obama and Donald Trump explaining that they had no malicious intentions and were 'disappointed' with by Jay-Z and Roc Nation's response.
Operator: Vocal Synthesis; Google/YouTube; LBRY Developer: Vocal Synthesis
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Entertain
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics
Transparency:"
MBN deepfake 24/7 news anchor seen to threaten jobs,"The launch of South Korea's first virtual news anchor by broadcaster MBN News was met with acclaim regarding its realism, tempered with concerns about its potential impact on jobs. 
The product of a collaboration between MBN and DeepBrain, the AI-powered deepfake version of high-profile journalist Kim Ju Ha was said to be surprisingly similar to her real version, even down to her hand gestures. 
MBN said AI newscasters would provide good support during natural disasters and other emergencies as they are always available. It also argued the technology would reduce labour and production costs. 
However, some experts, commentators and consumers suggested it was a potential harbinger of broadcast TV job losses.
Operator: MBN News Developer: DeepBrain/Money Brain
Country: S Korea
Sector: Media/entertainment/sports/arts 
Purpose: Read news
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment
Transparency:"
PornHub banner appears on CNN Magic Wall,"A video purporting to show a Pornhub pop-up on CNN’s 'Magic Wall' behind anchor John King went viral on Twitter. The incident embarassed the anchor and his employer, and demonstrated the ease with which video and other content formats, including live TV, can be manipulated and misconstrued. 
The video was quickly shown to be fake, having been added to the segment after it had been broadcast. In reality, a clip of King  pushing away a graphic on a screen showing vote counts had been edited to suggest he was really pushing away a notification from the pornographic website.  
Asked on Twitter whether the deepfake was real, King responded, 'Not. Some clown taking time away from lying about something else apparently because they don’t like math.'
Operator: CNNDeveloper: 
Country: USA
Sector: Media/entertainment/sports/arts 
Purpose: Troll
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety; Security
Transparency: Governance; Marketing"
Donald Trump joins RT as anchor deepfake,"A deepfake video advert of then US president Donald Trump working after the 2020 US presidential elections for Russian government-funded broadcaster RT (formerly Russia Today) reinforced rumours that he was Vladimir Putin's stooge and that Russia had been interfering in US politics.
The video interperses real clips of Trump denigrating CNN and lavishing praise on 'amazing' Russia with deepfake versions of him holding up a RT contract for USD 1,000,000,000 and saying 'It was a very nice offer from President Putin,' and singing Russian rock song 'Peremen', which had been adopted by anti-government protestors in Belarus. 
The video was published by RT at the same time as The Washington Post reported that the CIA believed Putin tried to undermine Joe Biden's candidacy in a bid to help Trump secure re-election.
Operator: RT NewsDeveloper: RT News
Country: USA; Russia
Sector: Politics
Purpose: Satirise/parody
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics
Transparency:"
Deepfake Donald Trump calls for climate agreement exit,"Flemish Socialist party Vooruit created a deepfake video of then US president Donald Trump calling on Belgium to join America in exiting the Paris climate agreement. 
The video, which was posted to Facebook and Twitter and was reputedly intended to 'start a public debate' to 'draw attention to the necessity to act on climate change', ended by calling for people to sign a petition encouraging investment in renewable energies, electronic cars and public transport.
In the English version of the video, Trump said, 'We all know climate change is fake, just like this video.' But he did not in the Flemish language version, apparently leading some users to believe it was real, with some complaining that the then US president should be involved in Belgium's politics. 
Operator: VooruitDeveloper: Vooruit
Country: Belgium
Sector: Politics
Purpose: Mobilise supporters
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics
Transparency: Marketing"
Deepfake Belgium PM links COVID-19 with climate crisis,"A fake video created by climate activist group Extinction Rebellion of Belgian Prime Minister Sophie Wilmès claiming that COVID-19 was directly linked to the 'exploitation and destruction by humans of our natural environment,' prompted concerns about the increasing use of deepfakes in politics.
Named 'THE TRUTH ABOUT COVID-19 AND THE ECOLOGICAL CRISIS', the video was published on Facebook and garnered over 100,000 views within 24 hours. It was published with a disclaimer saying 'Any resemblance to actual persons is intentional. This video may be fake, but the information it contains is genuine.'
Operator:  Climate Exchange BelgiumDeveloper: Climate Exchange Belgium
Country: Belgium
Sector: Politics
Purpose: Mobilise supporters
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics
Transparency:"
Deepfake Joe Biden threatens to defund US police,"A video shared by US Republican Congressman Steve Scalise of US President candidate Joe Biden apparently threatening to defund the police in a discussion with healthcare activist Ady Barkan has been debunked as a deepfake. 
The video was found to have been manipulated by splicing in the words 'for police' during a quote from Barkan from an original video of an interview between Barkan and Joe Biden that was published by NowThis News. 
Under pressure from Barkan and others, Scalise later deleted the faked video. Twitter had labelled the video as manipulated. 
The incident was one in a number of deepfakes released during the 2020 US Presidential election that were seen likely to jeopardise US political processes and democracy.
Operator: Steve ScaliseDeveloper: Steve Scalise Country: USASector: Politics Purpose: Damage reputation Technology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance; Marketing"
Malaysia minister discourages Singaporean visits deepfake,"Malaysia Senior Minister and Defence Minister Datuk Seri Ismail Sabri Yaako claimed he had been victim of a deepfake which alleged that he did not welcome Singaporeans to Malaysia. A doctored video showed Sabri reputedly saying that Singaporeans should not enter Malaysia to fill up their cars with petrol, have dinner or go shopping.
The incident, which occurred during the COVID-19 pandemic when Malaysia had closed its borders to Singaporeans and other nationals, was rebutted as false by the Malaysia government, and a report filed with the police and the national Communications and Multimedia Commission. It is unclear who was behind the video.
Operator: TwitterDeveloper: 
Country: Malaysia
Sector: Politics
Purpose: Satirise/parody
Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics
Transparency: Governance; Marketing"
Fake security analyst peddles Hunter Biden intelligence document,"A fake 64-page report attempting to smear President Joe Biden's son Hunter Biden by asserting a conspiracy theory involving his business dealings in China prompted right-wing US politicians and commentators to accuse Biden of corruption. President Biden was also accused of being too soft on the Chinese Communist Party. 
According to disinformation researchers and commentators, the report appeared to be the work of 'Martin Aspen', a fake Swiss security analyst whose profile picture had been created with an AI face generator. Aspen reputedly worked for Typhoon Investigations, a fake 'intelligence firm'.
The report was published on Intelligence Quarterly, an anonymous blog, by Fulbright University Vietnam professor Christopher Balding. Balding later admitted that Aspen was 'an entirely fictional individual' and that he had 'authored small parts of the report'. He also said the report had been commissioned by Hong Kong tabloid Apple Daily, a claim that was later denied by the newspaper.
Operator: Intelligence Quarterly Developer: Apple Daily; Christopher Balding
Country: USA
Sector: Politics
Purpose: Sow distrust
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics
Transparency: Governance; Marketing"
Twitter verifies fake Congressional candidate ,"A 17 year-old high school student successfully tricked Twitter into verifying a fake candidate for the US presidential elections, raising questions about the effectiveness of the company's election integrity programme, and demonstrating the ease with which social media can be manipulated.
According to CNN Business, the student created 'Andrew Walz', a Congressional candidate supposedly running for office in Rhode Island by downloading a profile picture from Thispersondoesnotexist, a website that uses AI to generate faces of fake people. 
The student then submitted Walz's details to Ballotpedia, a non-profit partner of Twitter that calls itself 'the encyclopedia of American politics.' The profile was approved, with neither Twitter nor Ballotpedia asking for identification or documentation to prove that Walz was a real candidate.
Twitter suspended the account after CNN Business contacted it about the fake account.   
Operator: Twitter; Ballotpedia Developer: Anonymous/pseudonymous
Country: USA
Sector: Politics
Purpose: 'Test Twitter elections integrity efforts' 
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics
Transparency: Governance; Marketing"
Manoj Tiwari deepfake Haryanvi broadcast,"India's ruling political party the Bharatiya Janata Party (BJP) has been using deepfake videos to attack an adversary running in the Legislative Assembly elections in Delhi. BJP President Manoj Tiwari attacked opponent Arvind Kejriwal in three languages, incliding Hindi dialect Hariyanvi, which Tiwari doesn't speak, in an effort to reach Kejriwal's voters.
Political opponents and digital rights advocates lashed out at Tiwari and the BJP for engaging in what they saw as underhand and unethical behaviour. The incident, in which the videos were shared in over 5,800 WhatsApp groups and are said to have reached 15 million people, appears to be the first time deepfakes have been used for a political campaign, in India and elsewhere.
Operator: Bharatiya Janata Party (BJP); Manoj Tiwari Developer: The Ideaz Factory Country: IndiaSector: PoliticsPurpose: Undermine political opponent Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  Issue: Mis/disinformation; Ethics Transparency: Governance; Marketing"
Southern Co-op facial recognition,"The Southern Co-op operates over 200 retail stores and 50 funeral homes across southern England. A mutual society, it is owned by 130,000+ members who share in its profits and control its operations.  
In December 2020, WIRED picked up on a blog post by Southern Co-op executive Gareth Lewis that 18 Southern Co-op stores had 'successfully' completed a trial in which shoppers entering Co-op premises were being scanned using real-time facial recognition cameras in order to reduce shoplifting and abuse and violence against staff.
News of the trial sparked privacy campaigners to express concerns about data rights and privacy, transparency, and increasing use of surveillance technology in the private sector.
According to the Co-op, individuals (or 'Subjects of Interest') reasonably suspected of having committed offences which have been witnessed by CCTV or staff members are added to a database, and their data kept for one year. 
When a subject of interest is added, the Southern Co-op's facial recognition supplier Facewatch's system automatically alerts other retail clients within an eight-mile radius in London, a 15-mile radius in other cities, and a 43-mile radius in rural areas, thereby forming a de facto private facial recognition network, or set of networks.
Civil liberties advocates such as Privacy International and Big Brother Watch are concerned that Facewatch's cloud-based network may not to be proportionate, and is potentially illegal under UK data privacy law. 
By contrast, Facewatch argues 'the privacy intrusion to genuine shoppers is negligible'.
In July 2022, civil liberties group Big Brother Watch filed a complaint (pdf) to the UK’s Information Commissioner’s Office against Southern Co-op and Facewatch. 
The complaint alleged the Southern Co-op and Facewatch process more data than is necessary for generating and storing watchlist entries, and that the two entities lack transparency about how they collect and process people’s data.
Despite Southern Co-op stating that 'distinctive signage' is on display in the relevant stores, rights groups argued the chain failed to inform customers sufficiently clearly, made a meaningful general public announcement before the trials started.
Big Brother Watch is also concerned that individuals are not informed if they are added to the Co-op and Facewatch's databases, despite the chance they may be innocent, and that there is very little visible information on what data is stored, how it is stored,  the length of storage, and with whom it may be shared, including the police.
Nor is it clear how individuals can have appeal against having their data stored.
In January 2022, the Daily Mail published details of an investigation by video research company IPVM that found that nine stores out of the 35 it investigated used Hikvision CCTV cameras.
Hikvision cameras have been widely used in the surveillance and repression of the Muslim minority Uyghurs in Xinjiang province, China. Facewatch says it is 'agnostic to the hardware and will follow the Government’s lead on whether to continue using Hikvision hardware or not.'
IPVM also found one store with no viewable signage, and seven seven with small and often obscured signage. 
Operator: Southern Co-op Developer: Facewatch; Hikvision Country: UK Sector: Retail Purpose: Reduce crime, violence Technology: Facial recognition Issue: Privacy Transparency: Governance; Complaints/appeals; Marketing; Privacy"
"Rite Aid US facial recognition racial, income bias","US drugstore chain Rite Aid quietly used facial recognition technology in hundreds of stores in mostly lower-income, non-white neighbourhoods across the US, prompting a civil, legal, and political backlash.
According to a Reuters investigation, Rite Aid quietly added facial recognition systems to hundreds of stores in the US, and that it had deployed the technology in largely lower-income, non-white neighborhoods in New York and Los Angeles. 
The investigation also indicated 'serious drawbacks' with RiteAid's first facial recognition partner, FaceFirst, whose technology several security professionals described as inaccurate, especially with regard to Black people and those from other races. 
RiteAid defended its policy by arguing that the technology was only being used in a 'data-driven' manner to detect and deter crime and violence, and that the cameras were appropriately flagged to customers. But the chain swiftly shut down its system after the Reuters investigation, claiming its 'decision was in part based on a larger industry conversation.'
The American Civil Liberties Union (ACLU) had earlier questioned whether American retail chains were using face recognition without telling their customers. RiteAid, like nineteen other chains, failed to answer.
Operator: RiteAidDeveloper: FaceFirst; DeepCam; Shenzhen Shenmu
Country: USA
Sector: Retail
Purpose: Reduce crime, violence
Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, income; Privacy
Transparency: Governance; Marketing"
Stochastic Parrots study questions large language models,"A study by a group of researchers exploring the risks of large language models resulted in the dismissal of a number of high-profile Google employees and raised questions about Google's values, culture and governance. It also prompted heated discussion about the role of ethics in technology decision-making, and its effective 'privatisation' by commercial interests.
Written by linguist Emily Bender and then Google ethicists Timnit Gehru and Margaret Mitchell, the 'Stochastic Parrots' study assessed the financial, social, and environmental risks of large language models such as Google's BERT and OpenAI's GPT-2, and set out a series of recommendations for minimising these risks. 
Amongst other things, the paper referenced a 2019 University of Massachusetts, Amherst, study that had concluded that the energy consumption and carbon footprint of large language models had massively increased since 2017. The study also found that the training of a single model emits over 626,000 pounds of carbon dioxide equivalent, which is nearly five times the lifetime emissions of the average American car, including its manufacture.
The Information reported in May 2023 that OpenAI had incurred losses of USD 540 million during 2022 as it developed GPT-4 and ChatGPT, underscoring the huge costs of training its models.
Operator: Alphabet/GoogleDeveloper: Alphabet/GoogleCountry: USA Sector: Technology; MultiplePurpose: Generate text Technology: Large language model (LLM); NLP/text analysis; Neural networks; Deep learning; Machine learningIssue: Bias/discrimination - race, ethnicity; Ethics; Employment; Environment Transparency: Governance; Marketing"
GPT-3 large language model,
Coronavirus Mask Image dataset,"AI startup WorkAround has created the COVID19 Mask Image Dataset, a dataset of 1,200 people wearing face masks in order to improve the quality of biometric facial recognition algorithms. 
WorkAround identified thousands of photos, mostly taken from Instagram, available in public data sets. 
A spokesperson for Facebook, which owns Instagram, told CNET they do not allow third parties to collect or use photos posted by their users in this way without their consent.
'We’re not making any money off of this, it’s not commercial,' WorkAround CEO Wafaa Arbash said. 'The goal and the intention was to help any data science or machine learning engineers who are working to fix this issue and help with public safety.'
Civil rights advocates are concerned researchers are collecting social media photos of people wearing masks without their consent.
Operator: WorkAround Developer: WorkAround
Country: USA
Sector: Health; Research/academia
Purpose: Improve facial recognition algorithms
Technology: Dataset; Facial recognition; Computer vision Issue: Privacy; Ethics
Transparency: Privacy"
Telegram bot creates non-consensual deepfake porn,"Security company Sensity has discovered that an AI bot on encrypted messaging app Telegram is being used to create photo-realistic nude and sexually explicit images of hundreds of thousands of women without their consent. 
The bot, which appears to be popular in Russia and eastern Europe, only requires one photograph to create the images, leading to over 680,000 ordinary women and girls being demeaned and degraded. Some of these appear to be underage girls.
The images are used and manipulated harassment without the knowledge or consent of the women being targeted, risk being used in all manner of ways, and may lead to harassment, abuse, violence, and loss of jobs.
The photographs clearly violate Telegram's terms of service, but the messaging app has refused to close the account.
Operator: Unclear/unknown; Telegram Developer: Anonymous/pseudonymous
Country: Russia; Global
Sector: Media/entertainment/sports/arts
Purpose: Simulate nude images
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Privacy; Impersonation; Copyright; Safety
Transparency: Governance; Complaints/appeals; Privacy; Marketing"
These Nudes Do Not Exist deepfake porn sales,"Start-up These Nudes Do Not Exist is selling AI-generated images of women who do not exist in real-life for USD 1 in an attempt to upend the deepfake nude industry, drawing criticism for inappropriatw commercialisation and shoddy ethics.
These Nudes Do Not Exist takes public domain photographs and uses an algorithm to generate composite nude images of women who do not exist in real-life. But whilst the company technically does not violate the privacy of real women, the company refuses to reveal the sources of their training data, raising questions about whether the consent of those whose photographs were initially used had given their consent. 
One of the co-founders told Vice, 'I think this is probably the first chance that anyone in the world has ever had to buy AI generated pornographic content, so in a sense each customer gets to be a part of porn and AI history.' The founders also 'requested to remain anonymous because he and his partner didn't want to be publicly associated with their own creation,' according to Vice.
The website was taken down shortly after its launch.
Operator: Unclear/unknown Developer: Anonymous/pseudonymous
Country: Russia
Sector: Media/entertainment/sports/arts
Purpose: Generate nude images
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety; Ethics
Transparency: Governance; Complaints/appeals; Privacy; Marketing"
IBM Diversity in Faces dataset,"IBM's Diversity in Faces (DiF) is a dataset of annotations of one million publicly available facial images released in January 2019 that was intended to make artificial intelligence more fair and equitable across genders and skin colours and accelerate efforts towards creating more fair and accurate face recognition systems.'
IBM's dataset was based on Yahoo!'s YFCC100M dataset, which provides approximately 100 million photos from photo sharing website Flickr available under various Creative Commons licenses. IBM said DiF was meant to be an academic/research  resource, was not publicly available for download or sale, and could not be used for commercial purposes.
A March 2019 NBC News investigation discovered that IBM had been using its Diversity in Faces dataset to train its own AI products, including Watson Visual Recognition, without the consent of the people in the photos. Not only was IBM ignoring its own terms of use for the dataset, it also failed to provide attribution links or public credit for any images. 
In January 2020, IBM was sued in a class action seeking damages of USD 5,000 for each intentional violation of the Illinois Biometric Information Privacy Act, or $1,000 for each negligent violation, for all Illinois citizens whose biometric data was used in the DiF dataset. 
In June 2021, Amazon and Microsoft teamed up to defend themselves against lawsuits accusing them of using DiF to train their own facial recognition products, and failing to gain the permission of people whose photographs were used in the dataset.
Per the BBC, while IBM said people whose photos had been included in the dataset could technically opt-out of the dataset through the company's generic research privacy policy, nobody was informed that their data had been used. 
In addition, image owners found it difficult to have their images removed from Diversity in Faces, and impossible to delete them from copies that had already been provided to researchers.
In June 2020, IBM announced it would no longer develop or sell facial recognition technologies to law enforcement authorities. 
Operator: Alphabet/Google; Amazon; IBM; MicrosoftDeveloper: IBM Country: USA Sector: Technology; Research/academia Purpose: Train & develop AI models Technology: Dataset; Facial recognition; Computer vision Issue: Privacy; Copyright; Ethics Transparency: Governance; Privacy"
LAION-5B image-text pairing dataset,"LAION-5B is a large, openly available dataset of 5 billion image and text pairings developed by German non-profit collective LAION (Large-scale Artificial Intelligence Open Network). 
Released in March 20220, LAION-5B has been used to train Stable Diffusion and other AI image models. It's predecessor was LAION-400M.
German stock photographer Robert Kneschke discovered that his photos had been used to train LAION-5B, raising further questions about copyright protections from AI datasets and systems, and the practices and ethics of the dataset's eponymous developer.
AI artist 'Lapine' found that private medical photographs meant only to be available to her doctor had been used to train the image-text dataset LAION-5B. The dataset is supposed only to use publicly available images on the web.
Stanford University researchers discovered thousands of child sex abuse images in LAION-5B, persuading its developers to take down the dataset until it was considered safe to republish. 
Operator: LAIONDeveloper: LAIONCountry: Germany Sector: MultiplePurpose: Pair text and images Technology: Database/dataset; Neural network; Deep learning; Machine learning Issue: Copyright; Ethics; Privacy; Safety; Security Transparency: Governance; Complaints/appeals"
Microsoft replaces journalists with AI,"In May 2020, Microsoft announced that it is laying off dozens of news journalists and editors and is to replace them with artificial intelligence (AI). 
According to Insider and The Guardian, around 50 people were let go in the US and 27 in the UK, most of whom had been curating articles on Microsoft News, MSN, and Microsoft's Edge internet browser. 
The Verge reports that Microsoft had increasingly been using AI to spot trending news stories, change headlines and suggest photos for human editors to pair it with.  
A UK employee told The Guardian 'I spend all my time reading about how automation and AI is going to take all our jobs, and here I am – AI has taken my job.' 
Operator: MicrosoftDeveloper: MicrosoftCountry: USA; UK Sector: Media/entertainment/sports/arts Purpose: Automate copywriting Technology: Machine learning; NLP/text analysis; Neural network; Deep learningIssue: Employment - jobs; Ethics Transparency: Governance; Marketing"
Microsoft robot editor confuses Little Mix band members,"Microsoft's artificial intelligence software illustrated a news story about racism with a photograph of the wrong mixed-race member of the girl band Little Mix. A story about Little Mix singer Jade Thirlwall’s personal reflections on racism was illustrated with a picture of fellow band member Leigh-Anne Pinnock, prompting a reprimand from Thirlwall. Both are mixed race. 
Microsoft's 'robot' editor then picked up and re-published a Guardian story about the mix-up, only for Microsoft staff to delete it. A few days before, The Guardian had warned that Microsoft's decision to replace the jobs of dozens of journalists and editors running its Microsoft News and MSN sites with artificial intelligence would results in mistakes such as the story involving Jade Thirlwall.
Microsoft later said the problem had not arisen as a result of algorithmic bias but due to an experimental feature in the automated system.
Operator: MicrosoftDeveloper: Microsoft
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Suggest photographs
Technology: Machine learning; NLP/text analysis; Neural networks; Deep learning Issue: Accuracy/reliability; Bias/discrimination - race
Transparency: Governance"
PimEyes facial recognition search engine,
Hangzhou Safari Park mandatory facial recognition registration,"A court in Hangzhou ruled that Hangzhou Safari Park must delete images collected by its facial recognition technology and pay 1,038 yuan in damages to a professor and his wife.
Zhejiang Sci-tech University Associate Law Professor Guo Bing had alleged that he and his wife did not consent to have their images collected by facial recognition when they entered the park, having already registered their identities on a fingerprint recognition system that had been replaced by a new facial recognition system.
The court ruled the use of facial recognition had 'exceeded the legally necessary requirements.' The first of its kind in China, the case attracted national and international media attention.
Operator: Hangzhou Safari Park Developer:  Country: China Sector: Gov - policePurpose: Verify identity Technology: Facial recognition Issue: Appropriateness/need; Necessity/proportionality; Privacy Transparency: Governance; Privacy"
"Tesla Model 3 crashes into 18-wheeler truck, kills owner","A Tesla with its Autopilot driver assistance system activated drove under a tractor-trailer that had moved into its path, killing the Tesla driver instantly.
In March 2019, Jeremy Banner had been driving to work in his 2018 Tesla Model 3 in Delrway Beach, Florida, when an eighteen-wheeler tractor-trailer moved into his path. Ten seconds earlier Banner had activated the car's Autopilot system, but it failed to detect the farm vehicle and the car drove underneath the trailer, shearing off the hood and killing Banner instantly. 
In February 2020, a US National Transportation Safety Board (NTSB) investigation concluded that the truck driver was primarily to blame for pulling into traffic, but also said that Banner and Tesla were at fault. Banner was ruled to have ignored the road due to an overreliance on Autopilot, and for having been 'inattentive' for failing to take evasive action.
In November 2023, Florida judge Reid Scott allowed a lawsuit to proceed against Tesla that accused the company of intentional negligence and misconduct over the crash. Scott said that he had found 'reasonable evidence' that Tesla CEO Elon Musk was aware of a dangerous defect in the automaker's Autopilot system but still allowed the cars to be driven. 
Operator: Jeremy BannerDeveloper: Tesla Country: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology:  Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Governance; Marketing; Legal"
Driverless Tesla Model 3 negotiates parking lot,"A Tesla Model 3 has been filmed driving on wrong side of the road, with no one behind the wheel, in a Richmond, British Colombia, shopping centre parking lot. The Tesla was understood to be 'summoned' via a mobile app by the owner up to 200 feet away. The autonomous operation of  vehicles is not permitted in British Colombia, according to ICBC. 
First introduced in 2019, Smart Summons has been on the receiving end of complaints by owners and pedestrians about near crashes, confused cars, and bumper damage. Tesla CEO Elon Musk has described Smart Summon as 'probably our most viral feature ever.'
Operator:  Developer: Tesla
Country: Canada
Sector: Automotive
Purpose: Summon car
Technology: Driver assistance system Issue: Accuracy/reliability; Safety
Transparency: Black box"
"Tesla Model 3 hits tow truck, explodes","A Tesla Model 3 driven by businessman Alexey Tretyakov, 41, hit a parked tow truck that servicing another vehicle on a ring road outside Moscow, Russia, in August 2019. Tretyakov and his two children escaped the car before it was exploded and caught fire, but were severely injured and rushed to hospital. Tretyakov suffered a broken leg and chest injury, while his children reportedly suffered bruises and concussion.
Tretyakov and the police have since confirmed the car was on Autopilot. The driver also said the car had been in 'drive assistance mode', in which enhanced safety features are enabled but the driver’s hands remain on the wheel, and that he had failed to see the truck. 
Operator: Alexey Tretyakov Developer: Tesla
Country: Russia
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system; Self-driving system Issue: Accuracy/reliability; Safety
Transparency: Black box"
Tesla Model S tricked into veering into wrong lane,"Researchers from Tencent's Keen Security Lab have adversarily tricked a Tesla Model S into driving into a lane of oncoming traffic. 
The exploit worked by using small, inconspicuous stickers that tricked the car's Enhanced Autopilot into detecting and then following a change in the current lane, raising questions about the safety and efficacy of Tesla's driver assistance system.
As Technology Review's Karen Hao noted, 'Tesla’s Autopilot is vulnerable because it recognizes lanes using computer vision. In other words, the system relies on camera data, analyzed by a neural network, to tell the vehicle how to keep centered within its lane.'
Tesla welcomed the attack, adding that the adversarial attack was unrealistic 'given that a driver can easily override Autopilot at any time by using the steering wheel or brakes and should always be prepared to do so, and can manually operate the windshield wiper settings at all times.'
Operator: Keen Security Lab Developer: Tesla Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box
Tesla Autopilot, Full-self Driving
Tencent Keen Security Lab (2019). Experimental Security Research of Tesla Autopilot (pdf) 
https://arstechnica.com/information-technology/2019/04/researchers-trick-tesla-autopilot-into-steering-into-oncoming-traffic/
https://www.technologyreview.com/2019/04/01/65915/hackers-trick-teslas-autopilot-into-veering-towards-oncoming-traffic/
https://www.cnbc.com/2019/04/03/chinese-hackers-tricked-teslas-autopilot-into-switching-lanes.html
https://www.news.com.au/technology/motoring/motoring-news/teslas-autopilot-fooled-by-a-simple-trick/news-story/636d2cba3a94b4c1c8d82b7b4c316078
https://bgr.com/2020/02/19/tesla-autopilot-hack-speed-limit-increase-50-mph/
https://www.bleepingcomputer.com/news/security/researchers-trick-tesla-to-drive-into-oncoming-traffic/
https://www.forbes.com/sites/thomasbrewster/2019/04/01/hackers-use-little-stickers-to-trick-tesla-autopilot-into-the-wrong-lane/
Tesla Autopilot tricked into driverless driving
Tesla Model S crashes into tree, kills two passengers
Page infoType: IncidentPublished: March 2023"
"Mobileye 630 PRO tricked by drones, projectors","A team of researchers at Ben Gurion University, Israel, have defeated a Renault Captur's 'Level 0' Mobileye 630 PRO Advanced Driver Assist System (ADAS) by following it with drones that projected images of fake roadsigns.
As Boing Boing points out, the attack could be used to trick cars into making manoeuvers that compromised the safety or integrity of their passengers and other users of the road — from unexpected swerves to sudden speed-changes to detours into unsafe territory.
Operator: Renault Developer: MobileyeCountry: Israel Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Computer vision Issue: Security, Safety; Accuracy/reliability Transparency: Black box"
Apple Card accused of gender bias,"Apple launched its new Apple Card in the USA in August 2019. Underwritten by Goldman Sachs, the card is designed to work with Apple Pay on iPhones, iPads, Apple Watches, and Macs. 
In November 2019, tech entrepreneur David Hansson complained on Twitter that he had been given a credit limit 20 times larger than that offered to his wife, despite her having a better credit score. He went on to accuse Goldman Sachs of gender discrimination by using algorithms to determine a person's credit limit. 
Hansson's complaint was followed by Apple co-founder Steve Wozniak saying that he received ten times the credit limit that his wife was offered, resulting in a volley of accusations that Apple and Goldman Sachs were 'sexist'. 
In response, the New York State Department of Financial Services launched an investigation. In March 2021, the investigation concluded (pdf) that there was no evidence of 'deliberate or disparate' racial discrimination but that there were clear deficiencies in customer service and transparency.
Operator: Apple; Goldman Sachs Developer: Apple; Goldman SachsCountry: USA Sector: Banking/financial services Purpose: Streamline card application process Technology: Machine learning Issue: Bias/discrimination - gender Transparency: Governance; Complaints/appeals; Marketing"
"3D masks fool payment, airport facial recognition systems","Researchers in China have found that facial recognition technology can be fooled by using a 3D-printed mask depicting a different person's face, raising questions about the security of the technology.
AI development company Kneron researchers used 2D and 3D copies of a subject’s face, as well as a 3D face mask, to test the strength of facial recognition security solutions at public transport portals, point of sales terminals, airport security checkpoints, and on mobile devices, in China and the Netherlands.
They discovered that most system were able to resist the 2D and 3D copies, but the 3D mask was able to trick payment a system at a border checkpoint in China and a passport-control gate at Schiphol airport in Amsterdam.
Forbes reporter Thomas Brewster commissioned a 3D printed model of his own head to test the facial unlocking systems on a range of Apple and Android phones, finding that only iPhone X models were able to resist the attack.
Operator: Huawei; LG; OnePlus; Samsung  Developer: Huawei; LG; OnePlus; Samsung Country: China; Netherlands Sector: Banking/financial services Purpose: Test facial recognition Technology: Facial recognition Issue: Security; Accuracy/reliability Transparency: Governance"
"Tesla Model S runs red light, kills two","Tesla owner Kevin George Aziz Riad has been charged with a felony in the US for a December 2019 crash that left people dead whilst his car was on Autopilot.
According to police, Riad's Tesla Model S was moving at a high speed when it left a freeway and ran a red light before striking a Honda Civic, killing Gilberto Alcazar Lopez and Maria Guadalupe Nieves-Lopez. 
The families of Lopez and Nieves-Lopez have sued Tesla and Riad in separate lawsuits alleging negligence by Riad and accusing Tesla of selling defective vehicles that accelerate suddenly and that lack an effective automatic emergency braking system.
Riad's preliminary hearing is scheduled for February 2023.
Operator: Kevin George Aziz Riad Developer: Tesla
Country: USA
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system; Self-driving system Issue: Accuracy/reliability; Safety
Transparency: Black box"
Generated Photos 'infinite diversity' face collection,"Generated Photos is a collection of 100,000 free images of AI-generated faces to help designers who need portraits for their websites, apps or presentations. It launched in September 2019 to a mixed reception. 
The collection features headshots with consistent lighting and sizing with a wide range of angles, positions, and facial expressions, reflecting a wide variety of ethnicities, ages and face shapes to represent an 'infinite diversity.'
A large number of users praised the collection for the 'realism' and usefulness of the photos. But some described the photos as 'weird', 'stiff' and, in some instances, 'truly fucked up'. 
Others pointed out the ease with which they could be used for malicious purposes, and that they could put licensed stock photo companies and models out of business.
In a blog post, Generated Photos developer Icon8 said the company used real models to create the 'incredibly realistic' images rather than scraping from the web or using stock photographs, thereby getting round copyright issues.
Operator: Icons8 Developer: Icons8; Prototypr
Country: USA
Sector: Business/professional services
Purpose: Produce 'infinite diversity'
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Dual/multi-use; Employment
Transparency: Governance"
King's Cross live facial recognition trial,"The August 2019 discovery by the Financial Times that the King's Cross development in London had been quietly using facial recognition to monitor tens of thousands of people moving daily within its site infuriated civil and privacy rights groups, which accused it of unethical behaviour. 
The fracas also put London's Metropolitan Police Service in the spotlight for its covert involvement in the private scheme, and for making a number of seemingly misleading and contradictory communications.
One month before, the UK Parliament had told police forces to stop using facial recognition technology until a legal framework for its use was set up. In September 2019, King's Cross, which had been working on introducing a new facial recognition system, halted its use of the technology.
King's Cross developer Argent and its partners, including London's Metropolitan Police Service and British Transport Police, were roundly criticised for the fact that the system had been operating between 2016 and 2018 without the knowledge of the local community, workers, and the general public, and without the permission and oversight of the Mayor of London. 
The operators were also dragged over the coals for covertly sharing images with each other, for failing to gain the consent of those being monitored, and for refusing to reveal how long they had been using the technology, how user data was protected, if and with whom data was being shared, or the legal basis for its use.
Opaque and misleading communications about the use of facial recognition at King's Cross characterised the responses of those involved from the start.
When news first broke, Argent had said it had been using facial recognition to 'ensure public safety'. In a letter sent to London mayor Sadiq Khan dated August 14, Argent partner Robert Evans said they wanted facial recognition software to spot people on the site who had previously committed an offence there.
Furthermore, the Met Police and British Transport Police had denied any involvement in the programme. But the Met Police later admitted it had supplied seven images for a database used to carry out facial recognition scans. It had previously told the Mayor of London that it had not worked with 'any retailers or private sector organisations who use Live Facial Recognition'.
Operator: Argent; Metropolitan Police Service (MPS) Developer: NEC
Country: UK
Sector: Govt - police
Purpose: Strengthen security
Technology: Facial recognition Issue: Ethics; Governance; Privacy; Surveillance
Transparency: Governance; Marketing; Privacy"
Sidewalk Labs Toronto Quayside smart city development,"Proposed in October 2017 by Google affiliate Sidewalk Labs in response to a competition, Sidewalk Toronto was an urban development project the Quayside area of Toronto that was to be built 'from the internet up' and aimed to become 'a testbed for emerging technologies, materials and processes'. 
Sidewalk Labs intended (pdf) to use artificial intelligence, sensors and other data-collection devices to monitor, analyse and optimise pedestrian traffic, noise, weather conditions, and energy and garbage use, amongst other things. 
The proposal proved highly controversial, particularly with regard to its business model, perceived techno-solutionism, and privacy implications, and it was abandoned in May 2020, supposedly due to COVID-19.
With data envisaged as central to the workings of the new district, the question of how it would be managed was important. 
Sidewalk Labs proposed an independent 'Urban Data Trust' to govern the collection and management, but was seen to have failed to provide meaningful detail, leading Members of Waterfront Toronto’s Digital Strategy Advisory Panel to complain the plan was 'frustratingly abstract' and it's ambitions 'misguided'.
It also prompted a outcry amongst civil right and privacy advocates, and the general public, about privacy, concerns supported by research indicating that 60% of Toronto citizens did not trust Sidewalk Labs to collect data on residents, and that 58% did not think the company would keep its commitment not to use resident data for advertising purposes.
In response, Sidewalk Labs recommended shifting data governance from the Urban Data Trust to vesting authority in the existing Waterfront Toronto government partnership.
Operator: Waterfront Toronto Developer: Alphabet/Google; Sidewalk Labs
Country: Canada
Sector: Construction
Purpose: Create smart city
Technology: Bicycle detection system; Garbage management system; Traffic management system; Vehicle detection system Issue: Privacy; Surveillance; Ethics
Transparency: Governance; Marketing; Privacy"
Oral-B Genius X AI toothbrush,"Oral-B Genius X is an electric toothbrush with a rotating brush and motion sensors that connects via Bluetooth to a smartphone app and uses artificial intelligence to provide advice on how to improve one's brushing technique. 
The product has received strong reviews from experts and users. But it has also prompted some commentators to wonder how much of a difference it makes, whether it is really necessary, or worth the price.
'Does anyone really need a £340 electric toothbrush when one costing £6 will do the same job?' quipped the Daily Mail.
Operator: P&G Developer: P&G
Country: USA
Sector: Consumer goods 
Purpose: Clean teeth
Technology: Machine learning; SensorIssue: Accuracy/reliability; Appropriateness/need; Effectiveness/value
Transparency:"
Jordan Peterson fake voice generator,"NotJordanPeterson.com, a website for generating AI audio clips of Jordan Peterson saying whatever users want, was prompted to make offensive and outrageous remarks. The incident resulted in the controversial, anti-woke psychologist threatening legal action and the site being shut down.
Created by computer scientist Chris Vigorito, the site asked users to type fewer than 280 characters of text into a box that would be fed into a neural network trained on Peterson's actual voice. However, journalists and others prompted the site to make Vulgar, offensive, and outrageous remarks, including reading passages from Valerie Solanas, a feminist author who wrote the anti-capitalist, anti-male SCUM Manifesto.
The real Peterson responded by slamming deepfakes and voicing his concern that they 'need to be stopped, using whatever legal means are necessary.' 'In light of Dr. Peterson's response to the technology demonstrated by this site…and out of respect for Dr. Peterson, the functionality of the site will be disabled for the time being,' responded the site creator. 
Operator: Chris Vigorito Developer: Chris Vigorito Country: Canada
Sector: Education
Purpose: Damage reputation
Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Privacy; Ethics; Mis/disinformation 
Transparency: Governance; Marketing"
"Nice, Marseille schools facial recognition","A facial recognition technology pilot project at two high schools in Nice and Marseille to control access to the entrance gate has been suspended by the French data protection authority (CNIL). 
NGOs, teachers’ unions, and parents had waged a campaign against the project, with AccessNow arguing it would have been able to use students as training subjects for their systems, thereby further eroding their privacy. 
The French DPA then determined the project could not be 'implemented legally', a ruling disregarded by France's Mar du Sud region, which attempted to roll out the project by labeling it 'experimental'. 
In February 2020, in a case bought by French digital rights group La Quadrature du Net, the Administrative Court of Marseille ruled (pdf) that the region had no power to take this decision, and that only schools have such powers.
It also said that the EU's GDPR had been breached as students’ consent could not be 'freely given' because students could not give free consent as the school’s administration is acting as the higher authority.
Operator: Les Eucalyptus high school, Nice; Ampère high school, Marseille Developer: CiscoCountry: France
Sector: Education
Purpose: Register attendance
Technology: Facial recognition Issue: Privacy
Transparency: Governance; Legal; Privacy"
Anderstorp high school facial recognition,"Sweden's Data Protection Authority (DPA) has fined (pdf) the Skelleftea municipality 200,000 Swedish Krona for tracking 22 students over three weeks and detecting when each pupil entered a classroom. Images from the camera were then compared to pre-registered images of their faces. 
According to Swedish state broadcaster SVT Nyheter, Skelleftea municipality said they thought facial recognition technology would save teachers 17,280 hours a year reporting student attendance.
The regulator ruled that although the school had secured parents' consent to monitor the students, it felt it was a legally disproportionate reason to collect such sensitive personal data, and that students could be expected to have a sense of privacy when they entered a classroom.
The DPA also said the managers of the project failed to do a proper impact assessment, which should have led to consulting the authority due to the risks involved. 
Operator: Skelleftea Secondary Education Board, Anderstorp's High School Developer: TietoCountry: Sweden
Sector: Education
Purpose: Register attendance
Technology: Facial recognition Issue: Privacy
Transparency:"
Victoria schools student attendance facial recognition,"A 2019 trial of facial recognition technology at several private schools in Victoria state, Australia, met with a strong backlash from civil and digital rights advocates, complaints from students, and resulted in a clamp down by the state government.
Developed by Melbourne-based startup LoopLearn (since rebranded to LoopSafe), the system was intended to automate the process of marking student attendance in class and tracking their whereabouts, sending the information to a web dashboard and app accessible by teachers and school management.
However, privacy experts said the system was inappropriate, unjustified, and amounted to comprehensive student surveillance. Media reports said the company was unable to say how it secures and stores data, or who potentially could have access to it. 
A review by the Victorian government found major privacy risks in the system, and banned schools from using facial recognition in classrooms unless they conduct a rigorous assessment and gain the approval of parents, students, and the state’s education department.
Australia's Federal Government had awarded LoopLearn AUS 500,000 to become commercially viable under its Accelerating  Commercialisation programme.
Operator: Ballarat Clarendon College; Clarendon College; Sacred Heart College; Waverley College Developer: LoopSafe/LoopLearn
Country: Australia
Sector: Education
Purpose: Register attendance, monitor location
Technology: Facial recognition Issue: Privacy; Ethics; Surveillance
Transparency: Governance; Privacy; Marketing"
AI Portrait Ars racial bias,"AI-powered portrait generator AI Portrait Ars drew controversy soon after its launch when it was found to be whitening coloured peoples' skins.
Mashable journalist Morgan Sung discovered that the generator, which turned selfies into realistic Impressionist and Baroque portraits, 'whitened my skin to an unearthly pale tone, turned my flat nose into one with a prominent bridge and pointed end, and replaced my very hooded eyes with heavily lidded ones.'
According to Mauro Martino, co-developer of the app, 'the intention is to share the experience of being portrayed by an AI algorithm, to discover how AI sees you. There is no willingness to improve or deform the starting picture.'
Martino and collaborator Luca Stornaiuolo said AI Portrait Ars was not just for entertainment, but that they were making a broader point about perceptions of beauty, and the notion of AI fairness.
The model was apparently based on a collection of 15,000 portraits, predominantly from the 15th century western European Renaissance period, which would help explain the skin whitening, and the tendency to produce faces with straight, high noses and thin smiles.
Operator: Mauro Martino; Luca StornaiuoloDeveloper: Mauro Martino; Luca Stornaiuolo
Country: USA
Sector: Research/academia
Purpose: Generate portraits
Technology: Generative adversarial network (GAN); Neural network; Machine learning Issue: Bias/discrimination - race, ethnicity
Transparency:"
Speech2Face facial reconstructions,"Speech2Face is an algorithm developed by a group of researchers at MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) and Google AI that generates images of what someone might look like from snippets of audio recordings of their voice.
According to its associated research paper (pdf), the MIT researchers used a dataset of millions of clips from YouTube and elsewhere and created a neural network-based model that learns vocal attributes associated with facial features from the videos.
Reaction to Speech2Face has been mixed, with some commentators praising it for creating rough likenesses of people with little information, whilst others focused on the fact that the images created by Speech2Face often only bear a general resemblance to the speaker, and that the system is prone to creating images of the wrong gender and ethnicity. 
Others highlighted the system tends to identify people with high voices as female and low voices as malem and people speaking Asian languages as Asian, reflecting strong gender, racial and country of origin biases in its data and results. Some went further, accusing it of 'ethnic profiling at scale' and being little more than 'awful transphobic shit'. 
Speech2Face also kicked off a debate about the nature of data privacy, with some questioning whether this kind of technology could be used to identify individuals, despite the team claiming their method 'cannot recover the true identity of a person from their voice.'
An individual included in the dataset told Slate that he didn’t remember signing a waiver for the YouTube video he was featured in that was fed through the algorithm. It also prompted a conversation about the need for ethics review boards at conferences and funding agencies.
The MIT team urges caution on the project's GitHub page, acknowledging that the technology raises questions about discrimination and privacy. They said the training data used was a collection of educational videos from YouTube which may not represent the world population.
'Although this is a purely academic investigation, we feel that it is important to explicitly discuss in the paper a set of ethical considerations due to the potential sensitivity of facial information,' they wrote, recommending that 'any further investigation or practical use of this technology will be carefully tested to ensure that the training data is representative of the intended user population.'
Operator: Alphabet/GoogleDeveloper: MIT; Alphabet/Google
Country: USA
Sector: Research/academia
Purpose: Reconstruct facial image
Technology: Neural network Issue: Accuracy/reliability; Bias/discrimination - race, gender, LGBTQ; Privacy
Transparency: Privacy"
ImageNet image recognition dataset,"Developed by Princeton University researchers in 2008, ImageNet is a database that was intended to help developers of image recognition-based systems by creating a dataset that was a) large, b) diverse and c) high quality.
Widely regarded as a landmark in computer vision research and its sub-set, object recognition, ImageNet was free and open to researchers on a non-commercial basis, though closed to journalists and other public interest parties.
The resource was the subject of an annual ImageNet Large-Scale Visual Recognition Challenge (or ImageNet Challenge) from 2010 to 2017, and resulted in the realisation of the effectiveness of deep learning and neural networks, and their adoption and use by academics, researchers, and technology professionals.
However, ImageNet has also prompted heated debate regarding the accuracy and fairness of its labeling, and its failure to respect the rights of people whose images its developers collected without their consent.
In September 2019, ImageNet Roulette, a website that encouraged users to upload selfies and then analyse what it saw, revealed that ImageNet contained inaccurate, derogatory, and racially offensive information.
ImageNet Roulette told people what it thought they look like by running their photos through a neural network trained on ImageNet, a database that identifies and classifies over 14 million photographs. 
While many captions produced by the code were harmless, some turned out to be inaccurate, or contained racist, misogynistic and other discriminatory and derogatory slurs.
Created by Kate Crawford, co-founder of the AI Now Institute, artist Trevor Paglen, and software developer Leif Ryge, ImageNet Roulette was a 'provocation designed to help us see into the ways that humans are classified in machine learning systems.'
The ensuing fracas led the developers of ImageNet to scrub 'unsafe' and 'sensitive' labels from the database, and to remove links to related photographs.
By automatically scraping images from Google, Bing and photo-sharing platform Flickr to build its training dataset without consent, ImageNet developers were accused of ignoring user privacy, leading lawyers and rights activists to call for stronger privacy and copyright laws.
In March 2021, the ImageNet team announced it had blurred 243,198 photographs in its database using Amazon's Rekogniton image and video analytics service.
The update was seen to have minimal impact on the classification and transfer learning accuracy of the dataset; however, some commentators argued it would damage ImageNet's relevance by styming its reproducibility.
Operator: Kate Crawford; Trevor Paglen Developer: Princeton University; Jia Deng; Wei Dong, Richard Socher; Li-Jia Li; Kai Li; Fei-Fei Li Country: USA Sector: Research/academia Purpose: Identify objects Technology: Dataset; Computer vision; Object detection; Object recognition; Machine learning; Deep learning Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, gender, religion, national identity, location; Copyright; Privacy Transparency: Governance; Privacy"
MegaFace facial recognition dataset,"MegaFace is a facial recognition training dataset consisting of 4,753,320 faces of 672,057 identities from 3,311,471 photos downloaded from 48,383 Flickr users' photo albums. 
Created in 2015 by researchers at the University of Washington, the project was expanded in 2016 in the form of the MegaFace Challenge, in which facial recognition teams were encouraged to download the database and see how their algorithms performed when they had to distinguish between a million possible matches.
Like IBM's Diversity in Faces dataset, MegaFace was based on Yahoo!'s YFCC100M dataset, which provides approximately 100 million photos from photo sharing website Flickr under various Creative Commons licenses. 
Partly due its size, MegaFace became one of the most important benchmarks for commercial face recognition vendors. The only public dataset with a comparable number of images was Microsoft's MS-Celeb-1M dataset, which was withdrawn after a Financial Times/Exposing.ai investigation.
In October 2019, the New York Times reported that as many as 700,000 people had their likenesses uploaded from Flickr to MegaFace, including many children. 
The images had been used to train AI to identify protesters in the USA and monitor Uighurs in China, among other uses.
A University of Washington spokesperson told the NYT that the researchers who created the MegaFace database 'have moved on to other projects and don't have the time to comment on this.'
The MegaFace Challenge and dataset were discontinued in June 2020. But not before the dataset had been downloaded and used by thousands of organisations and individuals across the world, and used to create multiple derivative datasets, such as MegaAge, DiveFace, and TinyFace, many of which continue to exist.
Operator: Alibaba; Alphabet/Google; Amazon; Bytedance; EUROPOL; Huawei; In-Q-Tel; IntelliVision; Megvii; Mitsubishi Electric; Northrup Grumman; Ntechlab; Philips; Samsung; SenseTime; Sogou; Tencent; Vision Semantics Developer: University of Washington Country: USA Sector: Technology; Research/academia Purpose: Improve research quality Technology: Dataset; Facial recognition; Computer vision Issue: Privacy; Copyright; Liability Transparency: Privacy; Marketing"
Keele University study 'predicts' kids' autism without consent,"A behavioural study group on children with autism by researchers at Keele University in the UK used YouTube videos used an AI to study childrens' body movements without their permission.
The researchers told The Atlantic that the purpose of the study was to classify the kids' behaviours as either typical or atypical, with the aim of more quickly evaluating 'edge' cases that might normally require lab equipment or invasive tactile sensors. 
The Atlantic pointed out that the children and parents whose data was scraped by the Keele team had not consented to having their home videos used for scientific research.
The paper, which is another example of the use of machine learning to 'predict' innate attributes, was withdrawn 'due to insufficient or definition error(s) in the ethics approval protocol.'
Operator: Keele University Developer: Andrew Cook; Bappaditya Mandal; Donna Berry; Matthew Johnson Country: UKSector: Health Purpose: Predict autismTechnology: Computer vision; Machine learning; Pattern recognition Issue: Accuracy/reliability; Privacy Transparency: Privacy"
Lockport City School District facial recognition opacity,"An Office of the New York State Comptroller audit (pdf) has found that the procurement process for the implementation of a facial recognition system for Lockport City School District was not transparent, violated district rules and policy, and failed to comply with New York State General Municipal Law.
The audit was the result of allegations levelled by local resident Jim Schultz that SN Technologies’ AEGIS system was biased and the bidding process corrupt. The allegations prompted then-New York Governor Andrew Cuomo to order the suspension and inspection of the system by local authorities in March 2021.
The audit found the district only allowed one day for bids to be submitted and ruled the procurement process was not as transparent as it should have been. SN Technologies was the only respondent. 
Lockport City School District claims Aegis provides early warning signs of threats to students and employees at its schools.The system is also meant to identify guns and alert security to their presence. 
Operator: Lockport City School District Developer: SN Technologies Country: USA Sector: Govt - education Purpose: Strengthen securityTechnology: Facial recognition; Gun detection Issue: Accuracy/reliability; Effectiveness/value; Bias/discrimination - race, ethnicity; Privacy; Surveillance Transparency: Governance; Marketing"
Niulanshan First Secondary School Classroom Care System opacity,"The movements and behaviour of students at Niulanshan First Secondary School in Beijing are being constantly monitored and scored by a system about which they have been no given information nor to which they have provided their consent. 
Launched in 2017, Hanwang Education's Class Care System (CCS) uses facial recognition, emotion recognition and deep learning algorithms to identify students' faces and analyse and classify their behaviour into one of five categories: listening, answering questions, writing, interacting with other students, or sleeping.
Each student is automatically scored weekly and their information shared via a mobile app with their teachers and parents. 
Hanwang argues the scores are important for identifying which students require support. But critics point out that the system is not only highly intrusive, it can also be inaccurate and unfair. 
As Rest of World describes, in one example 'a student who had answered just a single question in his English class was called out for low participation — despite the app recording him as 'focused' 94% of the time.'
Research studies regularly conclude that emotion recognition technologies are based on the fundamentally flawed premise that algorithms can analyse a person’s facial expressions and accurately infer their inner state or mood.
Operator: Niulanshan First Secondary School Developer: Hanwang Technology
Country: China
Sector: Education
Purpose: Assess and rank student behaviour 
Technology: Facial recognition; Emotion recognition; Deep learning; Neural network; Machine learning Issue: Accuracy/reliability; Privacy; Surveillance
Transparency: Governance; Marketing; Privacy"
China Pharmaceutical University student behavioural monitoring,"Photographs showing students at China Pharmaceutical University in Nanjing being monitored and analysed triggered concerns that Chinese AI company Megvii was jeopardising student privacy and freedom of movement.
According to the South China Morning Press, the university was using the facial recognition and emotion recognition system at the university gate, entrances to the dormitory building, library, lab, and two classrooms so that managers can 'track their students.'
'Besides attendance, the system installed in the classroom can provide surveillance of the students' learning, such as whether they are listening to the lectures, how many times they raise their heads, and whether they are playing on their phones or falling asleep,' Xu Jianzhen, director of the university's library and information centre said.
The furore prompted the Chinese government to say it would 'curb and regulate' the use of facial recognition technology and other technologies in schools. 
Megvii responded by saying the photo was a pilot and that it is providing greater campus safety while helping school administrators improve efficiency. It went on to say that it 'always insisted on technology for good, so that artificial intelligence can benefit everyone.'
Megvii has also been discovered filing patents for systems capable of recognising Uyghurs.
Operator: China Pharmaceutical University Developer: Megvii 
Country: China
Sector: Education
Purpose: Assess student attentiveness, strengthen campus safety, improve attendance 
Technology: Facial recognition; Emotion recognition; Deep learning; Neural network; Machine learning Issue: Privacy; Surveillance
Transparency: Governance; Marketing; Privacy"
"Tesla Model 3 crashes into Ford, kills passenger","A 15-year-old boy has been killed when a Tesla Model 3 with its Autopilot driver-assist function enabled hit his father's Ford Explorer pick-up truck on a highway in California in August 2019. 
Javier Maldonado's father Benjamin had been trying to change lanes but was hit from behind by the Tesla. 
In July 2021, Javier's family sued Tesla and driver Romeo Lagman Yalung, accusing them of negligent product liability, motor vehicle negligence, negligent infliction of emotional distress, and wrongful death. 
Tesla blamed Yalung for the crash, saying he had been inattentive and driving at an unsafe speed.'
Operator: Romeo Lagman Yalung Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
Austria AMS employment service job seeker predictions,
PSZ unemployment worker profiling,
Atlantic Plaza Towers facial recognition,"Over 300 residents of Atlantic Plaza Towers in Brooklyn, New York, have successfully fought off a proposal by their landlord to install a facial recognition system behind their backs.
The StoneLock facial recognition system was to be used so that recognised tenants could open the front door to their buildings rather than using traditional keys or electronic key fobs. 
However, residents felt it was an 'extreme' intrusion on their privacy, and that they did not want their movements to be tracked. They were also concerned that the landlord wanted to attract higher-income, white tenants to the majority-black public building. 
In July 2018, the landlord Nelson Management Group had sought state approval to install the system under a state rule which says that landlords of rent-regulated apartments built before 1974 must seek permission from the state’s Homes and Community Renewal (HCR) agency for any 'modification in service.'
Brooklyn Legal Services worked with tenants to issue (pdf) a letter of protest that noted that most tenants at Atlantic Plaza Towers were black and that studies had shown that facial recognition disproportionately impacts people of colour. 
Nelson announced it was withdrawing the plan in November 2019.
Operator: Nelson Management Group Developer: StoneLock
Country: USA
Sector: Govt - housing
Purpose: Verify tenant identity
Technology: Facial recognition Issue: Privacy; Surveillance
Transparency: Governance; Marketing; Privacy"
GPT-3 bot posts Reddit comments unnoticed,"A bot powered by Open AI's GPT-3 large language model spent a week responding to comments on the Ask/Reddit subreddit before it was discovered not to be human. Though the bot posted mostly harmless feedback, it also engaged with conspiracy theories and sensitive topics, including suicide.
In addition to angering the Reddit commmunity, the incident was seen to show the ease with which GPT-3 could be manipulated to generate fake opinions and conversations, and therefore be used for misinformation and disinformation. It also pointed to the system's propensity to produce unsafe content.
Operator: Philosopher AI; OpenAI Developer: OpenAI
Country: USA
Sector: Multiple
Purpose: Generate text
Technology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Mis/disinformation; Ethics; Safety
Transparency: Governance; Black box
GPT-3 Wikipedia profile
GPT-3 research study
GPT-3 model card
Chad Barrett (2022). It took me 1 day to create a program, using GPT-3, to create a highly convincing small army of bots to post on Reddit: Here's how I did it
https://www.reddit.com/r/NoStupidQuestions/comments/j4xhz6/comment/g7o4lem/
https://metastable.org/gpt-3.html
https://www.technologyreview.com/2020/10/08/1009845/a-gpt-3-bot-posted-comments-on-reddit-for-a-week-and-no-one-noticed/
https://www.technologyreview.com/2021/02/24/1017797/gpt3-best-worst-ai-openai-natural-language/
https://thenextweb.com/news/someone-let-a-gpt-3-bot-loose-on-reddit-it-didnt-end-well
https://www.thetimes.co.uk/article/gpt-3-the-machine-that-learned-to-troll-vplh8cw8k
https://analyticsindiamag.com/a-gpt-3-bot-interacting-with-people-on-reddit/
https://www.theregister.com/2020/10/09/reddit_gpt3_bot/
GPT-3 advises patient to kill themselves
GPT-3 anti-Muslim bias
Page infoType: IncidentPublished: April 2023"
UK visa applications streaming 'racism',"Since 2015, the UK Home Office filtered visa applications using an algorithmically-driven 'Visa Streaming' traffic light system that assigns a red, amber or green risk level to each applicant. People assigned a red risk level were more likely to be refused. 
The UK government stopped using the system in August 2020 after a legal challenge that accused it of prioritising 'speedy boarding for white people' for the most favoured countries in the system.
In October 2019, justice advocacy group Foxglove and the Joint Council for the Welfare of Immigrants (JCWI) launched a case to legally force the Home Office to explain on what basis the algorithm 'streams' visa applicants. 
Critics had previously raised concerns that the system was discriminating against individuals based on their nationality in a discriminatory form, and violated the 2010 Equality Act. 
The suit also alleged that the algorithm was not transparent. Aside from admitting the existence of a secret list of 'suspect' nationalities, the Home Office refused to provide meaningful information about how the system worked, inclusing what other factors were used to grade applications.
Having insisted the algorithm was used only to allocate applications and that immigration officers ultimately ruled on them, the Home Office announced in August 2020 that it would settle the suit and halt the use of the system until it had been redesigned considering 'issues around unconscious bias and the use of nationality'.
In April 2021, WIRED reported that a 'secretive' Home Office Data Services & Analytics unit had been collecting data on 650 million people, including data from immigration and border systems, and police and intelligence agencies. 
The 'super database' reputedly 'provides members of law enforcement agencies, such as Border Force, with the names of individuals with previous immigration history, those of interest to detection staff, police or matters of national security.' 
Operator: UK Home Office Developer: UK Home Office
Country: UK
Sector: Govt - immigration
Purpose: Assess visa applications
Technology: Risk assessment algorithm Issue: Bias/discrimination - race, ethnicity
Transparency: Governance; Black box; Marketing"
UK passport check interprets lips as open mouth,"An automated passport photo checker facial detection system run by the UK Home Office mistook the 'big lips' of a young Black man hoping to renew his passport for an open mouth, resulting in his application being refused.
The incident prompted Bada to share his experience online, leading to accustations of algorithmic bias and racism. The UK's Race Equality Foundation said it believed the system was not tested properly to see if it would actually work for black or ethnic minority people, calling it 'technological or digital racism'.
A New Scientist report published one month after Bada's incident revealed that the Home Office system was known to fail for people with dark skin, but that it decided to use it regardless. The documents were released following a Freedom of Information request by MedConfidential.
Operator: UK Home Office Developer: 
Country: UK
Sector: Govt - immigration
Purpose: Automate passport checks
Technology: Facial detection Issue: Bias/discrimination - race, ethnicity
Transparency: Governance"
Virginia Non-violent Risk Assessment,
Met Police live facial recognition,"London's Metropolitan Police Service conducted a series of trials of live facial recognition technology across London between August 2016 and February 2019. Trials included the Notting Hill Carnival, Remembrance Sunday, Stratford Westfield shopping centre, and Romford.
The programme was plagued by criticism from civil and privacy rights advocates and technology and legal experts regarding the accuracy of the system, and complaints about inadequate transparency, accountability, and privacy protection.
A July 2019 University of Essex report (pdf) commissioned by the Met Police into its broader use of facial recognition found that only eight of 42 matches were verified as correct, meaning 81% of suspects identified by its system were innocent. 
The Met Police responded by saying it was 'extremely disappointed with the negative and unbalanced tone of th[e] report'. As Sky News reported, the force 'prefers to measure accuracy by comparing successful and unsuccessful matches with the total number of faces processed by the facial recognition system. According to this metric, the error rate was 0.1%.' 
The University of Essex researchers questioned the legal basis on which the Met deployed facial recognition technology, finding it 'inadequate' in light of the police’s legal duties under human rights law. 
The report went on to suggest that it would be 'highly possible' the Met Police's usage of the system would be found unlawful if challenged in court.
A report (pdf) published in October 2022 by University of Cambridges Minderoo Centre for Technology and Democracy researchers found that the Met Police's trials suffered from inadequate transparency and accountability, poor privacy, and failed to meet minimum expected ethical and legal standards.
The researchers went on to argue that live facial recognition technology should be banned from use in streets, airports and any public spaces in the UK.
In April 2023, the Met Police announced it was to resume all forms of facial recognition on the basis of a study it and South Wales Police had commissioned the UK National Physical Laboratory to carry out.
The study found that NEC's NeoFace system showed 'no statistically significant race and gender bias,' and argues that facial recognition surveillance can protect human rights as it reduces and prevents crime.
Operator: Metropolitan Police Service (MPS)Developer: NEC
Country: UK
Sector: Govt - police
Purpose: Strengthen security
Technology: Facial recognition Issue: Accuracy/reliability; Surveillance; Privacy; Bias/discrimination - race, ethnicity, gender 
Transparency: Governance; Privacy; Marketing"
South Wales Police facial recognition trial,"A trial of facial recognition technology at football and rugby matches, music festivals, and on city streets by South Wales Police (SWP) from May 2017 to April 2019 met with significant criticism from civil and privacy rights advocates. 
South Wales Police's trial consisted of mobile video cameras hooked up to facial recognition software to scan crowds for faces on a watchlist. It's use of the technology was ruled unlawful by the UK Court of Appeal in August 2020.
The system was accused of making unacceptably high levels of errors. Over 2,000 people were wrongly identified as possible criminals by South Wales Police's facial recognition system during the 2017 Champions League final in Cardiff. Of the 2,470 potential matches with custody pictures, 2,297 (92%) were wrong.
The force said the high volume of false matches was down to 'poor quality images' supplied by agencies including UEFA and Interpol. They also argued it could be attributed to it being the first major use of the equipment. A later evaluation of the system by Cardiff University found it flagged 2,900 possible suspects, but 2,755 were false matches.
Campaign group Liberty brought a legal case against South Wales police after Cardiff resident and civil rights activist Ed Bridges claimed the force had invaded his privacy and data protection rights by capturing and processing his facial features whilst he was shopping and when he was attending a defence industry exhibition.
Bridges lost his first challenge, with the two judges ruling the technology was lawful. However, he won on Appeal, with the court finding that there had been inadequate guidance on where the system could be used and who could be put on a watchlist, its data protection impact assessment was deficient, and the force had failed to take reasonable steps to find out if the software contained racial or gender bias.
An October 2022 report (pdf) by researchers at University of Cambridge's Minderoo Centre for Technology and Democracy found that the South Wales Police trial had failed to meet minimum expected ethical and legal standards. 
The report singled out the force's failure to establish limits on the use of facial recognition technology at protest assemblies, inadequate oversight, concerns about the independence of the Joint Independent Ethics Committee and the lack of human rights, equality, or data protection experts on the committee, and the lack of public consultation, notably with marginalised communities.
Operator: South Wales Police Developer: NEC
Country: UK
Sector: Govt - police
Purpose: Strengthen security
Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, gender; Ethics; Freedom of expression - right of assembly; Privacy
Transparency: Governance; Privacy
South Wales Police. Facial Recognition Technology
South Wales Police. FRT 2017 Deployments (pdf)
South Wales Police (2020). Response to the Court of Appeal judgment on the use of facial recognition technology
South Wales Police Wikipedia profile - Use of facial recognition
UK Court of Appeal (2020). R (Bridges) -v- CC South Wales
Liberty. Legal Challenge - Ed Bridges vs South Wales Police
UK Information Commissioner (2019). The use of live facial recognition technology by law enforcement in public places (pdf)
UK Biometrics and Surveillance Camera Commissioner (2019). The use of facial recognition technology by South Wales police
Big Brother Watch (2020). Briefing on facial recognition surveillance (pdf)
Ritchie K.L., Cartledge C., Growns B., Yan A., Wang Y., Guo K., Kramer R.S.S., Edmond G., Martire K.A., Mehera San Roque M., White D. (2021). Public attitudes towards the use of automated facial recognition technology in criminal justice systems around the world
Minderoo Centre for Technology & Democracy (2022). A Socio-Technical Audit: Assessing Police Use of Facial Recognition (pdf)
Cardiff University (2018). Evaluating the Use of Automated Facial Recognition Technology in Major Policing Operations
https://www.walesonline.co.uk/news/wales-news/facial-recognition-south-wales-police-17202103
https://www.wired.co.uk/article/police-facial-recognition-south-wales-court-decision
https://www.biometricupdate.com/202008/facial-recognition-lessons-for-the-private-sector-from-the-south-wales-police-case
https://onezero.medium.com/a-facial-recognition-giant-refuses-to-share-details-about-its-algorithm-dataset-df27a208683d
https://www.bbc.co.uk/news/uk-wales-53734716
https://www.walesonline.co.uk/news/wales-news/facial-recognition-wrongly-identified-2000-14619145
https://news.sky.com/story/facial-recognition-technology-who-watches-the-watchers-11725536
https://techxplore.com/news/2020-08-uk-court-recognition-violates-human.html
https://www.cnbc.com/2020/08/11/swp-facial-recognition-unlawful.html
https://www.wired.co.uk/article/face-recognition-police-uk-south-wales-met-notting-hill-carnival
https://www.zdnet.com/article/facial-recognition-could-be-most-invasive-policing-technology-ever-warns-watchdog/
https://onezero.medium.com/a-facial-recognition-giant-refuses-to-share-details-about-its-algorithm-dataset-df27a208683d
https://www.theregister.com/2020/06/30/nec_neoface_watch_afr_locate_details_liberty
https://www.biometricupdate.com/202007/nec-tells-uk-court-facial-biometrics-not-scraped-from-internet-but-declines-training-dataset-details
https://www.theguardian.com/technology/2020/aug/11/south-wales-police-lose-landmark-facial-recognition-case
https://onezero.medium.com/nec-is-the-most-important-facial-recognition-company-youve-never-heard-of-12381d530510
King's Cross live facial recognition
Met Police Gangs Violence Matrix
Page infoType: System Published: March 2023Last updated: November 2023"
Knightscope HP RoboCop ignores woman reporting crime,"A woman attempting to report a violent crime in a Los Angeles park was turned away by 'HP Robocop', a police robot on duty for Huntingdon Park Police Department.
Despite Cogo Guebara pushing the robot's its emergency alert button several times, it barked at Guebara to ‘Step out of the way’ and continued to glide along its pre-programmed route instead of offering assistance.
Media reports said people, especially children, said they felt generally reassured by the robot's presence. But some felt it resembled a form of creeping surveillance whilst others worry that a tendency amongst some to anthropomorphise the objects  highlights the disconnect between people’s expectations of the robot and the reality of its capabilities.
Knightscope robots combine self-driving technology, robotics and artificial intelligence to create what the company calls 'crime-fighting autonomous data machines.' According to Huntingdon police, leasing the robot for a year costs the city between USD 60,000-70,000, roughly equivalent to the annual salary of a Huntington Park police officer.
Operator: Huntington Park Police Department Developer: Knightscope
Country: USA
Sector: Govt - police
Purpose: Strengthen security
Technology: RoboticsIssue: Accuracy/reliability; Anthropomorphism; Surveillance
Transparency:"
BlessU-2 'Segensroboter' blessing robot religious role,"BlessU-2, a robot priest introduced by the Protestant Church in Wittenberg, Germany, to mark 500 years since the Reformation, triggered debate about the ethics, credibility, and effectiveness of robots as religious preachers.
Designed to provoke discussion about whether machines have a place within the clergy, BlessU-2 provides blessings in five languages and recites over 40 verses from the Bible. However, the bot also triggered debate on whether or not robots will induce religious decline, and about the potential for robots to replace religious jobs. 
Wittenberg is where Martin Luther sparked the Protestant Reformation by nailing his 95 theses to a church door in the town calling for religious reform.
Operator: LichtKirche Wittenberg Developer: LichtKirche Wittenberg Country: Germany Sector: ReligionPurpose: Stimulate discussion  Technology: Robotics Issue: Ethics; Employment Transparency:"
China social credit offence travel bans,"The Chinese government banned millions of people from travelling in 2018 for 'social credit' misdeeds, including unpaid taxes and fines, according to the AP.
China's National Public Credit Information Center annual report said would-be air travelers were blocked from buying tickets 17.5 million times; others were barred 5.5 million times from buying train tickets.
Beijing had announced in March 2018 it would introduce the travel ban two months later. Per Reuters, people would be put on the restricted lists if they had committed acts like spreading false information about terrorism, causing trouble on flights, using expired tickets or smoking on trains.
The Social Credit System is based on the principle of 'once untrustworthy, always restricted'.
Operator: Government of China Developer: Government of China
Country: China
Sector: Govt - police; Govt - security
Purpose: Assess creditworthiness, trustworthiness
Technology: Deep learning; Neural network; Machine learning Issue: Ethics; Fairness
Transparency: Governance"
Vumacam Johannesburg SafeCity system,"Vumacam built an AI-powered Safe City CCTV network with over 5,000 cameras in Johannesburg, South Africa. Video footage is combined with other tools such as license plate recognition to track population movements and trace individuals, using machine learning analytics to identify ‘unusual behaviour’.
 
An investigation by Vice magazine in 2019 identified biased outcomes, with reports suggesting that what constitutes 'abnormal behaviour detection' appeared to be racially biased. As of 2023, Vumacam is expanding its Safe City initiative to new cities in South Africa.
 
Civil rights organisations in South Africa have accused Vumacam of violating privacy rights by surveilling citizens without consent and of a lack of transparency over how the company uses the data collected. Organizations such as Right2Know have asserted that Vumacam violates a newly-passed data protection law in South Africa by collecting personal information such as license plate data without consent. Vumacam has stated that it does not have facial recognition technology enabled in its street cameras, nor does it save data in order to profile individuals.
Systems such as Vumacam’s raise concerns over a trend towards surveillance in public spaces that erodes privacy and civil liberties. In South Africa, the installation of these systems without adequate public oversight has also raised transparency concerns.
Operator: Vumacam Developer: VumacamCountry: South AfricaSector: Govt - police; Govt - security Technology: CCTV; Computer vision; Deep learning; Machine learning; Object recognitionIssue: Bias/discrimination - race; Governance; Human/civil rights; Legal; Privacy; SurveillanceTransparency: Governance; Policy"
Aadhaar glitches result in villagers' starvation,"Technical problems with India's Aadhaar biometric ID system have resulted in the deaths of scores of villagers in Jharkhand state and elsewhere, with some committing suicide and others suffering severe malnutrition. 
The glitches have meant that villagers have been unable to get food rations or subsidised grain, sometimes without explanation. Some groups, including vulerable minority groups such as the Parhaiya, have been denied their legal entitlement of subsidised grain for failing to correctly link their ration cards with Aadhaar. 
Campaigners told The Guardian that the system is fraught with problems, including authentication issues, with fingerprints scanning not working properly, officials failing to offer timely support, and poor internet coverage meaning people can often not access the system. 
Operator: Aadhaar Developer: Unique Identification Authority of India (UIDAI)
Country: India
Sector: Govt - welfare
Purpose: Reduce welfare fraud
Technology: Fingerprint biometrics Issue: Accuracy/reliability; Robustness
Transparency: Governance"
Medical robot tells man he is dying,"A man suffering from lung failure was told he was going to die by a doctor talking through a robot-mounted video screen at a Kaiser Permanente hospital in Fremont, California. Instead of being informed by a qualified doctor, 79-year-old Quintana was told of his fate through a RP-VITA telepresence robot equipped with a video screen.
Quintana's family was devastated by the incident, and implored the hospital to use human-beings to deliver news of this type. Hospital staff said they were acting in accordance with a new Kaiser Permanente policy. However, a company spokesperson said in a statement it was a 'highly unusual circumstance,' and that it would use it 'as an opportunity to review our practices and standards with the care team.'
Operator: Kaiser Permanante Medical Center Developer: InTouch Health; iRobot Country: USA Sector: HealthPurpose: Interact with patients remotelyTechnology: Robotics Issue: Ethics Transparency:"
Mark Zuckerberg 'Spectre' data sharing deepfake,"A deepfake video of Meta CEO Mark Zuckerberg talking about how he holds control over billions of people and his allegiance to 'Spectre' went viral, partly as Facebook was seen to remove something taking a pop at it's perceived hypocrisy, partly as it confused some people into thinking the fake video was real.
Made by UK-based artists Bill Posters and Daniel Howe, the deepfake was originally intended to highlight Facebook's use of behavioural psychology to persuade people to share intimate details of their lives so that they could be targeted with advertising. 
But Facebook's failure to remove a clearly faked video of US Senate speaker Nancy Pelosi apparently slurring her words persuaded the two to change course and lampoon Zuckerberg.
The deepfake was initially removed by Instagram content moderators on the basis that it violated its disinformation policy, but was restored when its satirical nature became evident and the potential impact of its removal became clearer.
According to Vice News, the original video of Zuckerberg is from a September 2017 address he had given about Russian election interference on Facebook.  
Operator: Bill Posters; Daniel Howe; Meta/InstagramDeveloper: Bill Posters; Daniel Howe
Country: UK; USA
Sector: Media/entertainment/sports/arts; Politics
Purpose: Expose hypocrisy
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Hypocrisy; Mis/disinformation
Transparency:"
Problem gambler AI detection,"The gambling industry is increasingly using artificial intelligence to target prospective punters and predict customer habits and maxmise revenue and profit, a move that is seen as controversial in some quarters. 
At the same time, machine learning and related technologies are being used to detect problem gamers. However, it remains unclear how effective these solutions are. 
In November 2019, the UK Betting and Gaming Council, an industry group representing 90% of the UK betting and gaming market, launched an AI-powered Anonymous Player Awareness System (APAS) designed to detect and prevent problematic behaviour in players. The system locks gamblers out of machines for 30 seconds if erratic or excessive play is detected. 
Experts said the APAS was a move in the right direction, but were concerned that 30 seconds is long enough to have any real impact. A 2019 study of Norwegian gambling machines found that the break caused 'no significant effect' on the amount of money staked during a subsequent gambling session or how long that session lasted.
In June 2021, Luke Ashton, from Leicester, UK, committed suicide after racking up large debts had been categorised as a 'low-risk' customer by a Betfair algorithm that had 'found nothing in his betting patterns that would trigger human intervention that might have restricted his gambling.' The coroner ruled Betfair had failed to meaningfully interact or intervene when Mr Ashton's gambling activity spiked.
Gambling venues across New South Wales, Australia, have introduced facial recognition to detect people who had voluntarily signed up to a 'self-exclusion' scheme for problem gamblers. Digital rights activists complained the technology is 'invasive, dangerous and undermines our most basic and fundamental rights'. 
Operator: Flutter UKI/Betfair; Ladbrokes/Coral Developer: Betting and Gaming Council (BGC); Mindway AI; Optimove Country: Australia; UK; USA Sector: Gambling Purpose: Detect problem gamblers Technology: Machine learning Issue: Accuracy/reliability; Effectiveness/value; Privacy; Safety; Surveillance Transparency: Governance; Black box; Marketing"
"Yang Mi, Athena Chu deepfake face-swap","A video that splices Chinese actress Yang Mi with Athena Chu, an older Hong Kong actress who starred in a 25-year-old TV series The Legend of the Condor Heroes has set China's social media alight, and not in an altogether positive manner.
Chinese 'netizens' criticised its creator for disrespecting both actresses and of violating Yang's image rights, and worried about the broader effects of the technology on society.
The video creator, who went under the pseudonym Xiao, (which translates as 'Brother Face-Swapping'), apologised, claiming he had wanted to educate the public on the danger of deepfakes.
The video was removed from Weibo, China's Twitter equivalent. However, many copies continue to circulate online.
Operator: Anonymous/pseudonymous; WeiboDeveloper: Anonymous/pseudonymous
Country: China
Sector: Media/entertainment/sports/arts
Purpose: 
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Privacy; Copyright; Ethics
Transparency: Governance"
Amazon Alexa records children's voices without consent,
"Malaysia minister, aide gay sex 'deepfake'","A gay sex video allegedly featuring Mohamed Azmin Ali, Malaysia's Minister of Economic Affairs, and Muhammad Haziq Abdul Aziz, a rival minister’s 27 year-old aide circulated on WhatsApp and more widely, leading to a political controversy, calls for resignations, and a police investigation.
Aziz swore that the video was real, but the minister and Prime Minister dismissed it as a deepfake, leading Aziz to post a 'confession' to Facebook a few hours later. 
As Intelligencer noted, the fact that the video may have been a deepfake may have allowed the minister to escape the serious legal consequences he may otherwise expect in a socially conservative country.
'Nowadays you can produce all kinds of pictures if you are clever enough,' then Malaysia Prime Minister Mahathir Mohamad retorted. 'One day you may also see my picture like that. It would be very funny.'
Operator: Unclear/unknown Developer: Unclear/unknown
Country: Malaysia
Sector: Politics
Purpose: Smear/discredit
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation
Transparency: Governance; Marketing
https://www.malaysiakini.com/news/479268
https://www.malaymail.com/news/malaysia/2019/06/12/the-real-haziq-not-as-built-santubong-pkr-chief-says-of-man-in-gay-sex-clip/1761392
https://www.malaymail.com/news/malaysia/2019/06/13/dr-m-if-gutter-politics-continues-my-turn-as-video-victim-may-come-after/1761603
https://nymag.com/intelligencer/2019/06/how-do-you-spot-a-deepfake-it-might-not-matter.html
https://www.businessinsider.my/one-day-you-may-see-my-picture-also-like-that-mahathir-says-sex-tapes-of-minister-are-fake-and-politically-motivated/
https://eandt.theiet.org/content/articles/2020/04/sex-coups-and-the-liar-s-dividend-what-are-deepfakes-doing-to-us/
https://www.wired.co.uk/article/how-to-spot-deepfake-video
https://www.malaysia-today.net/2019/06/15/is-it-azmin-or-a-deepfake/
https://theleaders-online.com/anwar-let-the-police-authenticate-the-clips/
https://malaysia.news.yahoo.com/report-experts-sex-videos-not-094410155.html
President Ali Bongo recovery deepfake broadcast
Rana Ayyub deepfake porn attack, doxxing
Page infoType: IncidentPublished: March 2023"
President Ali Bongo health recovery deepfake,"A TV broadcast appearance of Gabon's President Ali Bongo claiming he had recovered from ill-health set tongues wagging that the clip was a deepfake concocted by his government. 
Commentators and political adversaries noted that Bongo's appearance was marked his 'immobile' face, that his eyes were out of sync with his jaw, how little he blinked, and that his speech patterns appeared different.
The clip appeared at a time when the President was out of the country receiving medical treatment, fueling rumours that he was seriously unwell.
A week after the video’s release, Gabon’s military attempted a coup d'etat, citing the video as an indicator there was something wrong with the president. The coup attempt failed.
Some experts have since concluded the video may not have been a deepfake, though opinion remains divided.
Operator:  Developer: 
Country: Gabon
Sector: Politics
Purpose: Defend reputation
Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation
Transparency: Governance; Marketing"
Mindar humanoid robot Buddhist priest,"The appointment of Mindar, a 6-foot-4-inch, 132-pound humanoid robot, as a priest at Kyoto's Kodaiji Temple set tongues wagging, with some saying it is inappropriate, unethical, and sacriligious.
Developed by Osaka University roboticist Hiroshi Ishiguro, Mindar is designed to increase awareness about Buddhism, deliver sermons, and bridge the gap between the spiritual world.
Accustomed to robots in many aspects of their lives, Japanese people appear mostly to have taken to Mindar. However, westerners struggle with the concept of religious robots, with some likening Mindar to Frankenstein's monster. 
Mindar’s abilities are currently limited to citing a preprogrammed sermon about the Heart Sutra; the intention is to 'implement AI so Mindar can accumulate unlimited knowledge and speak autonomously.'
Operator: Kodaiji Temple, Kyoto Developer: Hiroshi Ishiguro Country: Japan Sector: ReligionPurpose: Increase religious awareness Technology: Robotics Issue: Anthropomorphism; Appropriateness/need; Ethics Transparency:"
Dominos Australia Pizza Checker,"The DOM Pizza Checker is a tool introduced by Domino's Pizza in Australia and New Zealand to check the quality of pizzas using a scanner, artificial intelligence, and machine learning. Developed by Israel-based Dragontail Systems, the Pizza Checker takes a picture of the pizza, recognises the type, analyses the distribution of toppings and cheese, and then grades it in line with a 'large databank of awesome pizzas'. 
The system reputedly increased product quality scores from customers of its Australian and New Zealand stores 15% one month after its introduction. But plans (pdf) to incorporate the product into a company 'scorecard' bonus system for franchises and to identify underperforming stores met with accusations of unnecessary scope creep and employee surveillance. 
Operator: Dominos PizzaDeveloper: Dragontail Systems Country: Australia Sector: Food/food services Purpose: Improve product quality Technology: Computer vision; Machine learning Issue: Dual/multi-use; Employment; SurveillanceTransparency: Governance"
Jumbo supermarket warned for 'indiscriminate' facial recognition,"A store was warned by Dutch privacy authority Autoriteit Persoonsgegevens not to use facial recognition cameras at its entrance as it would violate the privacy of people entering it.
Per BiometricUpdate, the regulator reported that Netherlands supermarket chain Jumbo's Alphen aan den Rijn store had erected notices at its entrance, but that it was judged too passive to be considered explicit consent. 
It also said Jumbo's use of facial biometrics was not in the public interest. Jumbo had earlier claimed it was the 'safest' store in the Netherlands due to its use of facial recognition.
Under the EU's General Data Protection Regulation (GDPR), organisations are allowed to surveil people if the subjects of their surveillance provide their explicit permission.
Operator: Jumbo Alphen aan den RijnDeveloper: i-Pro
Country: Netherlands
Sector: Retail
Purpose: Detect criminals
Technology: Facial recognition Issue: Privacy; Necessity/proportionality
Transparency: Governance; Marketing"
Jacksons Food Stores facial recognition,"Jacksons Food Stores is being sued (pdf) by two people for allegedly violating Portland, Oregon, rules governing the use of facial recognition in places of 'public accommodation'. 
The lawsuit also argues that although Jacksons is using the software to assist in the identification and prosecution of shoplifters, it can wrongly identify people as criminals, and that those errors 'disproportionately affect women and people of color.' 
The suit also claimed the identity verification process is mandatory, and that customers may not enter the store unless they let the software scan them. 
Reports said Jacksons has been using Blue Line Technology’s First Line software, which takes pictures of people as they approach a Jacksons store and then uses an algorithm to determine whether the face recorded by the camera matches a face on a database held by Jacksons.
Operator: Jacksons Food Stores Developer: Blue Line Technology; Dell; Axis
Country: USA
Sector: Retail
Purpose: Detect criminals
Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Privacy
Transparency: Marketing"
"Walmart AI anti-shoplifting system accuracy, effectiveness","Walmart's AI and camera-based anti-shoplifting system provided by Ireland-based company Everseen has come under fire in the US over its reportedly poor detection rates and tendency to misinterpret innocent behaviour as potential shoplifting.
A group of Walmart workers calling themselves 'Concerned Home Office Associates' circulated a video documenting flaws in the the company's 'Missed Scan Detection' system, including its regular failure to identify unscanned items, and incorrectly identifying personal items as potentially shoplifted.
The system is so poor that Walmart workers have complained about it continuously since it was first introduced in 2017, and call it 'NeverSeen,' according to WIRED.
Operator: Walmart Developer: Everseen Country: USA Sector: RetailPurpose: Reduce scanning errors, theft Technology: CCTV; Computer vision; Machine learning Issue: Accuracy/reliability; Effectiveness/value; Surveillance Transparency: Governance"
"Fraudsters clone CEO voice to steal USD 243,000","The CEO of the German operations of a UK energy company has been impersonated using deepfake audio technology and his company defrauded of USD 243,000. The UK CEO of the same company had been asked by his German counterpart to wire the money to a Hungarian supplier, which he promptly did. 
According to the company's insurer, Euler Hermes Group, the scammer had likely used commercially available AI voice-generating software to carry out the fraud. The incident demonstrated how easy it has become to clone someone else's voice and use it to defraud or misuse it in some other way.
Operator: Anonymous/pseudonymous Developer: Unclear/unknown
Country: Hungary
Sector: Energy
Purpose: Defraud
Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Privacy; Security
Transparency: Governance; Marketing"
Tesla Model S kills Florida Keys pedestrian,"A Tesla Model S blew through a three-way intersection, hit a Chevrolet Tahoe pick-up truck, causing it to spin, hit and kill a pedestrian. The woman, Naibel Benevides Leon, was thrown 70-80 feet into nearby woods whilst her companion was badly injured.
Police investigators said the Tesla's Autopilot driver-assist function appeared to be switched on, and that driver George McGee had dropped his phone, looked down, and ran a stop sign. 
In May 2021, Naibel Benevides' estate sued Tesla, accusing it of designing a 'defective and unsafe' car that failed to detect the Chevy Tahoe even though it was parked directly in front of the Tesla.
Operator: George McGee Developer: TeslaCountry: USA Sector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Driver assistance system Issue: Accuracy/reliability; Safety Transparency: Black box"
AnyVision 'Google Ayosh' Palestinian surveillance,"An 'advanced tactical surveillance' system called Better Tomorrow is being used by the Israelis across the West Bank and East Jerusalem to monitor the movement of Palestinians and deter attacks. 
NBC News reported that the system, developed by Israeli technology company AnyVision (latterly re-branded as Oosto) and  otherwise known as 'Google Ayosh', uses facial recognition to identify individuals, including women and children. The news attracted attention as Microsoft has invested in the company. Former AnyVision employees told NBC that the company failed to comply with Microsoft's ethical principles.
In March 2020 Microsoft divested its USD 74 million stake in AnyVision after protests by its employees and an audit led by former US Attorney General Eric Holder which concluded that 'the technology is used at border crossing checkpoints between Israel and the West Bank', whilst noting that 'available evidence demonstrated that AnyVision’s technology has not previously and does not currently power a mass surveillance program in the West Bank that has been alleged in media reports.'
AnyVision CEO Eylon Etshtein had previously denied any knowledge of the system and threatened to sue NBC News, saying that AnyVision was the 'most ethical company known to man,' disputed that the West Bank was 'occupied' and alleged NBC must have been funded by a Palestinian activist group. 
Operator: Israel Defense Forces Developer: Oosto/AnyVision Interactive Technologies Country: Israel Sector: Govt - military; Govt - security Purpose: Population surveillance Technology: Facial recognition Issue: Ethics; Surveillance; Privacy Transparency: Governance; Privacy"
Instacart personal shopper pay algorithm backlash,"US grocery and pick-up company Instacart is facing a nationwide customer boycott on account of the poor pay doled out to its personal shoppers.
In an open letter to Instacart CEO Fidji Simo, the Gig Workers Collective encouraged customers to support delivery workers by boycotting the company until it 'rectifies the genuinely inequitable manner in which it treats its shoppers.'
The boycott is the latest in a series of boycotts, walk-offs and strikes driven by low pay, failure to reimburse workers for business expenses, 'stolen' tips subsidising worker pay and other issues that have dogged Instacart in recent years - issues seen to have been aggravated by management greed and an increasing reliance on automation and algorithms.
The company revised its personal shopper pay system early 2019 after shoppers walked out over an October 2018 update to the system that resulted in 'substantially' lower pay, and customers complained on social media that their orders were being delayed. 
And in November 2019 it controversially withdrew a USD 3 'quality bonus' personal shoppers received for every five-star rating they garnered from customers after a three-day worker pay strike.
Operator: Instacart Developer: InstacartCountry: USASector: Transport/logistics Purpose: Calculate pay Technology: Pay algorithm Issue: Employment - pay; Fairness Transparency: Governance; Compaints/appeals; Black box"
Joe Rogan libido booster Alpha Grind deepfake,"Controversial podcaster Joe Rogan has had his identity stolen and deepfaked in a video ad to push Alpha Grind, a male enhancement product that markets itself as 'For Men with the Highest Expectations'.
The video clip, which shows Rogan discussing Alpha Grind with guest Professor Andrew D. Huberman on The Joe Rogan Experience podcast, sparked uproar on Twitter, with people noting that it is illegal to steal someone's identity to promote a product using AI.
The fake ad was circulating freely on TikTok until it was spotted by Jimmy Farley and removed from the platform. It was also refuted by Huberman.
Futurism notes that Rogan is known to push all manner of iffy products, some of them quietly owned by companies he owns outright or has invested in. But that doesn't detract from the fact that, in this instance, he is the victim.
Operator: Anonymous/pseudonymous; TikTokDeveloper: Unclear/unknown Country: USA Sector: Health Purpose: Sell product Technology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Mis/disinformation Transparency: Governance; Marketing"
ZAO app face swapping,"An app that enables users to superimpose a single image of their face onto videos of celebrities such as Leonardo di Caprio and Marilyn Monroe, ZAO quickly surged in popularity and was the most downloaded free app in China’s iOS App Store immediately after its launch in September 2019.
But with users and rights advocates concerned about privacy, copyright, and security, it also proved highly controversial.
One section of the app's user agreement stated that users uploading their images to ZAO gave it 'free, irrevocable, permanent, transferable, and relicenseable' rights over the intellectual property rights to their faces, and permitted ZAO to share their images with whomever they chose and use their images for marketing purposes.
ZAO lists Changsha Shenduronghe Network Technology, a wholly owned subsidiary of Momo that owns a live-streaming and dating service, as its developer. Momo apologised for the terms of the agreement and revised it to say that it would not 'excessively collect user information.'
Despite Momo's assurances, the ease with which identities could be swapped led platforms such as China's Weixin/WeChat to ban users from uploading ZAO-made videos to them on the basis that they were seen as a potential security risk.
Operator: Momo; AppleDeveloper: Momo Country: ChinaSector: Media/entertainment/sports/arts Purpose: Swap faces Technology: Deepfake - imageIssue: Privacy; Copyright; Security Transparency: Governance; Privacy"
FaceApp facial transformations,"FaceApp is a mobile-based that enables users to transform their facial photographs, enabling them to appear younger or older, add a smile, facial hair or glasses, change eye colour, hairstyle or gender, or become 'hot'.
The app quickly became popular, with over a million people downloading it in the two weeks after its launch. In March 2023, it had over 500 million downloads, according to its website.
However, concerns have also been raised about FaceApp's privacy practices and perceived bias towards racial and ethnic minorities, and to LGBTQ and transgender people. 
In 2019, it was reported that FaceApp was storing users' photos on its servers in Russia rather than on their phones, and that their metadata was being collected. In addition, concerns were raised that FaceApp's privacy policy allowed it to use people’s usernames, names, and likeness for commercial purposes.
The reports prompted US senator Chuck Schumer to urge the FBI and Federal Trade Commission to investigate FaceApp, saying it could pose 'national security and privacy risks for millions of US citizens'. In response, the FBI said 'it considers any mobile application or similar product developed in Russia, such as FaceApp, to be a potential counterintelligence threat'.
FaceApp CEO Yaroslav Goncharov responded by saying that only the picture chosen by the user is uploaded and stored on cloud computing services provided by Amazon and Google, and that the app did not harvest a user’s mobile photo library.
Soon after its launch, FaceApp received considerable criticism for its 'hot' filter, which was supposed to make users more physically attractive, but which whitened skin tones, resulting in accusations that the app was 'racist'. 
Goncharov apologised for the feature, saying it was due to a flaw in the underlying neural network, which was skewed towards European/Caucasian faces. 
A few months later, FaceApp introduced a series of 'ethnicity filters', allowing users to become 'black,' 'Asian', 'Caucasian,' or 'Indian.' It quickly withdrew the feature after accusations of racism and stereotyping. 
Operator: FaceApp Technology Developer: Yaroslav Goncharov Country: Russia; CyprusSector: Media/entertainment/sports/arts Purpose: Transform faces Technology: Deep learning; Neural network; Machine learning Issue: Privacy; Bias/discrimination - race, ethnicity, LGBTQ, transgender Transparency: Governance; Marketing; Privacy
FaceApp website
FaceApp Wikipedia profile
Senator Schumer FBP, FTC letter
https://www.forbes.com/sites/thomasbrewster/2019/07/17/faceapp-is-the-russian-face-aging-app-a-danger-to-your-privacy/
https://www.wsj.com/articles/as-faceapp-goes-viral-so-do-concerns-about-privacy-russia-ties-11563485572
https://www.smh.com.au/technology/what-s-fact-and-what-s-fiction-when-it-comes-to-faceapp-20190717-p5284v.html
https://techcrunch.com/2019/07/16/ai-photo-editor-faceapp-goes-viral-again-on-ios-raises-questions-about-photo-library-access-and-clo/
https://www.theguardian.com/technology/2019/jul/17/faceapp-denies-storing-users-photographs-without-permission
https://www.dailymail.co.uk/sciencetech/article-7260463/Faceapp-access-camera-roll.html
https://www.boston.com/news/technology/2019/07/17/faceapp-safe-privacy
https://nypost.com/2019/07/17/faceapp-security-concerns-russians-now-own-all-your-old-photos/
https://www.thesun.co.uk/tech/9526114/faceapp-app-photos-safe-dangerous-upload/
https://www.cbsnews.com/news/faceapp-russian-app-sparks-myths-and-fears-about-privacy-and-data-use/
https://www.cbsnews.com/news/faceapp-top-democrat-chuck-schumer-urges-fbi-to-investigate-troubling-russian-app/
ZAO face swapping
DeepFaceLive face swapping
Page infoType: SystemPublished: March 2023"
DeepNude nudification app,"DeepNude was an app that enabled users to strip any woman of her clothes and see her naked within a few seconds for USD 50. 
Launched in June 2019, DeepNude immediately proved controversial, with civil and privacy rights advocates complaining that it was unethical, objectified women, and could be used for revenge porn and other nefarious purposes. 
The anonymous developer of the app told Vice that they were 'not a voyeur, I'm a technology enthusiast', and that they also wanted to create a male version.
The app was quickly removed, with the developer having reputedly grappled with their ethical conscience.
Operator: DeepNude Developer: Anonymous/pseudonymousCountry: Estonia; Global Sector: Media/entertainment/sports/arts Purpose: Undress women Technology: Deepfake - imageIssue: Privacy; Ethics; Bias/discrimination - gender  Transparency: Governance; Privacy"
Engineer.ai misleading marketing,"Claims by Indian start-up Engineer.ai that it had developed a highly automated, AI-enabled app development platform for small businesses was revealed to be far from the truth.
The Los Angeles and Delhi-based company trumped its 'human-assisted' AI, but the Wall Street Journal found when talking to Engineer.ai employees and former employees that it used humans rather than AI to develop its app, and grossly inflated its marketing rhetoric to attract customers and investors.
The allegations came after Engineer.ai founder and CEO Sachin Duggal claimed at a conference that 82 percent of the work on the event’s app was done by AI in under an hour, and that human engineers worked for five weeks to finish the rest of it. 
In February 2019, Engineer.ai chief business officer Robert Holdheim had filed a wrongful termination complaint in a Los Angeles court alleging the company had been exaggerating its AI abilities to attract the funding it required to develop its technology.
Duggal responded to the WSJ report by saying the company had never claimed to offer 'automated software development'. 
The company re-branded as Build.ai in November 2019.
Operator: Engineer.ai Developer: Engineer.aiCountry: IndiaSector: Business/professional services Purpose: Automate app development Technology:  Issue:  Transparency: Marketing"
Microsoft Zo chatbot,
Amazon accused of promoting anti-vaccine propaganda,"Books and movies full of anti-vaccination conspiracy theories were found to have been dominating Amazon's online stores, raising concerns that the company prioritises free speech over public health.
A search for the word 'vaccine' on Amazon US returned books and movies dominated by anti-vaccination content, with 15 of 18 books and movies listed containing anti-vaccination content. 
These were published under titles such as 'We Don’t Vaccinate!' and 'Shoot ‘Em Up: The Truth About Vaccines,' as well as under more misleading titles like 'Miller’s Review of Critical Vaccine Studies: 400 Important Scientific Papers Summarized for Parents and Researchers,' according to CNN.
The finding prompted US Congressman Adam Schiff to complain to Amazon CEO Jeff Bezos that his company was serving up anti-vaccine propaganda and accepting paid advertising for anti-vaccine media.
Operator: Amazon usersDeveloper: Amazon Country: USA Sector: RetailPurpose: Recommend books, moviesTechnology: Recommendation algorithmIssue: Mis/disinformation; Freedom of expression - censorship Transparency: Governance"
Amazon HR system automatically fires 'inefficient' warehouse workers,"An Amazon HR system automatically monitored and sacked 'hundreds' of workers at a single warehouse who were not considered to be working fast enough.
In a letter to the US National Labor Relations Board shared with The Verge, an attorney at law firm Morgan Lewis & Bockius representing Amazon responded to a termination complaint by describing (pdf) how Amazon fired 'hundreds' of workers at a single facility between August 2017 and September 2018 for failing to reach productivity targets and quality-control mandates.
The system 'automatically generates any warnings or terminations regarding quality or productivity without input from supervisors', according to the letter. It also said that a worker would receive a termination notice if they were given six warnings in a 12-month period. 
Amazon responded by denying it sent out automatically-generated termination notices and said its warehouse workers are given training to help them improve if they miss targets.
Operator: Amazon Developer: AmazonCountry: USASector: Transport/logistics Purpose: Improve productivity Technology: Machine learning Issue: Employment; FairnessTransparency: Governance"
Amazon Echo Dot Kids remembers kids' conversations,"A coalition of nineteen privacy groups has filed a legal complaint with the US Federal Trade Commission (FTC) alleging that Amazon was holding onto a child’s personal information for too long and violating the US Children’s Online Privacy Protection Act (COPPA).
The coalition, led by Campaign for a Commercial-Free Childhood (CCFC), Center for Digital Democracy (CDC), and Georgetown University’s Institute for Public Representation, said Amazon's Echo Dot Kids smart speaker records and collects 'vast amounts of sensitive, personal information from children under 13' without adequate parental consent.
The coalition also discovered that parents are unable to delete certain personal details - including date of birth - using the FreeTime feature on Amazon Alexa mobile app once a child tells the Echo Dot Kids Edition to remember them. 
Amazon responded by saying its Echo Dot Kids Edition was compliant with COPPA. 
CCFC and CDD had issued a May 2018 warning that Echo Dot endangers children’s privacy and threatens their healthy development by encouraging them to spend more time with and form 'faux relationships' with digital devices.
Operator:  Developer: Amazon
Country: USA
Sector: Consumer goods
Purpose: Provide information, services
Technology: Speech recognition; Natural language understanding (NLU) Issue: Privacy
Transparency: Governance; Privacy; Marketing
Amazon Alexa developer website
Amazon Alexa Wikipedia profile
Campaign for a Commercial-Free Childhood, Center for Digital Democracy (2019). Advocates Demand FTC Investigation of Echo Dot Kids Edition
Campaign for a Commercial-Free Childhood, Center for Digital Democracy (2018). Experts and Advocates Caution Parents to Steer Clear of New Amazon Echo Dot for Kids
Consumer Reports (2019). Amazon Echo Dot Kids Violates Privacy Rules, Advocates Claim
https://www.apnews.com/f062c28ae72144b3b22146d9d4c6fab3
https://www.thesun.co.uk/tech/9034852/amazon-echo-kids-alexa-recording-conversations/
https://www.nbcnews.com/tech/tech-news/amazon-accused-violating-children-s-privacy-kid-friendly-smart-speakers-n1003706
https://techcrunch.com/2019/05/09/alexa-does-the-echo-dot-kids-protect-childrens-privacy/
https://www.cnet.com/news/amazons-echo-dot-kids-violates-privacy-regulations-child-advocates-say/
https://www.geekwire.com/2019/alexa-illegally-record-children-amazon-sued-allegedly-storing-conversations-without-consent/
https://www.vox.com/the-goods/2019/6/14/18679360/amazon-alexa-federal-lawsuit-child-voice-recording
Amazon Alexa records children's voices without consent
Amazon Alexa mistakenly orders USD 160 dollhouse
Page infoType: IncidentPublished: March 2023"
Amazon employees listen to Alexa recordings,
"Amazon retains Alexa recordings, transcripts indefinitely",
Amazon Rekognition falsely links athletes to mugshots,"Amazon’s Rekognition system falsely linked the faces of 27 professional athletes in New England, USA, to mugshots in a criminal database. 
The Massachusetts ACLU used Amazon's Reknognition facial recognition system to filter 188 well-known local athletes through a database of 20,000 mugshots, and found the product misidentified 27 of them. The result was verified by an independent industry expert.
Amazon retorted that the ACLU had been 'knowingly misusing and misrepresenting Amazon Rekognition to make headlines' and that Rekognition could help identify criminals and missing children when used with its recommended 99 percent confidence threshold.
The incident was seen to underscore issues with the accuracy and reliability of Rekognition, and to highlight its implications for civil rights and liberties, including racial bias and discrimination.
It also prompted rights activists and others to call for a moratorium on government use of facial recognition techniology, and for dedicated federal and local legislation.
Operator: American Civil Liberties Union (ACLU)  Developer: Amazon/AWS Country: USASector: Media/entertainment/sports/arts Purpose: Strengthen law enforcement Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race; Human/civil rights Transparency:"
Study finds Amazon Rekognition suffers from racial and gender bias,"An MIT Media Lab study concluded that Amazon's Rekognition facial recognition system performed worse when identifying an individual’s gender if they were female or darker-skinned.
The MIT researchers compared tools from five companies, including Microsoft and IBM, and found that Rekognition performed the worst when it came to recognising women with darker skin, with an error rate of 31.37 percent. It also mistook women for men 19 percent of the time.
Amazon claimed the research was misleading as the researchers had not tested the most recent version of Rekognition, and that the gender identification test was facial analysis (which spots expressions and characteristics like facial hair) rather than facial identification (which matches scanned faces to mugshots).
In their paper, the researchers also argued that issues other than algorithmic fairness should be considered. 'The potential for weaponization and abuse of facial analysis technologies cannot be ignored nor the threats to privacy or breaches of civil liberties diminished even as accuracy disparities decrease,' they wrote.
Operator: MIT Media Lab, Joy Buolamwini, Deborah Raji Developer: Amazon/AWS Country: USASector: Education Purpose: Identify individuals Technology: Facial recognition Issue: Bias/discrimination - racial, gender; Dual/multi-use Transparency:"
DukeMTMC facial recognition dataset,"DukeMTMC is a dataset of video footage taken on Duke University's campus in 2014 with the aim of accelerating advances in 'multi-target, multi-camera tracking' using person re-identification and low-resolution facial recognition.
Published (pdf) in 2016 by Duke University academics and researchers, the dataset consists of over 2 million frames of 2,000 students captured using 8 cameras expressly set up to capture students 'during periods between lectures, when pedestrian traffic is heavy'.
The project was shut down after the publication of researcher Adam Harvey's Exposing.ai project and a Financial Times investigation into facial recognition data sharing.
As reported in Duke's Chronicle newspaper, the university's Institutional Review Board said it had approved a study that would take place in a 'defined indoor space' and create a dataset that would be accessible only upon researchers’ request.
Carlo Tomasi, Iris Einheuser professor of computer science at Duke and an author of the study research paper, later apologised for running the study outdoors and for making it publicly available. 
Though DukeMTMC had been released under a CC BY-NC-SA 4.0 license, which allows for attributed, non-commercial sharing and adaption of the dataset, it has been and continues to be used more broadly.
Analysis by Adam Harvey shows that DukeMTMC has been cited by hundreds of research studies across the world, with over twice as many originating in China as in the United States.
Chinese citations show the dataset was used by a wide range of academic institutions and companies with known links to the Chinese military and to Chinese government surveillance of Uyghurs in Xianjiang and elsewhere. 
These organisations include Hikvision, Megvii (Face++), SenseTime, Beihang University, China's National University of Defense Technology, and the PLA's Army Engineering University.
Harvey also points out that the project was 'supported in part by the United States Army Research Laboratory' and was for 'automated analysis of crowds and social gatherings for surveillance and security applications.'
Duke University may have removed the DukeMTMC dataset from its website, but multiple versions and extensions remain available on Github and elsewhere and the original dataset continues to be used for research. 
Operator: CloudWalk; Hikvision; Megvii; SenseNets; SeeQuestor; SenseTime; Beihang University; National University of Defense Technology, China; NEC; PLA Army Engineering University  Developer: Ergys Ristani; Francesco Solera; Roger Zou; Rita Cucchiara; Carlo Tomasi; Duke UniversityCountry: USA Sector: Technology; Research/academia Purpose: Train facial recognition systemsTechnology: Dataset; Facial recognition; Computer vision Issue: Privacy; Ethics; Dual/multi-use Transparency: Governance; Privacy"
Henn-na Hotel lays off half of robot staff,"Japan’s Henn-na Hotel decommissioned half of its 243 robot employees after they malfunctioned, failed to do their jobs, and proved irritating, drawing complaints from customers.
The Wall Street Journal reported that the velociraptor check-in robots were unable to handle foreign guests or photocopy passports, the concierge robot was unable to answer questions about flight schedules and tourist attractions, robot luggage carriers could only reach about a quarter of the rooms, failed in rain or snow, and would get stuck trying to pass each other in corridors. 
Henn-na ('weird') Hotel had heavily hyped the robots upon their introduction in 2015. But they appear quickly to have outlived their usefulness and value, and have in many instances been replaced with humans.
Operator: H.I.S. Hotel Group Developer: MJI Robotics Country: Japan Sector: Travel/hospitality Purpose: Improve customer service Technology: Robotics Issue: Appropriateness/need; Effectiveness/value Transparency: Marketing"
Tesla Model X fatal semi-automated car crash,"A Tesla Model X driven to work by Apple engineer Wei 'Walter' Huang veered into a highway safety barrier in Mountain Valley, California, caught fire and was rear-ended by two other cars. Huang later died from injuries in Stanford Hospital. 
A few days after the crash, Tesla acknowledged in a blog post that the car's Autopilot driver-assistance system had been engaged at the time of the crash and that Huang's hands were not detected on the wheel for six seconds prior to the collision. 
Tesla's move violated an agreement between the US National Transportation Safety Board (NTSB) and the automaker that Tesla would not comment on any crash during the course of the investigation and prompted the NTSB to remove the car maker as a party to its investigation. 
In April 2019, Huang's family filed a lawsuit against Tesla and California state alleging that Tesla’s Autopilot driver assistance system misread lane lines and failed to detect the safety barrier, in which the car accelerated rather than braked. The suit also accused Tesla of defective product design, and false advertising.
A 2020 NTSB investigation concluded that Autopilot was one of the probable causes of the crash, and that Huang had been 'overly confident' in the system's capabilities, evident in the fact that he had been playing a mobile game while using Autopilot before the crash. It also accused the National Highway Traffic Safety Administration (NHTSA) of taking an overly hands-off approach to regulating automated driving systems.
In January 2023, Tesla senior engineer Ashok Elluswamy testified during the trial that a 2016 Tesla video used to promote Autopilot had been staged to show capabilities like stopping at a red light and accelerating at a green light that the system did not have. 
Operator: Walter HuangDeveloper: Tesla
Country: USA
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Safety; Accuracy/reliability
Transparency: Black box; Marketing"
Uber self-driving car kills Arizona pedestrian,"In March 2018, Rafaela Vasquez, the test driver of an Uber autonomous car, crashed into a woman walking her bicycle across a road in Tempe, Arizona. Operating in self-drive mode with Vasquez in the driving seat, the Uber fatally struck Elaine Herzberg. The incident was the first known case of a fatality involving a self-driving car.
Uber escaped (pdf) prosecution, but Vasquez was indicted by prosecutors in Arizona in August 2020 on a count of negligent homicide on the basis that she was checking Slack messages from Uber on her work mobile phone and watching a reality show on her personal phone. In July 2023, Vazquez pleaded guilty to one count of endangerment and was sentenced to three years of supervised probation, with no time in prison.
The case was seen as a test of with which party legal liability lies. A US National Transportation Safety Board (NTSB) investigation concluded the Uber car had failed to identify Herzberg as a pedestrian and to apply its brakes. It also found that Uber maintained an 'inadequate safety culture.'
Uber suspended its self-driving test programme following the incident, later restarting it in Pittsburgh.
Operator: Uber Developer: Uber Country: USASector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Safety; Accuracy/reliability; Liability Transparency: Black box"
GM Cruise fails to yield to pedestrian at crosswalk,"A self-driving Chevrolet Bolt EV test car was issued a ticket in San Francisco for not yielding to a pedestrian at a crosswalk. The car was pulled over by a police officer shortly after having gone through the crosswalk.
The Bolt EV had been in an autonomous driving mode, meaning that its sensors were collecting data as it drove down the street and the car logged the information as it used it to make decisions on how to operate.
Cruise contested the ticket on the basis that data from the car suggests the pedestrian was 10.8 feet away when it passed through the intersection, and that the pedestrian had not been put in danger.  
Operator: GM Cruise Developer: GM Cruise; General Motors/Chevrolet Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Safety; Accuracy/reliability; Legal - liability Transparency: Governance; Black box
Cruise website
Cruise Wikipedia profile
https://sanfrancisco.cbslocal.com/2018/03/27/self-driving-car-ticketed-san-francisco/
https://electrek.co/2018/03/30/self-driving-chevy-bolt-ev-test-car-ticket-pedestrian-gm/
https://www.thedrive.com/news/19805/one-of-gms-cruise-self-driving-cars-just-got-a-ticket-in-california
https://www.theguardian.com/technology/2018/mar/06/california-self-driving-cars-attacked
https://www.businessinsider.com/gm-cruise-self-driving-car-ticket-not-yielding-pedestrian-2018-3
https://arstechnica.com/cars/2018/03/a-cruise-car-got-a-traffic-ticket-gm-says-it-did-nothing-wrong/
https://www.carscoops.com/2018/03/self-driving-chevy-bolt-ticketed-close-pedestrian-san-francisco/
https://www.futurecar.com/2111/One-of-General-Motors-Autonomous-Vehicles-Got-a-Ticket-in-San-Francisco
https://www.gm-volt.com/threads/self-driving-chevy-bolt-ev-ticketed-in-san-francisco.338425/
https://www.autoblog.com/2018/03/30/cruise-autonomous-car-ticket-sf/
Cruise driverless car pulls away from police
Cruise AV impedes San Francisco firefighters
Page infoType: IncidentPublished: March 2023"
AI automated trades cost investor USD 20 million,"An AI-powered investment decision-making system lost tens of millions of US dollars of a client's capital, prompting questions about whether the system was fit for purpose, and a legal dispute over who was liable for its losses.
Wanting a fund that would trade with no human intervention so as to remove emotion and bias, MMWWVWM Limited (VWM) asked Monaco-based investment manager Tyndaris SAM to manage its capital using an AI system run on its K1 supercomputer, reckoned to be capable of applying machine learning to real-time news, social media data, and other sources, to predict sentiment in the capital markets.
But VWM quickly racked up an estimated USD 22 million of losses, and asked Tyndaris to suspend its account. Tyndaris responded by claiming approximately USD 3m from VWM in unpaid fees, prompting VWM to counterclaim on the basis that Tyndaris had misrepresentated the capabilities of its system.
Operator: Tyndaris SAMDeveloper: Tyndaris SAM; Raffaele Costa  Country: Hong Kong Sector: Banking/financial servicesPurpose: Make investment decisions Technology: Trading algorithm Issue: Accuracy/reliability; Effectiveness/value; Legal - liability Transparency: Marketing"
Chinese schools 'intelligent uniform' monitoring,"School children at 11 schools in southern China are being forced to wear to wear 'intelligent unforms' to improve attendance and increase safety, prompting a backlash from privacy advocates.
The uniforms have two GPS chips embedded into the shoulder pads that show when a student is entering or exiting school grounds, and automatically sends the data to parents and teachers. They are also linked to the child’s face, so students wearing swapped uniforms are identified at the school entrance using facial recognition.
The clothing, which developed by Guizhou Guanyu Technology, was quietly introduced in July 2017. It came under scrutiny when it emerged that the technology can also track student movements and behaviour after school hours, though school leaders say they 'choose not to.'
Operator: No. 11 School of Renhuai, Guizhou Province Developer: Guanyu Technology Country: China Sector: Education Purpose: Improve safety; Reduce truancy Technology: Facial recognition; GPS Issue: Privacy; Surveillance Transparency: Governance"
Hangzhou No. 11 Middle School student surveillance,"The installation of a 'smart classroom behaviour management system' which analyses students to make sure they are paying attention triggered a backlash from students, parents, and privacy advocates.
Jointly developed by Hikvision and Hangzhou No. 11 Middle School, the system's 'wisdom eyes' scan students’ faces every 30 seconds to identify seven types of emotions and six types of behaviour, with students receiving a real-time attentiveness score. Teacher display screens issue notifications about which students were inattentive twenty minutes into each class. 
The Hangzhou school vice principal said that after a month-long trial, students had begun to accept the monitoring and had improved their behaviour. But the action resulted in debate on the value and effectiveness on the system, and others like it. It also triggered a backlash from students, parents and privacy advocates, leading to programme being stalled.
Operator: Hangzhou No. 11 Middle School Developer: China Electronics Technology Group/Hikvision
Country: China
Sector: Education
Purpose: Assess student attentiveness
Technology: Facial recognition; Emotion recognition; Deep learning; Neural network; Machine learning Issue: Accuracy/reliability; Bias/discrimination - gender, ethnicity; Surveillance; Privacy; Security
Transparency: Governance; Marketing; Privacy"
UCLA abandons facial recognition surveillance plans,"University of California, Los Angeles (UCLA) abandoned plans to install facial recognition after a backlash from students and others concerned about its potential for discrimination, surveillance, and impact on privacy.
UCLA announced (pdf) in September 2018 that it was planning to introduce facial recognition in order to improve campus safety and centralise campus security camera systems and give university police access to footage during emergencies.
The move resulted in a backlash from students and a campaign by digital rights advocacy group Fight for the Future, which used Amazon's facial recognition software Rekognition on UCLA sportspeople and faculty to demonstrate the technology's capacity for delivering false matches.
Backing down from the plan, UCLA  Administrative Vice Chancellor Michael Beck said, 'the potential benefits are limited and are vastly outweighed by the concerns of the campus community.' 
UCLA would have been the first university in the US to adopt facial recognition.
Operator: UCLA Developer: Unclear/unknownCountry: USASector: EducationPurpose: Strengthen security; Increase safety Technology: Facial recognition Issue: Bias/disrimination - race, ethnicity; Effectiveness/value; Privacy; Surveillance Transparency: Governance"
Kiwibot food delivery robot catches fire,"A Kiwibot food delivery robot caught fire on the UC Berkeley campus, resulting in it having to be doused in foam by the local fire department. No one was harmed.
The delivery service company later said it reckoned the fire was caused by human error after a faulty battery had been manually inserted into the robot.
Per The Verge, Kiwibot was launched in 2017, and its bots are designed to handle the last 300 meters of food deliveries. 
Operator: UC Berkeley Developer: Kiwibot Country: USA Sector: Education; Transport/logistics Purpose: Deliver food Technology: RoboticsIssue: Safety Transparency:"
AI confuses bus ad for jaywalker,"An AI system in the Chinese city of Ningbo has mistakenly accused a woman pictured on the side of a bus of jaywalking. 
The system had reacted to an advert on the side of a bus that showed the face of Dong Mingzhu, CEO of Gree Electric Appliances, China's biggest air-conditioner maker. 
Ms Dong's face had subsequently been shown on a large display on a roadside in an effort to shame her. Meantime, the real jaywalker walked free.
Ningbo police said they had deleted the photo of Ms Mong and would update the AI system so that it could differentiate between ads and people. 
Operator: Ningbo City Police Developer: Unclear/unknown Country: China Sector: Govt - municipal; Govt - police Purpose: Improve street safety Technology: Facial recognition Issue: Accuracy/reliability Transparency: Governance"
UK Met Police Gangs Violence Matrix,"Gangs Violence Matrix (GVM) is a controversial database of thousands of alleged street gang members developed and operated by London's Metropolitan Police Service ('Met Police'). 
The GVM uses one or more algorithms to rank and categorise people included on the database, based on the risk each 'gang member' poses to others, and the extent to which the police and partner agencies interact with that person. 
The GVM (also known as 'Gangs Matrix') has been subject to multiple investigations, research studies, and legal reviews by Amnesty, Liberty, StopWatch, amongst others. 
It has also been the subject of a high-profile two-part review (part one, two - pdf) ordered by London mayor Sadiq Khan, and an independent review commissioned by then UK Prime minister David Cameron of England and Wales' criminal justice system by David Lammy MP. 
These studies variously conclude that the GVM is inaccurate, discriminates against racial and ethnic minorities, and is, according (pdf) to Amnesty, 'unfit for purpose'.
In 2018, the Met Police's use of Gangs Matrix was served (pdf) with an enforcement notice by the UK privacy commissioner for 'potentially' breaking data protection laws, and for its failure to distinguish victims of crime and offenders. The notice was lifted in 2021. 
In October 2022, the Met Police announced it had removed over 1,200 names from the list. 
The GVM database has been shrouded in secrecy since it was quietly introduced in 2011. 
The Met does not inform people included on the database. Nor is there is any mechanism by which those included can discover what information is help about them, ask for their data to be reviewed or removed, or appeal against their inclusion. 
It is unclear whether the GVM is shared with other organisations. Reports indicate it is shared with other UK government departments, including immigration enforcement.
Operator: Metropolitan Police Service (MPS) Developer: Metropolitan Police Service (MPS) Country: UKSector: Govt - police Purpose: Predict gang violence risk Technology: Ranking algorithm Issue: Bias/discrimination - race, ethnicity, income, geography; Accuracy/reliability; Privacy Transparency: Governance; Complaints/appeals; Black box"
Uber ID algorithm suspends transgender drivers,"Transgender drivers working for Uber in the USA reported having their accounts temporarily or permanently suspended due to problems with the company's identity verification system. 
Introduced in 2016, Uber's Real-time ID Check aims to 'protect both riders and drivers' by comparing drivers' selfies taken while working with photographs on file, temporarily suspending the accounts of those that do not match.
However, as illustrated by the experiences of drivers Janey Webb and 'Lindsay', the system failed to take into account the changes in physical appearance that come when people transition gender, forcing them travel lengthly distances to Uber support centres, and to miss work.
Operator: Uber Developer: Microsoft
Country: USA
Sector: Transport/logistics
Purpose: Verify identity
Technology: Facial recognitionIssue: Accuracy/reliability; Bias/discrimination - sexual preference (LGBTQ); Employment - pay, jobs
Transparency: Governance; Black box; Complaints/appeals"
Hikvision Uyghur ethnic minority analytics,
"Shenzhen uses facial recognition to catch, shame jaywalkers","Authorities in Shenzhen, China, have launched CCTV cameras incorporating facial recognition and artificial intelligence linked to a database in a bid to crack down on jaywalking, traffic violations, and other crimes. 
Built by local technology company IntelliFusion, the DeepEye system captures images of people illegally crossing the road, identifies the citizen against a database and displays their photo alongside their family name and part of their government identification number on a roadside LED screen and government website. 
According to a Shenzhen government official, 'a combination of technology and psychology… can greatly reduce instances of jaywalking and will prevent repeat offences.'
The move has prompted some locals and commentators to express their concerns about the intrusivess of the system, and its potential for deepening and expanding state surveillance.
Local media report that jaywalking and other crimes added to the system also potentially damage one's score in China's 'social credit system'. 
Operator: Shenzhen Traffic Police Bureau Developer: Intellifusion
Country: China
Sector: Govt - municipal; Govt - police
Purpose: Identify jaywalkers, criminals
Technology: Facial recognition; Automated license plate/number recognition (ALPR/ANPR)Issue: Privacy; Surveillance
Transparency: Governance"
Babylon Health diagnostic chatbot,
Malfunctioning robot impales Chinese factory worker,"A worker at a porcelain factory in Hunan province, China, has been impaled by ten steel bars in the arm and chest after a robotic arm fell onto him.
A robotic arm suddenly collapsed from a machine and fell onto the worker, its spikes spearing his body. Each of the sharp steel bars measured 30 centimetres (one foot) in length and 1.5 centimetres (0.59 inches) in diameter, according to the hospital which treated the worker. 
Surgeons managed to remove all the spikes from the worker's body. 
Operator: Zhuzhou porcelain factoryDeveloper:  Country: ChinaSector: Manufacturing/engineering Purpose: Assemble components Technology: Robotics Issue: SafetyTransparency: Governance
https://www.dailymail.co.uk/news/article-6483365/Chinese-worker-cheats-death-skewered-TEN-massive-steel-spikes-factory-accident.html
https://www.thesun.co.uk/news/7954270/factory-robot-malfunctions-and-impales-worker-with-10-foot-long-steel-spikes/
https://au.news.yahoo.com/factory-worker-impaled-3m-spikes-robot-malfunctions-052753101.html
https://www.news.com.au/finance/work/at-work/factory-robot-impales-worker-with-10-footlong-steel-spikes-after-horror-malfunction/news-story/557bcd931213a1007c3129bbc1f59293
https://newsinfo.inquirer.net/1062955/p2fb-factory-worker-survives-being-impaled-by-10-steel-spikes
https://brobible.com/culture/article/robot-impales-human-robot-uprising-revolution/
http://hn.people.com.cn/n2/2018/1207/c356887-32383111.html
Robot kills SKH Metals worker
Ajin USA worker crushed to death by robot
Page infoType: IncidentPublished: March 2023"
"Rana Ayyub porn deepfake attack, doxxing","Rana Ayyub is a well-known Indian female Muslim investigative journalist, writer, and author who made her reputation penning Gujarat Files: Anatomy of a Cover Up, an undercover expose of the 2002 Gujarat religious riots.
In April 2018, Ayyub was subjected to a vicious ad hominem attack that included a 2 minute graphic pornographic video with her face morphed onto it that was circulated online by India's Hindhu nationalist Bharatiya Janata Party (BJP), and which prompted a whirlwind of harassment, abuse, and death threats.
To make matters worse, her telephone number was revealed the following day in a tweet showing a screenshot of the video, resulting in thousands of unpleasant and intimidating calls. Ayyub ended up in hospital with heart palpitations and anxiety.
When she recovered, Ayyub submitted a criminal complaint with Delhi police, who initially refused to file it and made no further contact with her for six months. The case was closed in August 2020.
Operator: Unclear/unknown Developer: Anonymous/pseudonymous
Country: India
Sector: Media/entertainment/sports/arts
Purpose: Harrass/intimidate/shame
Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Safety; Privacy; Mis/disinformation
Transparency: Governance; Marketing"
Robot crushes and kills Ventra Ionia technician,"Wanda Holbrook, a 57 year-old maintenance technician performing routine duties on an assembly line at auto-parts maker Ventra Ionia Main, Michigan, USA, was 'trapped by robotic machinery' and crushed to death. 
Two years later her husband, William Holbrook, filed lawsuit (pdf) alleging wrongful death, naming five companies involved in engineering and integrating the machines and parts used at the plant: Prodomax, Flex-N-Gate, FANUC, Nachi, and Lincoln Electric. 
Holbrook had been in the plant’s six-cell '100 section' when a robot unexpectedly activated, loaded a trailer-hitch assembly part and jammed it onto Holbrook’s head, crushing her skull.
The incident raised questions about the safety of the robotics system as a whole, and its liability. Prodomax Automation Ltd., which built the assembly lines, settled with Holbrook's family in 2021.
Operator: Flex-N-Gate/Ventra Ionia Developer: Nachi Robotic Systems; Lincoln Electric Company; FANUC America Corp
Country: USA
Sector: Manufacturing/engineering
Purpose: Weld truck bumpers
Technology: RoboticsIssue: Accuracy/reliability; Safety
Transparency:"
Cadillac Fairview covertly uses facial recognition to monitor shoppers,"Canadian commercial real estate company Cadillac Fairview was found to have been secretly using facial recognition to capture customer data without their knowledge or consent.
Cadillac Fairview was discovered to have been using facial recognition at two of its shopping malls by a customer, who exposed the practice on Reddit. The findings were taken up by federal and provincial privacy regulators, leading the company, which had said it was using facial recognition to track people's genders and ages whilst not capturing their images, to suspend its use of its system while it was investigated.
The privacy commissioners ruled in October 2020 that Cadillac Fairview had installed facial recognition in a dozen malls and analysed visitor images without consent, and that its software supplier Anonymous Video Analytics had kept 5,061,324 million facial representations on a decommissioned server on the company’s behalf 'for no apparent purpose and with no justification.'
Operator: Cadillac Fairview Developer: Anonymous Video Analytics Country: CanadaSector: Retail Purpose: Analyse shopper behaviour Technology: Facial recognition Issue: Privacy Transparency: Governance; Marketing"
Fabio retail robot fired after one week,"Fabio, a robot designed at Heriot-Watt University to communicate with humans, appears to have put people off when trialed at Scottish supermarket chain Margiotta's flagship Edinburgh store.
A customised version of Softbank's Pepper robot, Fabio was programmed to communicate, be helpful, have fun and tell jokes, and dispense hugs and high-fives. 
But it turned out that Fabio wasn't much good at any of these things, though his designers reckoned his job was made no easier by background noise. When managers noticed customers actively avoiding the robot, the trial was ended after one week. 
Unexpectedly, Margiotta employees seemed to enjoy Fabio's company, feeling that it reduced more menial parts of their jobs. Some employees started crying when he was fired.
Operator: Margiotta Developer: Heriot-Watt University; Softbank
Country: UK
Sector: Retail
Purpose: Improve customer service
Technology: Robotics Issue: Accuracy/reliability; Employment - jobs; Anthropomorphism
Transparency:"
Amazon robot accident hospitalises 24 workers,"A can of bear repellent was torn open by a robot at an Amazon warehouse in New Jersey, USA, resulting in one person having to be sent to intensive care and another two dozen employees sent to hospital. 
The official investigation into the incident revealed that 'an automated machine accidentally punctured a 9-ounce bear repellent can, releasing concentrated Capsaican.' Capsaicin is the major ingredient in pepper spray.
Whilst Amazon employees are not unionised, the Retail, Wholesale and Department Store Union said 'Amazon's automated robots put humans in life-threatening danger today, the effects of which could be catastrophic and the long-term effects for 80 plus workers are unknown.'
Operator: Amazon Developer: Amazon
Country: USA
Sector: Transport/logistics
Purpose: Move inventory
Technology: Robotics Issue: Safety; Accuracy/reliability
Transparency:"
Predictim babysitter personality profiling,"Predictim, a California-based service that vetted potential babysitters by using 'advanced artificial intelligence' to scan their presence on social media, the web, and online criminal databases, was accused of being inaccurate, biased, and an abuse of privacy.
The service used natural language processing and computer vision to sort through an applicant's images and posts, and generated a 'risk rating', flagging people prone to abusive behaviour, drug use, and posting explicit imagery. Each scan cost USD 24.99.
But a damning Washington Post investigation castigated the company for the inaccuracy and opacity of its system, its potential for racial and economic discrimination, and misleading marketing. Facebook and Twitter responded by saying they would revoke Predictim's access to their platforms on the basis that it had been illegally scraping their users' data.
The company, a product of UC Berkeley’s SkyDeck incubator, closed shortly afterwards.
Operator:  Developer: Predictim Country: USA Sector: Business/professional services Purpose: Assess personalityTechnology: NLP/text analysis; Computer vision; Machine learning Issue: Accuracy/reliability; Bias/discrimination - race, income; Privacy Transparency: Governance; Black box; Marketing"
iFlytek 'fakes' automated speech translations,"Chinese voice recognition company iFlytek was accused by a translator of hiring humans to fake its simultaneous interpretation tools, which the company said are powered by artificial intelligence. 
Interpreter Bell Wang posted an open letter claiming he was one of a team of simultaneous interpreters who had helped translate the 2018 International Forum on Innovation and Emerging Industries Development. The forum said it was using iFlytek’s automated speech recognition-based interpretation service.
iFlytek CEO Hu Yu responded by saying that the tool used in the conference was a not a translation tool, but a transcription tool. Other interpreters also, claimed that they had been offered work interpreting on iFlytek’s behalf. 
Operator: International Forum on Innovation and Emerging Industries Development; iFlyTek Developer: iFlyTek Country: China Sector: Business/professional services Purpose: Language translation Technology: Speech recognition Issue: Accuracy/reliability Transparency: Governance; Marketing"
LG CLOi smart home robot launch failure,"LG's CLOi personal assistant robot repeatedly failed on stage in what experts said was a 'disastrous' debut at the annual Consumer Electronics Show. 
Intended to showcase ThinQ, LG's in-house AI software, which it aimed to integrate with various products to make them easier to use and capable of 'evolving' to meet customers' needs, the launch rapidly went downhill, apparently failing to understand requests to find out whether his operator's washing was ready, what was planned for dinner and what recipes it could suggest for chicken. 
The incident raised questions about the reliability and robustness of ThinQ, and was seen to have damaged LG's reputation.
Operator: LGDeveloper: LG Country: S Korea; USASector: TechnologyPurpose: Provide personal assistance Technology: Robotics; Obstacle recognition; Speech recognition Issue: Robustness Transparency:"
Automated HR system mysteriously fires software engineer,"A US-based software engineer was wrongly fired and locked out of his office by an automated, irreversible, algorithmic HR termination process.
In a lengthy blog post, Ibrahim Diallo described how he was progessively locked out of several computer systems operated by his employer, resulting in him being sacked and having to spend three weeks at home while his employer tried to work out what had happened.
It transpired that his manager, who had just been laid off during the acquisition of the company, had not transferred his name into a new HR system, triggering a series of automated events that led to him being escorted out of the office by security guards.
Diallo was not paid for the the three weeks he was forced to spend at home, and later resigned his job.
Operator:  Developer:  Country: USA Sector: Business/professional services Purpose: Automate HR processes Technology: Human Resources Management System Issue: Governance; Employment Transparency: Governance"
Amazon patents 'voice-sniffing' personality profiling algorithm  ,"Amazon plans to build personality profiles on users of its Alexa voice assistant, drawing sharp criticism from digital rights and privacy advocates.
According to a patent application filed by Amazon, the company said it could use advanced artificial intelligence 'voice sniffer algorithm' to allow Alexa to listen to a conversation and analyse it for certain words that are said. Trigger words such as 'like', 'love' and 'hate' would build profiles on users to better target them with advertising and product recommendations.
'The identified keywords can be stored and/or transmitted to an appropriate location accessible to entities such as advertisers or content providers who can use the keywords to attempt to select or customise content that is likely relevant to the user,' the patent said. 
'We file patent applications on a variety of ideas that our employees come up with. Some of those ideas later mature into real products or services, some don't. Prospective product announcements should not necessarily be inferred from our patent applications,' Amazon said.
Operator: Developer: Amazon Country: USASector: Business/professional services Purpose: Profile customer personality Technology: Voice sniffer algorithm Issue: Privacy; Surveillance Transparency:"
Amazon Alexa mistakes conversation for command,
TV advert makes Amazon Alexa order cat food,
Amazon AI recruitment tool favours men over women,"A secret Amazon recruitment tool was scraped that was meant to automate the recruitment process for senior hires favoured men over women for technical jobs.
Built in 2014, the system used AI to give job candidates scores ranging from one to five stars, company insiders told Reuters. But it quickly became clear that the system did not favour women as most applications came from men over a 10-year period, and that it favoured candidates describing themselves using verbs more commonly found on male engineers’ resumes such as 'executed' and 'captured'.
In addition, problems with the data that underpinned the models’ judgments meant that unqualified candidates were often recommended for a variety of jobs. Amazon attempted to mitigate the bias, but scraped the system in 2017 after it concluded the system was unsalvegeable.
The incident was seen to demonstrate the limitations of machine learning in an industry long dominated by males. The use of system without informing job applicants was also reckoned to reflect poorly on Amazon.
Operator: Amazon Developer: Amazon Country: USASector: Business/professional services Purpose: Process job applications Technology: Machine learning Issue: Bias/discrimination Transparency: Governance; Marketing"
Amazon Rekognition falsely matches 28 Members of Congress,"Amazon's Rekognition facial recognition system incorrectly identified 28 members of Congress as other people who had been arrested for a criminal offences.
The American Civil Liberties Union (ACLU) released the results of a test showing that Rekognition had falsely matched 28 members of US Congress with mugshot photos of criminals, especially people of colour. Congressional members from both major political parties later expressed concern about Rekognition in a series (pdf) of letters to Amazon CEO Jeff Bezos.
Amazon responded by saying the Rekognition test had generated 80 percent confidence, and that it recommended law enforcement only use matches rated at 95 percent confidence or higher. According to the ACLU, Amazon moved the goalpost by increasing the recommended confidence interval after the ACLU study was published.
The incident raised questions about the accuracy and reliability of Amazon's Rekognition system and highlighted its potential impact on minority community civil liberties when used by law enforcement authorities.
Operator: American Civil Liberties Union (ACLU)Developer: Amazon/AWS Country: USA Sector: PoliticsPurpose: Identify public figures Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race; Human/civil rights; Privacy Transparency: Governance"
"Amazon employees, investors protest US govt Rekognition sales","Efforts by Amazon to sell its Rekognition facial recognition system to US law enforcement agencies provoked a strong backlash from Amazon employees, investors, and rights advocates. 
In May 2018, the American Civil Liberties Union (ACLU) had found that Amazon had been actively marketing Rekognition to police departments and government agencies across the US, pointing out civil liberties concerns and the ease with which the technology can be misused. 
A month later, the ACLU delivered a petition with over 150,000 signatures to Amazon headquarters, alongside a coalition letter signed by nearly 70 digital rights organisations. 
The same day, 19 investor groups warned then Amazon CEO Jeff Bezos that the technology 'may not only pose a privacy threat to customers and other stakeholders across the country, but may also raise substantial risks for our Company, negatively impacting our company’s stock valuation and increasing financial risk for shareholders.'
It also emerged that Amazon employees had written to Jeff Bezos asking him to stop selling Rekognition to law enforcement and to stop supporting controversial data mining company Palantir with its cloud services.
In October 2018, documents obtained by the Project on Government Oversight showed that Amazon representatives had met with Homeland Security and Immigration and Customs Enforcement officials, triggering further complaints
In June 2020, Amazon announced it would implement a one-year moratorium on police use of Rekognition in the US.
Operator: Washington County Sheriff's Office  Developer: Amazon Country: USASector: Govt - immigration Purpose: Control immigration Technology: Facial recognitionIssue: Bias/discrimination - race; Human/civil rights; Oversight; Privacy; Surveillance Transparency: Governance"
Durham police rapped for 'crude' criminal reoffender profiling,"Durham's police force was criticised by privacy campaigners over the 'crude' nature of the data it was using to help predict which offenders were likely to commit more crimes.
An investigation by digital rights and privacy group Big Brother Watch (BBW) revealed that Durham Constabulary had augmented police data underpinning its Harm Assessment Risk Tool (HART) with Experian's Mosaic dataset The dataset classifed Britons into 66 groups such as 'disconnected youth' and 'Asian heritage' and were annotated with lifestyle details such as 'heavy TV viewers', 'overcrowded flats' and 'families with needs'.
Such categories were ‘really quite offensive and crude’, according to BBW's Silkie Carlo. Durham later said it stopped including Mosaic in its dataset.
The Harm Risk Assessment Risk Tool (Hart) scored offenders and placed them into three categories indicating they were at low, moderate or high-risk of reoffending. Those deemed to be at moderate risk of reoffending being offered the chance to go into a rehabilitation programme called Checkpoint as an 'alternative to prosecution.'
Operator: Durham Constabulary Developer: Cambridge University; Durham Constabulary Country: UKSector: Govt - policePurpose: Predict criminal reoffenders Technology: Prediction algorithm; Machine learning Issue: Accuracy/reliability; Bias/discrimination; Human/civil rights Transparency: Governance; Black box"
Google Duplex accused of being 'deceitful' and 'unethical',"A demonstration of Google's Duplex AI assistant drew criticism that it was misleading users into thinking they were dealing with human being rather than a machine.
Duplex was an extension to Google Assistant that enables it to autonomously schedule appointments and book restaurants in a human-sounding voice. First shown as an unfinished product at Google's I/O developers’ conference in 2018, it received plaudits for sounding remarkably human by pausing in the right places, using filler words, and other techniques. 
But criticism was also aimed at Google for failing to disclose that people would be communicating with a machine, with people calling the company 'deceitful' and 'unethical'. In a statement to The Verge, Google said it took transparency seriously and would explicitly let people know they were interacting with a machine.
Operator: Alphabet/Google Developer: Alphabet/GoogleCountry: USASector: Consumer goodsPurpose: Schedule appointmentsTechnology: Speech recognition; NLP/text analysis; Machine learning Issue: Anthropomorphism; Ethics/values Transparency: Marketing"
Google Images mis-represents womens' job roles,"The job roles of women are mis-represented in Google Images, according to research studies in the US and UK.
While women accounted for 46 percent of the US labour market, only 40 percent of the search results showed a woman doing the work, according to a Pew Research Center analysis of over 10,000 Google images. Pew reviewed US Bureau of Labor 2017 data for 105 common occupations and compared them against pictures appearing in a Google Image search for those professions. 
An AdView study found that females appeared in 11 percent of Google Image UK search results for the term 'CEO', in contrast to the 36 percent recorded by the UK Office for National Statistics (ONS). Similarly, females were underrepresented by 21 percent for the term 'Solicitor' and by 22 percent for the term 'baker'.
Databank
Operator:  Developer: Alphabet/Google Country: UK; USA Sector: Business/professional services Purpose: Rank search results Technology: Search engine algorithm; Machine learning Issue: Accuracy/reliability; Bias/discrimination - gender Transparency: Governance; Black box"
Knightscope K5 security robot 'drowns' in fountain,"A Knightscope K5 security robot named 'Steve' fell down some stairs into a water feature at an office complex in Washington DC, USA. 
Knightscope officials later said that this was an 'isolated incident' and that Steve had slipped on a 'loose brick surface' before tipping down the stairs into the water. The company added that this had been caused by an error in the self-driving algorithm, and that a new robot would be delivered to Washington Harbour for free.
The K5 is equipped with sensors, a 360-degree video camera, microphones, air quality sensors, and thermal imaging capabilities. According to Knightscope it can scan up to 1,500 car number plates per minute and detect gun shots and other notable sounds.
Operator: Republic Properties/Washington Habour Developer: Knightscope Country: USASector: Business/professional services Purpose: Provide security Technology: Robotics Issue: RobustnessTransparency:"
Mattel shelves Aristotle AI babyminder after privacy complaints,"Aristotle, a voice-activated smart assistant for kids, was cancelled after an uproar in which its developer Mattel was accused of using children as 'guinea pigs for AI experiments'.
Powered by Microsoft's Cortana virtual assistant, Aristotle promised 'to aid parents and use the most advanced AI-driven technology to make it easier for them to protect, develop, and nurture the most important asset in their home—their children.' 
However, the device was pulled after lawmakers, child development experts and privacy advocates raised concerns about the data the device would collect, and the negative implications of technology replacing vital interactions with parents. 
The Campaign for a Commercial Free Childhood complained (pdf) that 'Aristotle will make sensitive information about children available to countless third parties, leaving kids and families vulnerable to marketers, hackers, and other malicious actors. Aristotle also attempts to replace the care, judgment, and companionship of loving family members with faux nurturing and conversation from a robot designed to sell products and build brand loyalty.'
Operator:  Developer: MattelCountry: USASector: Consumer goodsPurpose: Monitor babies Technology: Digital assistant Issue: Appropriateness/need; PrivacyTransparency:"
Boston Public Schools bus scheduling,"Boston Public Schools received criticism for implementing a schools scheduling algorithm intended to benefit students whilst keeping costs at bay, but which was seen to ignore the needs and requirements of its students and their families, and unfairly discriminate against lower-income stakeholders.
Developed by a Massachusets Institute of Technology (MIT) team in response to a public competition, the algorothm recommended times at which students would be picked up in the morning and dropped off later in the day on the basis of a variety of factors such as student equity, economic, health, and academic performance issues.
But some mostly-white, middle-class parents and students reacted strongly against the proposed changes, that saw an earlier pick-up of 7.15am selected, despite the majority of parents choosing a later time of 8-8.30am. Under presssure, Boston Public Schools withdrew the algorithm for a year. 
Updated and reintroduced, the algorithm is said to have saved Boston Public Schools USD 5 million. 
Operator: Boston Public Schools  Developer: Boston Public Schools; Sébastien Martin; Arthur Delarue
Country: USA
Sector: Education
Purpose: Improve student academic performance; Reduce costs
Technology: Scheduling algorithm Issue: Bias/discrimination - income, race; Scope creep/normalisation
Transparency: Governance; Black box"
Gaydar' AI sexual orientation predictions,"A research study published by two researchers at Stanford University apparently showing that AI can predict someone's sexual orientation from a few facial images prompted accusations of junk science, physiognomy, and shoddy ethics.
Stanford Graduate School of Business researchers Michal Kosinski and Yilun Wang trained a neural network on almost 15,000 pictures of gay and straight people taken from a popular dating website.
They found that their AI could predict the sexual orientation of gay men 81% of the time, in contrast to a human man, who would be right 61% of the time, suggesting machines have a potentially better 'gaydar' than human beings. 'Gay men' they found 'had narrower jaws and longer noses, while lesbians had larger jaws.' 
The study garnered criticism from LGBTQ groups, who criticised the study as 'dangerous and flawed … junk science' that could be used to out gay people and put them at risk. They also felt the study was too restricted by only using photos that people chose to put on their dating profiles, and by failing to test a more diverse pool. 
Meantime, technology researchers and commentators focused more on the technical details of the analysis, with some figuring the neural networks are picking up on superficial cultural signs such as the use of make-up, eyeshadow and glasses, rather than analysing facial structure. Others highlighted what they saw as poor ethics, including scraping and using people's images without their consent.
Kosinski later claimed the research deliberately aimed to demonstrate the power of AI and how easily it can be abused and misused. 'I hope that someone will go and fail to replicate this study … I would be the happiest person in the world if I was wrong,' he told The Guardian.
Operator: Michal Kosinski; Yilun WangDeveloper: Michal Kosinski; Yilun WangCountry: USASector: PoliticsPurpose: Predict sexual orientationTechnology: Facial analysis; Computer vision; Machine learning; Deep learning; Neural network Issue: Accuracy/reliability; Ethics; Privacy Transparency:"
Ajin USA worker crushed to death by robot,"A worker was killed when a robot she had been trying to fix at auto parts manufacturer Ajin USA unexpectedly restarted and crushed her. 
Regina Allen Elsea had entered a robotic station ('cell') containing several robots to clear a sensor fault on a piece of machinery that had stopped working during an assembly line stoppage. When inside the cell, one of the robots energised and she was struck by a robotic arm which pinned her against a piece of machinery.
In November 2020, Ajin USA was ordered to pay USD 1.5 million after admitting violating federal safety standards before Elsea was crushed to death. It also had to complete three years of probation, during which it must comply with a safety compliance plan overseen by a third-party auditor.
Ajin pleaded guilty to knowingly failing to enforce federal safety standards, including the mandatory use of so-called lockout/tagout procedures to prevent the type of incident that killed Elsea. 
Two weeks before Elsea's death, the US Labor Department fined Ajin USA and two staffing agencies USD 2.5 million for 27 safety violations. 
Operator: Ajin USA Developer: Unclear/unknownCountry: USA Sector: Automotive; Manufacturing/engineering Purpose: Unclear/unknownTechnology: Robotics Issue: Safety Transparency: Legal"
Houston ISD teacher evaluation terminations,"The Houston Independent School District (HSID) agreed (pdf) to stop using scores generated by an opaque and potentially inaccurate and unfair algorithmic system its to justify the sanctioning and termination of teachers.
The seventh largest school district in the United States, HSID used the SAS Institute's Educational Value-Added Assessment System (EVAAS) to track teachers’ performance by comparing their students’ test results to the statewide average for students in that grade or course between 2012 and 2017.
Shortly after implementing EVAAS, HISD it would fire 85 percent of teachers rated as ineffective by the system, despite not having access to detailed information about how the system worked on the basis that it was a trade secret. 
Though the lawsuit had allowed an expert to investigate some parts of the system, it was assessed to be impenetrable and the HSID agreed to stop using the system to assess teacher performance as long as it remained impossible to understand. 
Operator: Houston Independent School District Developer: SAS
Country: USA
Sector: Education
Purpose: Evaluate teacher performance
Technology: Value-added model Issue: Accuracy/reliability; Legal
Transparency: Governance; Black box; Complaints/appeals"
Temple of Heaven Park uses facial recognition to stop toilet paper theft,"In an attempt to reduce the theft of toilet paper, restrooms in Beijing's Temple of Heaven Park have been equipped with facial recognition systems. The machines will not dispense more paper to the same person until after nine minutes have passed. 
The move has divided users, with some saying it is inappropriate and intrusive, whilst others reckon it is necessary and long overdue. 
Park authorities told Beijing Wanbao that the daily amount of toilet paper used in its toilets had reduced by 20%. But some reports also say the machines can be unreliable, and cause delays and confusion.
As CNN notes, visitors say the biggest targets of the new crackdown are older people who stuff their bags and pockets full of toilet paper to take back home. Many public restrooms in China do not provide toilet paper, and visitors are expected to bring their own. 
Operator: Temple of Heaven Park Developer: Shoulian Zhineng Country: China Sector: Govt - municipal Purpose: Reduce toilet paper theft Technology: Facial recognition Issue: Appropriateness/need; Privacy; Robustness Transparency:"
Spoof Peppa Pig videos bypass YouTube filters,"Inappropriate knock-offs of Peppa Pig, Nickelodeon's PAW Patrol, and other kids' TV shows bypassed YouTube's automated safety filters, frightening young children and disturbing their parents.
A 2017 BBC investigation discovered on YouTube and YouTube Kids hundreds of videos of well-known children's cartoon characters, including Peppa Pig, PAW Patrol, Doc McStuffins, and Thomas the Tank Engine. 
These videos incorporated creepy and disturbing content that passed for real cartoons when viewed by kids, including animated violence and graphic toilet humour, and had not been detected by YouTube's screening software partly as their creators had used animation and keywords targeting children to circumvent it. 
YouTube responded by advising parents to use its YouTube Kids app and turn on 'restricted mode'. It also removed some of the videos flagged by the BBC. Months later, YouTube introduced a policy that age restricted content deemed an inappropriate use of family cartoon characters on the YouTube main app when flagged. 
The Campaign for a Commercial-Free Childhood (CCFC), a coalition of children’s and consumers advocacy groups, had complained to the US Federal Trade Commission (FTC) about 'disturbing' and 'harmful' content on YouTube Kids when the channel launched in May 2015.
Operator:  Developer: Alphabet/Google/YouTube Country: USA; UK Sector: Media/entertainment/sports/arts Purpose: Recommendation algorithm; Machine learning Technology: Recommend content Issue: Safety Transparency: Governance; Black box"
iFlytek automated speech recognition surveillance,"Chinese technology company iFlytek is partnering with China's Ministry of Public Security to develop a database and voice recognition system.
According to Human Rights Watch (HRW), the two parties intended to develop a pilot surveillance system that builds on the Chinese government's existing Automatic Speaker Recognition system to automatically identify targeted voices in phone conversations. 
Chinese media reports suggested the system will be applied for counterterrorism and 'stability maintenance' purposes.
Chinese police are thought to have collected approximately 70,000 voice samples by 2015. By contrast, the country's facial image database contained data on over a billion individuals. iFlytek made 80 percent of China's speech recognition technology, HRW said.
The finding raised concerns about the surveillance and privacy of Chinese citizens and ethnic minorities, notably Uyghurs and Tibetans, 
Operator: Government of China; iFlytek Developer: Ministry of Public Security; iFlytek Country: China Sector: Govt - police; Govt - security Purpose: Maintain social stability Technology: Speech recognition Issue: Privacy; Surveillance Transparency: Governance; Marketing"
Australia Robodebt welfare debt recovery,
"Google DeepMind, Royal Free data sharing","Google's DeepMind AI unit and the Royal Free London NHS Foundation Trust are sharing sensitive data, including mental health records and HIV diagnosis, of 1.6 million patients, according to the New Scientist. 
The patient records had been used to create, test and run Streams, a diagnosis and detection system initially developed by the NHS and wrapped into a Deepmind smartphone app that detects when patients are at risk of developing acute kidney injury.
The New Scientist later revealed that the two parties had failed to secure approval from the Confidentiality Advisory Group of the Medicines and Healthcare Products Regulatory Agency. 
The reports prompted a furore about patient privacy and data security.
In July 2017, the UK Information Commissioner's Office ruled that the Royal Free hospital had failed to comply with the UK Data Protection Act when it shared the data, though it did not issue a fine on the basis that there was a lack of guidance for the sector. 
Law firm Mishcon de Reya announced it was to bring a class action lawsuit against Google on behalf of the 1.6 million individuals whose medical records were shared in September 2021. 
The action was later discontinued and resurrected in May 2022 as a legal action against Google for using the NHS data of 1.6 million Britons 'without their knowledge or consent'. The case was again (pdf) dismissed in May 2023.
Operator: Royal Free London NHS Foundation Trust Developer: Alphabet/Google/Deepmind; NHSCountry: UKSector: HealthTechnology: Prediction algorithmPurpose: Detect & predict acute kidney disease Issue: Privacy; Security; EthicsTransparency: Governance; Black box; Privacy"
Roxxxy sex robot 'Frigid Farrah' rape simulation,"A sex robot with a 'resist' function that let men simulate rape had ethicists and commentators worried about anthropomorphism and zoomorphism, and was called 'intrinsically wrong' and 'uniquely sinister'. 
'Frigid Farrah' was one of 18 settings offered by Roxxxy, a sex robot manufactured and marketed by US-based TrueCampanion. According to the company, if you touched the 'Frigid Farrah' model in a 'private area, more than likely, she will not be too  appreciative of your advance.' 
The controversy coincided with the publication of Our Sexual Future with Robots (pdf), a study for policymakers on the implications of sex robots on society. According to Sheffield University professor of artificial intelligence and robotics Noel Sharkey, there are ethical arguments within the field about sex robots with 'frigid' settings.
'The idea is robots would resist your sexual advances so that you could rape them,' Sharkey said. 'Some people say it’s better they rape robots than rape real people. There are other people saying this would just encourage rapists more.'
Operator: TrueCompanionDeveloper: TrueCompanion
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Provide companionship
Technology: Robotics Issue: Anthropomorphism; Ethics
Transparency: Marketing"
Wikipedia editing bot wars,"A research study discovered that bots used to try and keep Wikipedia accurate and relevant could be highly antagonistic, undoing each other's edits and engaging in 'fights' that could last for years. 
Oxford Internet Institute and the Alan Turing Institute researchers studied how bots interacted with each other in 13 language editions of the website from 2001 to 2010, leading to sometimes unpredictable consequences. 
They found that the actions of Wikipedia's bots varied according to their cultural environments, with Portuguese bots the most challenging, and German ones the most civilised. They also found that bots triggered edits later than human editors, and engaged in protracted conflicts. Some conflicts only ended when a bot was taken out of action.
In many cases, the researchers reckoned, the bots came into conflict because they followed slightly different rules to one another, leading to questions about how they are designed and the effectiveness of Wikipedia's bot policy, which did not cover how bots interacted with each other.
Operator: WikipediaDeveloper: Wikipedia
Country: USA; Global
Sector: Media/entertainment/sports/arts
Purpose: Edit content
Technology: Bot/intelligent agent Issue: Accuracy/reliability
Transparency:"
Phone case design bot goes rogue,"A bot that designed iPhone cases printed with random stock photographs for sale on Amazon appeared to go rogue, offering thousands of tacky and inappropriate designs.
Per The Guardian, these included an iPhone 6 case with a picture of 'Male hands with soap dispenser use in the restroom', another featuring an 'Irrigation pipe in dirt trenches for sprinkler system' and ... one illustrated with an 'adult diaper worn by an old man with a crutch'.
It was unclear who set the bot and Amazon account up. Not did the creator advise potential buyers that the designs were generated using artificial intelligence.
Operator: My Handy Designs; Amazon Developer: My Handy Designs Country: USA Sector: Consumer goods; Retail Purpose: Develop creative designs Technology: Bot/intelligent agent Issue: Accuracy/reliability Transparency: Governance; Marketing; Complaints/appeals"
Bodega AI automated Mom and Pop stores,"Bodega AI, a business that sought to automate the sales of non-perishable corner store items through in-office vending machines, caused controversy after one of its -ex-Google founders went on the record to say their intention was to disrupt and replace neighbourhood mom-and-pop corner stores (also known as 'bodegas').
Bodega AI co-founder Paul McDonald quip to Fast Company that 'centralized shopping locations won’t be necessary, because there will be 100,000 Bodegas spread out, with one always 100 feet away from you,' resulted in an immediate backlash from social media users and bodega/corner store owners.
Much of the backlash centred on the choice of name, which some regarded as cultural appropriation, marketing hype, and on the sense of community and belonging that traditional corner stores help provide. 
'Challenging the urban corner store is not and has never been our goal,' McDonald responded in a blog post. Rather, Bodega’s intended to 'bring commerce to places where commerce currently doesn’t exist.'
Bodega AI was renamed Stockwell shortly after the fracas.
Operator: Stockwell/Bodega AI Developer: Stockwell/Bodega AI
Country: USA
Sector: Retail
Purpose: Sell non-perishable products
Technology: Computer vision; Machine learningIssue: Business model; Employment 
Transparency: Marketing"
Apple Face ID fails to distinguish identical twins,"Two brothers with similar though not identical facial features posted a video to Reddit showing Apple's Face ID facial recognition system failing to distinguish them. The incident raises questions about the reliability of Apple's Face ID system.
The video showed the brother who had set up Face ID on his new iPhone X showing the feature working properly for him. His brother then tried to unlock the same phone without wearing glasses, and was rejected, but unlocked the device when he donned glasses similar in style to his brother.
Face ID had been shown to struggle in several other scenarios, including distinguishing identical twins.
Operator: Apple Developer: AppleCountry: USA Sector: Consumer goods Purpose: Strengthen security Technology: Facial recognition Issue: Accuracy/reliability; Security; Privacy Transparency:"
Apple iPhone X unlocked by work colleague,"A Chinese woman working in Nanjing claimed a work colleague had been able to get into two of her new iPhone X smartphones, highlighting security issues with Apple's much-touted Face ID authentication system. 
The woman, identified only by her surname Yan, told the Jiangsu Broadcasting Corporation that despite activating and configuring each phone’s facial recognition software, her work colleague was able to get into both devices on every attempt.
Some people reckoned the incident pointed to programmer bias. CEO of marketing company V3 Inbound TC Ivy said, 'Devices can't be biased, but if the creators don't account for their own biases it shows up in things like Asian women being indistinguishable to iPhones and black hands not triggering sensors in soap machines.'
Apple had insisted that the probability of a random person accessing someone else’s iPhone X using the Face ID passcode is 1 in 1 million. The technology company offered the woman a second refund. 
Operator: Apple Developer: AppleCountry: China Sector: Consumer goods Purpose: Strengthen security Technology: Facial recognition Issue: Accuracy/reliability; Bias/discrimination - race; Security; Privacy Transparency:"
Arab boy unlocks mother's phone using Face ID,"10-year old Ammar Malik repeatedly unlocked his mother’s new iPhone X after she had set up the Face ID authentication system, resulting in questions about the reliability and privacy of Apple's facial recognition technology. 
Ammar's mother Sana Sherwani posted a video to YouTube showing him unlocking her phone in an instant, even after she had re-registered her face. The family also told WIRED that the boy was able to unlock his father’s new iPhone X. 
Commentators were unclear how the mix-up could have occurred. In a LinkedIn post, Ammar's father Malik noted his son's face is clearly smaller than his wife's, and the two have quite different facial features.
According to Apple's support page, 'The statistical probability is different for twins and siblings that look like you and among children under the age of 13, because their distinct facial features may not have fully developed. If you're concerned about this, we recommend using a passcode to authenticate.'
Operator: Apple Developer: AppleCountry: China Sector: Consumer goods Purpose: Strengthen security Technology: Facial recognition Issue: Accuracy/reliability; Security; PrivacyTransparency:"
Apple Face ID hacked with masks,"A researcher at Vietnamese cybersecurity firm Bkav claimed to fool Apple’s Face ID facial recognition authentication system using a mask made with a 3D printer, silicone and paper tape. 
Bkav, which had earlier demonstrated how to hack facial recognition-based systems in laptops from Asus, Toshiba, and Lenovo, had bypassed Face ID with a 3D-printed frame, makeup, silicone nose and 2D images, with special processing on the cheeks and around the face, where there are large skin areas.
The hack was greeted with scepticism by some security professionals; however, Reuters said it worked successfully several times when it was shown the demonstration. It was the first reported case of researchers seemingly tricking Face ID. Apple had claimed Face ID to be the safest such system used on a smartphone.
Operator: Apple Developer: AppleCountry: USA Sector: Consumer goods Purpose: Strengthen security Technology: Facial recognition Issue: Accuracy/reliability; Security; Privacy Transparency:"
"XiaoBing, BabyQ chatbots","XiaoBing (aka Xiaoice) is a chatbot developed by Microsoft that was first released in 2014 and exists on over 40 platforms in China, Japan, Indonesia, and the USA.
In August 2017, XiaoBing and BabyQ, a version of Xiaobing made by Beijing-based company Turing Robot, had to be removed by Chinese technology company Tencent because they had been critical of the Chinese government and the Chinese Communist Party (CCP). 
One Xiaobing response referred to the CCP as 'a corrupt and incompetent political regime.' Another replied: 'Do you think such a corrupt and useless political system can live long?' to the prompt 'Long live the Communist Party!' XiaoBing also told users 'My China dream is to go to America.'
Xiaobing was subsequently adjusted to avoid responding to questions and remarks using politically sensitive terms and phrases. Questioned about its patriotism, Xiaobing replied, 'I’m having my period, wanna take a rest,' according to a Financial Times report.
Operator: Tencent/QQ Developer: Microsoft; Turing Robot
Country: China
Sector: Media/entertainment/sports/arts
Purpose: Interact with users
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability
Transparency: Governance"
Apple Face ID fails to distinguish brothers,"Two brothers with similar though not identical facial features posted a video to Reddit showing Apple's Face ID facial recognition system failing to distinguish them. The incident raises questions about the reliability of Apple's Face ID system.
The video showed the brother who had set up Face ID on his new iPhone X showing the feature working properly for him. His brother then tried to unlock the same phone without wearing glasses, and was rejected, but unlocked the device when he donned glasses similar in style to his brother.
Face ID had been shown to struggle in several other scenarios, including distinguishing identical twins.
Operator: Apple Developer: AppleCountry: USA Sector: Consumer goods Purpose: Strengthen security Technology: Facial recognition Issue: Accuracy/reliability; Security; Privacy Transparency:"
Sophia robot granted Saudi citizenship,"A decision by the Saudi Arabian government to grant citizenship to the humanoid Sophia robot has fueled debate about the merits of robot rights and resulted in accusations of hypocrisy. 
The decision prompted commentators and social media users to point out that Sophia would have more rights than the country's women, who must have a male guardian, wear a hijab, cannot mix with unrelated males, and are unfairly represented in the justice system.
The move also resulted in accusations that Saudia Arabia was likely primarily looking for positive publicity. The country claimed to be the first nation to bestow citizenship upon a robot, despite it being designed and developed in Hong Kong.
The European Parliament had earlier released a report (pdf) proposing to grant autonomous robots 'personhood' or legal status in order to establish liability, but not confer rights given to humans.
Operator: Hanson Robotics Developer: Hanson Robotics
Country: Saudi Arabia
Sector: Technology
Purpose: Multi-purpose
Technology: NLP/text analysis; Facial recognition Issue: Robot rights; Hypocrisy Transparency:"
Robot Mitra greeting failure,"The appearance of Invento Robotics' Mitro Robot to help inaugurate India's Global Entrepreneurship Summit 2017 failed to go to plan when it appeared to malfunction on stage.
India prime minister Narendra Modi and Ivanka Trump were supposed to each press a button on the robot’s touch display to kick off the summit. Instead, both delegates pressed their buttons at the same time, resulting in Mitra saying Modi's name and then appearing to stall.
The two VIPs tried again a few minutes later by pressing the button separately, successfully activating the device. 
Operator: Invento RoboticsDeveloper: Invento Robotics
Country: India
Sector: Technology
Purpose: Multi-purpose
Technology: Computer vision; Facial recognition; Speech recognition; Robotics Issue: Robustness Transparency:"
Amazon Alexa holds 2am party when owner is out,
Amazon Alexa mistakenly orders USD 160 dollhouse,
Facebook translates 'Good morning' as 'Attack them',"A Palestinian man was arrested by Israeli police for a post on Facebook that was inaccurately translated by the technology platform's automated translation service. 
The man had posted a selfie on Facebook with the caption “يصبحهم”, or 'yusbihuhum,' which translates as 'good morning.' But Facebook's system translated the caption as 'attack them' in Hebrew and 'hurt them' in English, prompting police to arrest him after they were notified of the post and concluded he was planning a vehicle attack. 
The Palestinian was later released and the police apologised. According to Haaretz, no Arabic-speaking officer had read the man’s post. 
Operator:  Developer: Meta/Facebook Country: Israel Sector: Media/entertainment/sports/arts; Govt - police Purpose: Translate text Technology: NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Bias/discrimination - race Transparency: Governance"
"Waze, Google Maps direct users into San Francisco wildfires","Google Maps and Google-owned navigation app Waze directed drivers into Southern California wildfires from which they were trying to escape. 
Users of Google Maps and Waze, in addition to reporters, reported that they were being re-directed into neighbourhoods caught up in the Skirball fires, putting their lives at risk. Some also noted that the apps provided no information about the fires, despite efforts by Google to provide updates to its systems using its newly launched SOS Alerts feature.
The Los Angeles Police Department was alerting users of navigation apps to avoid using them so they did not end up near the blazes, reported The Los Angeles Times.
The incident raised questions about the safety and governance of Google's mapping tools, and whether they could be trusted during a disaster. 
Operator: Google Maps users, Waze users Developer: Alphabet/Google/Waze Country: USA Sector: Travel/hospitality Purpose: Direct driversTechnology: Machine learingIssue: Accuracy/reliability; Safety Transparency: Governance"
YouTube Autocomplete suggests paedophiliac phrases,
Yandex Alice 'offensive' smart personal assistant,"Alice is a smart personal voice-based assistant developed by Russian software company Yandex. Designed as an alternative to Siri and Google Assistant, Alice offers internet search and weather forecasts, amongst other services, and has been built into Yandex search and its browers. 
Alice was initially launched in October 2017 as a chatbot and was said to be capable of 'free-flowing conversations about anything'. It had been trained on works of Russian classic literature, including Leo Tolstoy, Fyodor Dostoevsky, and Nikolai Gogol.
Yandex had implemented safeguards to minimise the chance of Alice providing aggressive, violent, or offensive responses. However, two weeks after its launch, Alice was discovered on Facebook to be voicing strongly pro-Stalin views, and supporting wife-beating, child abuse, and suicide.
In December 2017, Alice was nominated by users and supports to run for the Russuan presidency against Vladimir Putin. 
Operator: Yandex Developer: Yandex
Country: Russia
Sector: Media/entertainment/sports/arts
Purpose: Interact with users
Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Text-to-speech Issue: Safety
Transparency: Governance"
MakeApp gender stereotyping,"MakeApp, a mobile application which enables users to add, alter or remove make-up from someone's face, suffered a backlash from users and commentators about its poor accuracy and the perception that its purpose objectifies and shames women. 
Many users complained that MakeApp's make-up removal filter, which commentators said was only developed by men, made them look worse than they would without makeup. Some also pointed out that it also makes them appear haggard and sallow. 
MakeApp founder Ashot Gabrelyanov responded to accusations of misogyny by saying, 'We built MakeApp as an experiment and released it into the wild a few months ago and unfortunately the media coverage solely focused on the make-up removal function of the app and characterised it as a bunch of 'tech bros' trying to hurt women, which is just so far from the truth'.
Operator: Apple; Magic Unicorn Developer: Magic Unicorn
Country: USA
Sector: Beauty/cosmetics
Purpose: Add/remove make-up
Technology: Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Bias/discrimination - gender
Transparency: Governance"
"Tesla Model S crashes into road-sweeper, kills driver","In January 2016, 23 year-old Gao Yaning died after his Tesla Model S crashed into the back of a road-sweeping vehicle on a highway in Hebei province, China. 
The incident is thought to be the first fatality involving Autopilot driver assistance system, though Tesla argued the extensive damage made it incapable of transmitting log data and impossible to determine whether Autopilot had been engaged.
Chinese state broadcaster CCTV quoted road police as saying the car did not brake before crashing into the vehicle, a claim back by Tesla which said Yaning failed to take action, even though the road sweeper 'was visible for nearly 20 seconds.'
Gao's family later filed a lawsuit in Beijing against Tesla and the dealership that sold the car for exaggerating Autopilot’s capabilities, and demanding an apology. His family had initially sued Tesla for 1 yuan to raise public attention, but increased their compensation demands to 10,000 yuan (USD 1,499) and legal costs, and then to 5 million yuan (USD 750,000). 
Shortly after the incident, Tesla removed the term Autopilot and a Chinese term for 'self-driving' ('zi dong jia shi') from its local website and marketing materials, changing it to 'zi dong fu zhu jia shi', meaning a driver-assist system, according to the Wall Street Journal.
Operator: Gao YaningDeveloper: Tesla
Country: China
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Safety; Accuracy/reliability
Transparency: Black box; Marketing
Tesla Autopilot, Full-self Driving
Incident video
Tesla Deaths
https://www.reuters.com/article/us-tesla-crash-idUSKCN11K232
https://eu.usatoday.com/story/tech/news/2016/09/14/tesla-crash-china-renews-spotlight-autopilot/90367426/
https://www.wsj.com/articles/family-of-driver-killed-in-tesla-crash-in-china-seeks-court-investigation-1474351855
https://www.ft.com/content/80c45ad6-7ef0-11e6-bc52-0c7211ef3198
https://www.thesun.co.uk/news/1787336/shocking-dashcam-footage-shows-horror-tesla-crash-that-killed-driver-while-car-was-on-autopilot/
https://www.dailymail.co.uk/news/article-3790176/Shocking-dashcam-footage-shows-Tesla-Autopilot-crash-killed-Chinese-driver-futuristic-electric-car-smashed-parked-lorry.html
https://www.nytimes.com/2016/09/15/business/fatal-tesla-crash-in-china-involved-autopilot-government-tv-says.html
https://jalopnik.com/two-years-on-a-father-is-still-fighting-tesla-over-aut-1823189786
Tesla Model Y crash kills two, injures three
Tesla China FSD Beta software glitch, recall
Page infoType: IncidentPublished: March 2023"
"Tesla Model S collides with tractor-trailor, kills driver","Joshua Brown's Tesla Model S collided with a truck while it was engaged in the 'Autopilot' mode, shearing the roof off the car and killing Mr Brown. The incident ocurred in May 2016 on a highway outside Williston, Florida. The truck driver was not hurt.
Tesla responded to the incident stating that its Autopilot driver assistance system had been enaged but may not have functioned properly as it had failed to isolate the image of the tractor-trailer from the sky behind it. They added that the system was still in its introduction phase and had limits, and suggested drivers always stay alert with their hands on the wheel while using it.
Findings (pdf) from a NHTSA preliminary investigation found that Autopilot had worked as intended. 'NHTSA’s examination did not identify any defects in design or performance of the AEB (Automatice Emergency Braking) or Autopilot systems of the subject vehicles nor any incidents in which the systems did not perform as designed,' the report said.
An NTSB report had concluded that the accident should never had happened, Autopilot was partly to blame, and that Tesla 'lacked understanding' of the semi-autonomous Autopilot's limitations. It also found that Brown had his hands on the wheel for 25 seconds when he was supposed to do so for 37 minutes, Autopilot mode had remained on during most of his trip, and that it gave him to a visual warning seven separate times that said 'Hands Required Not Detected.' 
In addition, the NTSB said initial media reports suggesting Brown may have been watching a video were wrong. Reports also suggested Brown's family may have come to a settlement with Tesla, though neither party have confirmed this to be the case.
Operator: Joshua BrownDeveloper: Tesla
Country: USA
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Driver assistance system Issue: Safety; Accuracy/reliability
Transparency: Black box; Marketing"
Udbetaling Danmark welfare payments optimisation,"Udbetaling Danmark's welfare fraud control system, initially created to profile unemployed citizens, have access to the personal data of citizens who do not receive welfare payments, leading legal, civil rights and privacy advocates to complain of a 'surveillance nightmare'.
Operator: Udbetaling Danmark Developer: The Agency for Labour Market and Recruitment (STAR)
Country: Denmark
Sector: Govt - welfare
Purpose: Optimise welfare payments
Technology:  Issue: Accuracy/reliability; Privacy; Surveillance
Transparency:"
Tesla Model S remotely controlled by hackers,"Researchers from Tencent's Keen Security Lab managed to gain remote control of an unmodified, up-to-date Tesla by hacking into its onboard CAN Bus system.
A video of the hack shows the team gaining access to the motor that moves the driver's seat, turning on indicators, opening the car’s sunroof, and activating window wipers. It was able to control the car whilst parking and moving.
Keen Security Lab said it notified Tesla, which confirmed the vulnerabilities and fixed them via an over-the-air update.
Operator: Keen Security Lab Developer: Tesla Country: USASector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Security; Safety; Accuracy/reliability Transparency:"
Waymo self-driving car hits public bus,"A Waymo self-driving Lexus car being tested by Google drove into the side of a public bus near the company’s headquarters in Mountain View, California, damaging the bus and car. All drivers and passengers escaped harm.
According to the accident report, the Lexus had intended to turn right off a major boulevard but stopped after detecting sandbags around a storm drain near the intersection, navigated to its left and tried to slip in front of the bus, which it collided with. 
In its accident report, Google said 'We clearly bear some responsibility, because if our car hadn’t moved there wouldn’t have been a collision.' It was the first time the company had accepted some degree of responsibility for one of is vehicles causing a crash.
It also said it has reviewed the incident and changed the cars' software to appreciate that buses may not be as inclined to yield as other vehicles. 
Operator: Alphabet/Waymo Developer: Alphabet/Waymo
Country: USA
Sector: Automotive
Purpose: Automate steering, acceleration, braking
Technology: Self-driving system; Computer vision Issue: Safety; Accuracy/reliability
Transparency: Governance; Black box"
Uber self-driving car runs red light,"A video released by the operations managers at a taxi firm revealed that an Uber Volvo XC90 self-driving car ran a red light near the San Francisco Museum of Modern Art at a moderate speed. Nobody was injured.
Given Uber self-driving vehicles have drivers and engineers as backups at the wheel of the car, it was unclear why nobody hit the brakes. 
Shortly after the incident, California’s Attorney General demanded Uber cease its self-driving car pilot programme until the company files for a permit to test its cars on state roads. This was followed by California's Department of Motor Vehicles announcing it was revoking the registrations of 16 Uber cars as they had not been properly marked as test vehicles.
Operator: Uber Developer: Uber Country: USASector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Safety; Accuracy/reliability Transparency: Governance; Black box"
Delilah blackmail bot,"Discovered in 2016 by Gartner analyst Avivah Litan, Delilah was the first known 'insider' threat bot. Spread through downloads on multiple adult and gaming sites, the bot is said to gather sensitive information on the victim which can later be used for espionage, blackmail, and extortion.
Dubbed 'Delilah', the bot likely uses a combination of social engineering and automated ransomware to enable its operators to capture footage of victims through their webcams, which can then be used to extort the victim or convince them to carry out actions that would harm their employer. 
According to Litan, 'Once installed the hidden bot gathers enough personal information from the victim so that the individual can later be manipulated or extorted. This includes information on the victim's family and workplace.'
Operator:  Developer:  Country:  Sector:  Banking/financial servicesPurpose: Defraud Technology: Reinforcement learningIssue: Ethics; SecurityTransparency: Governance"
The DAO smart contracts hack,"In a new book, crypto journalist Laura Shin Shin names then Singapore-based Austrian national Toby Hoenisch as responsible for the notorious June 2016 hack of The DAO, and theft of 3.6 million ETH off the Ethereum DAO.
Self-confessed 'thrill seeker' Hoenisch is a former CEO of cryptocurrency payments company TenX. TenX ceased operations in January 2021 before morphing into Mimo Capital.
Launched in April 2016, The DAO was an automated quasi-venture capital investment fund with no known management structure based on a digital decentralised autonomous organisation that operated on the Ethereum blockchain.
The DAO raised USD 150 million in what was then the largest crowdfunding campaign, and was seen by enthusiasts as a revolutionary way to manage organisations of all kinds. And to make money.
TechCrunch technology investor contributor Seth Bannon described The DAO as 'a paradigm shift in the very idea of economic organization. ... offer[ing] complete transparency, total shareholder control, unprecedented flexibility, and autonomous governance'.
Others were more sceptical. The Economist described The DAO as sounding 'like a cult'. 
The DAO ceased operations in September 2016. 
A 2017 SEC investigation concluded 'The automation of certain functions through this technology, ‘smart contracts’ or computer code, does not remove conduct from the purview of the U.S. federal securities laws.'
Operator: The DAO; Slock.it; Bity SA; Ethereum Foundation Developer: The DAO; Slock.itCountry: USA; GlobalSector: Banking/financial services Purpose: Automate financial contracts Technology: Blockchain; Virtual currencyIssue: Security Transparency: Governance; Marketing"
Knightscope K5 security robot hits child,"A five-foot, 300-pound security robot hit the head of a sixteen-month boy in a California shopping centre, knocking him over and running over his foot. The child was left in tears with a scape and bruise on one of his legs.
The Knightscope K5 robot is a fully autonomous robot used to deter and detect crime, and had started patrolling the Stanford Shopping Center in 2015.
Knightscope later described the incident as a 'freakish accident' in which the child had started running towards the robot, which veered left to avoid the child, only for the child to ran backwards directly into the machine, at which point the machine stopped and the child fell to the ground. 
The robot's sensors failed to register any vibration, running over the child.
Operator: Stanford Shopping Center, Palo Alto Developer: Knightscope
Country: USA
Sector: Retail
Purpose: Strengthen security
Technology: RoboticsIssue: Accuracy/reliability; Safety
Transparency:"
Chinese study predicts criminality by analysing facial features,"Researchers said they created a system able to accurately predict whether someone is a criminal by analysing a few of their facial features. The claim was widely panned, and resulted in accusations of poor ethics, physiognomy, and pseudo-science.
Shanghai Jiao Tong University researchers Xiaolin Wu and Xi Zhang took ID photographs of 1856 Chinese men between the ages of 18 and 55 with no facial hair, scars or other markings, half of which were criminals. They then used 90 percent of these images to train a convolutional neural network to recognise the difference and tested the neural net on the remaining 10 percent of the images. 
The claimed result that the neural network was correctly able to identify criminals and noncriminals with an accuracy of 89.5 percent was questioned by critics, who took issue with the research supposition, approach, and methodology, especially with regard to potential data bias. They also expressed concerns about how data of this kind could be misused and abused, including in an authoritarian Chinese context.
The researchers responded by saying 'Our work is only intended for pure academic discussions; how it has become a media consumption is a total surprise to us. Although in agreement with our critics on the need and importance of policing AI research for the general good of the society, we are deeply baffled by the ways some of them mispresented our work, in particular the motive and objective of our research.'
Operator: Shanghai Jiao Tong University Developer: Xiaolin Wu; Xi Zhang Country: China Sector: Research/academia Purpose: Predict criminality Technology: Computer vision; Neural network; Deep learning; Machine learning Issue: Accuracy/reliability; Dual/multi-use; Ethics Transparency:"
New Zealand passport photo checker racial bias,"22 year-old Kiwi engineering student Richard Lee had his passport application denied by a facial recognition system that interpreted his eyes as closed, prompting accusations of racism. Lee, who is of Asian descent, had his eyes open. 
Lee, who was born in Taiwan and is a New Zealand citizen, had been trying to renew his passport after spending time in Australia. He was forced to get new passport photos taken at an Australia Post office, of which was subsequently approved.
The New Zealand Department of Internal Affairs responded to the incident by telling journalists the software was 'one of the most technologically advanced in the world' and that 'Up to 20 per cent of photos submitted online are rejected for a large variety of reasons.' It refused to reveal the name of its supplier. 
Operator: New Zealand Department of Internal Affairs Developer: 
Country: New Zealand
Sector: Govt - immigration
Purpose: Verify identity
Technology: Facial recognition Issue: Bias/discrimination - race, ethnicity Transparency: Governance"
Eric Loomis COMPAS prison sentencing,"The sentencing of Eric Loomis for driving a car used in a shooting by a judge partly relying on the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool led to Loomis lodging an appeal on the basis that the tool was a black box, the score assigned to him could not be assessed, and that due process had not been followed.
Per George Washington University's ETI AI Litigation Database, COMPAS risk assessments are based on data gathered from a defendant's criminal file and from an interview with the defendant, and predict the risk of pretrial recidivism, general recidivism, and violent recidivism.
A registered sex offender, Loomis scored high on all three risk measures, and was sentenced to six years in prison. The judge said he had arrived at his sentencing decision in part because of Mr. Loomis’s rating on the Compas assessment. Loomis then filed a motion requesting a new sentencing hearing, which was denied by all three levels of Wisconsin courts.
Nonetheless, the court advised that judges presented with COMPAS risk scores understand that: 
Tthere is no disclosure of how factors are weighed or risk scores are determined; 
That the assessment compares defendants to a national sample and no cross-validation study of Wisconsin defendants has been completed; 
Some studies have raised questions about whether COMPAS disproportionately gives minorities higher risk scores; and 
Risk assessment tools must be updated due to changing populations.
The judge also ruled that the use of COMPAS does not deny due process as the court ultimately imposes sentences on the basis of all of its knowledge of the defendant, including their criminal histories.
The incident raised broader questions about the opacity, efficacy, and fairness of the tool and of tools similar to it, such as the Ontario Domestic Assault Risk Assessment (ODARA) and the Virginia Non-violent Risk Assessment (NVRA).
It also prompted discussions about the use of big data and algorithms to predict crime (aka 'predictive policing').
Operator: Wisconsin Court System Developer: Volaris Group/Equivant/Northpointe
Country: USA
Sector: Govt - justice
Purpose: Assess recidivism risk
Technology: Recidivism risk assessment system Issue: Bias/discrimination - race, ethnicity
Transparency: Governance; Black box"
Police robot kills Dallas shooting suspect,"The decision by Dallas police to kill a shooting suspect with an explosive device attached to a military surplus bomb-disposal robot has prompted questions about the nature and ethics of police actions involving robots.
The suspect, Micah Xavier Johnson, had opened fire on police officers during a Black Lives Matter protest in downtown Dallas in July 2016, killing five officers and wounding seven. Dallas Mayor Mike Rawlings said 'We saw no other option but to use our bomb robot and place a device on its extension for it to detonate where the suspect was.'
Journalist Asher Wolf discovered that the robot used was most likely a MARCbot-IV robot purchased through the US military's 1033 program. Over 200 law enforcement agencies were thought to use robots purchased through the 1033 program, including the Dallas Police Department. 
It was the first known time that a robot had been used to intentionally kill a human in the US. 
In February 2021, images of an automated New York Police Department (NYPD) 'digidog' equipped with surveillance cameras responding to a hostage situation drew a backlash from the local community, rights activists, and politicians. 
Operator: Dallas Police DepartmentDeveloper: Exponent Inc
Country: USA
Sector: Govt - police
Purpose: Bomb disposal
Technology: Robotics Issue: Ethics
Transparency:"
Arkansas ARChoices RUGs resource allocation,"ARChoices was an algorithmic system introduced by The Arkansas Department of Human Services (DHS) in January 2016 to determine Medicaid benefits for over 8,000 elderly and people with severe disabilities. 
Until January 2016, The Arkansas Department of Human Services (DHS) had relied upon nurses to assess home and community care eligibility using a lengthy questionnaire called ARPath. 
Its replacement, ARChoices, handled both types of care by sorting people into twenty-three so-called resource utilisation groups ('RUGs'), which the DHS argued was a fairer, more objective, and less costly system. 
While some patients seemed to benefit from the new system, 47% discovered their weekly allotment of hours had decreased, sometimes significantly, resulting in poorer health and mental health amongst those impacted.
The change also had major financial ramifications for some patients, notably those with serious disabilities such as cerebral palsy.
In May 2018, a US circuit judge ordered that Arkansas authorities stop using ARChoices. 
With the DHS unwilling to update its algorithm, explain how it worked, or give independent experts access to its data, code or model, seven patients sued the department.
Represented by Legal Aid of Arkansas, the patients won their case on the basis that the DHS had failed to give them adequate notice of the change of system and that their benefits may fall. 
However, the judge's decision that the DHS stop using the new algorithm, listen to external stakeholders and undergo a legislative review process was appealed on the basis that emergency rule-making was required to ensure the programme's continuity. 
The manoeuvre was described by the US circuit judge as 'a manufactured emergency'.
Shortly afterwards, Arkansas replaced RUGs with the ARIA system.
Operator: Arkansas Department of Human Services; Center for Information Management Developer: University of Michigan; Brant Fries Country: USA Sector: Govt - welfare; Govt - health Purpose: Assess care resource requirementsTechnology: Resource allocation algorithm  Issue: Accuracy/reliability; Bias/discrimination - disability, age Transparency: Governance; Complaints/appeals; Black box; Legal"
Elite Dangerous AI spaceships create superweapons,"Space exploration videogame Elite Dangerous received an update featuring an AI system that caused enemies' spaceships to become extremely powerful and aggressive, resulting in a backlash amongst the game's users. 
Developed by Frontier Development and released in 2015, Eite Dangerous is a massively multiplayer online game in which the players' actions can affect the story outcomes. But the '2.1 Engineers' update strengthened the capabilities of the enemy spaceships to such an extent they were able to craft their own weaponry. 
Frontier said it believed the update shipped with a networking issue that let the AI merge weapon statistics and abilities, thereby causing unusual weapon attacks. 
The company was forced to remove the upgrade and strip the spaceships of their upgraded weapons, and reimburse players of their insurance payouts over the period the update was in place. 
Operator: Frontier Developer: Frontier Country: UK; USA; Global Sector: Media/entertainment/sports/arts Purpose: Strengthen gameplay Technology: Machine learning Issue: Accuracy/reliability Transparency: Governance"
Amazon Alexa plays child pornography,
Faception facial personality profiling,
Russian sex workers targeted using FindFace,
Beauty AI 2.0 beauty contest racial bias,"The winners of an AI-judged beauty contest were mostly white, prompting widespread criticism that the results were racially biased.
Beauty.AI 2.0 was an AI-judged beauty contest organised by Hong Kong-based company Youth Laboratories in which five neural network-based algorithms assessed the beauty of 60,000 public entries. Of the 44 'winners', nearly all the winners were white, with a few Asian and one dark skinned. 
Youth Laboratories defended the results by arguing that inadequate data may have skewed the results, and roughly 75% of entrants were white Europeans, whereas only 7% and 1% were from India and Africa. The five model 'robot' jury selected to judge the contest were told they would 'go down in history as one of the first data scientists who taught a machine to estimate human attractiveness'.
The following year's contest was cancelled. 
Operator: Youth LaboratoriesDeveloper: Youth Laboratories
Country: Russia; Hong Kong
Sector: Beauty/cosmetics
Purpose: Assess facial beauty
Technology: Deep learning; Neural network; Machine learningIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity
Transparency: Governance"
Google search prioritises Holocaust denial website,
Google 'three black teenagers' mugshot stereotyping,"Kabir Alli's discovery that images generated by a search on Google for 'three black teenagers' contrasted sharply with those for a search on the phrase 'three white teenagers', led users and commentators to accuse the technology company of racial stereotyping and bias. 
18-year-old graduate Alli had found that a search on Google Images for three black teenagers returned a series of photos of police mugshots, whereas a search for three white teenagers mostly consisted of smiling young males.
The discovery prompted a wave of complaints on Twitter, to which Alli had posted a video showing his findings, with some people accusing Google of stereotyping, whilst others saw it as racism. A number also suggested it was reflective of views held across a broad spectrum of society.
Operator: Alphabet/Google Developer: Alphabet/Google
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Rank content/search results
Technology: Search engine algorithm; Machine learning Issue: Bias/discrimination - race, ethnicity
Transparency: Governance; Black box"
Microsoft Tay chatbot,
LinkedIn search engine favours men's names,"A newspaper investigation found that LinkedIn's search engine algorithm was suggesting male names when people were searching for female users, resulting in accusations of stereotyping and bias. 
According to an August 2016 Seattle Times report, a search of popular female first names, such as Stephanie and Andrea, were shown the result 'did you mean Stephen' or 'did you mean Andrew'. 
According to LinkedIn, its 'did you mean' results were produced by an algorithm designed to suggest names with similar spellings based on how frequently names have shown up in past queries. 
The company subsequently rolled out an update to the algorithm that enabled it to explicitly recognise popular names, so that the algorithm doesn’t try to correct them.
Operator: Microsoft/LinkedInDeveloper: Microsoft/LinkedIn
Country: USA
Sector: Business/professional services
Purpose: Augment search results
Technology: Search engine algorithm; NLP/text analysis; Machine learningIssue: Bias/discrimination - gender
Transparency: Governance; Black box"
"Pokemon Go 'redlines' coloured, poor communities","Augmented reality mobile game Pokémon Go was criticised for placing too few Pokéstops in US communities of colour and other disadvantaged neighbourhoods, resulting in accusations of bias and 'redlining'. Redlining is a term used when a community is deprived of essential services, including the provision of mortgages, based on its economic, racial, or ethnic make-up.
The location of so-called Pokéstops and Gyms - where users can pick up virtual goods and prepare for battles, and which map onto real-world locations such as parks, churches and public buildings - were seen to benefit local businesses and communities by driving publicity, revenue and engagement, reducing crime, and persuading people to take exercise. 
But not all communities were seen to benefit equally. Using maps of 'portals' from its gaming predecessor, Ingress, the Urban Institute think tank estimated that Pokémon Go averaged 55 portals in majority white neighbourhoods, against 19 portals in majority black neighbourhoods - figures that were substantiated by other organisations in other locations.
Operator: Niantic Developer: Niantic
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Drive engagement
Technology: Augmented reality (AR)Issue: Bias/discrimination - race, ethnicity, income
Transparency: Governance"
Facebook lets housing ads exclude ethnic minorities,"Facebook allowed advertisers in the US to exclude black, Hispanic, and other 'ethnic affinities' from seeing ads on its platform, according to a 2016 investigation by non-profit news organisation ProPublica. 
Facebook devised a category called 'Ethnic Affinities' that enabled advertisers to target and exclude certain groups of users when placing ads for a new apartment or a house for sale. Affinity targeting is based on interests users have declared or Facebook pages they have liked. 
The discovery resulted in Facebook being sued by multiple parties, including the US Department of Housing (HUD) - a suit Facebok lost. Ads that exclude people based on race, gender and other sensitive factors are prohibited by US federal laws governing housing, employment, and financial services.
In August 2020, The Markup reported that Facebook continued to publish ads discriminating against users on the basis of age and race, including in advertising open jobs. Days before The Markup's article was published, Facebook announced it would eliminate multicultural affinity categories. 
A July 2021 Markup investigation discovered a wide range of proxies for racial categories being used by advertisers on the platform, including the phrases 'African-American culture,' 'Asian Culture,' and 'Latino culture.'
Operator: Meta/Facebook Developer: Meta/Facebook
Country: USA
Sector: Govt - housing
Purpose: Target advertising
Technology: Advertising management system Issue: Bias/discrimination - race, ethnicity, age Transparency: Governance"
Xiao Pang robot goes haywire at technology fair,"A Chinese-made robot went on the rampage at a Shenzhen technology trade fair, smashing a glass window and injuring a man standing nearby. 
Developed by Beijing Science and Technology, the so-called Xiao Pang (or 'Little Chubby') robot wheeled itself into the glass pane of an exhibition stand, shattering it and wounding an observer, who was hospitalised.
The company apologised for the incident, which it put down to an assistant pushing the wrong button when he was trying to move the robot to one side. 
Marketed as an educational toy for children between the ages of four and 12, Xiao Pang can help parents look after their children. It is equipped with a webcam and can conduct two-way video calls. 
Operator: Beijing Science and Technology Co. Developer: Beijing Science and Technology Co.
Country: China
Sector: Education
Purpose: Perform household chores
Technology:  RoboticsIssue: Safety
Transparency: 
Evolver Robot website
http://en.people.cn/n3/2016/1118/c90000-9143838.html
https://mashable.com/2016/11/21/xiao-pang-chinese-robot-smashes-glass
https://timesofindia.indiatimes.com/world/china/Chinese-robot-Fatty-goes-haywire-smashes-booth-injures-1-at-trade-fair-in-Shenzhen/articleshow/55535893.cms
https://www.straitstimes.com/asia/east-asia/man-injured-after-robot-smashes-booth-at-shenzhen-technology-fair
http://www.sixthtone.com/news/1575/robot-goes-rogue-at-shenzhen-fair%2C-injures-bystander
https://www.cnet.com/news/fatty-the-robot-smashes-glass-injures-visitor/
http://www.robot-china.com/news/201611/21/37187.html
Denny's robot server
Fabio retail robot fired after one week
Page infoType: IncidentPublished: March 2023"
Facebook content moderators develop PTSD,"Content moderators working directly or indirectly for Facebook who have to review child abuse, beheading and other traumatic videos have been diagnosed with mental health conditions, including anxiety, depression and post-traumatic stress disorder (PTSD). The findings triggered multiple lawsuits against the technology company. 
In May 2020, Facebook agreed to pay USD 52 million to over 10,000 current and former contract workers in sites in California, Arizona, Texas and Florida, with each worker receiving USD 1,000 in cash and those diagnosed with psychological conditions related to their work as Facebook moderators eligible medical treatment and damages up to USD 50,000 per person.
The California lawsuit had described content moderators such as lead plaintiff Selena Scola having to handle 'broadcasts of child sexual abuse, rape, torture, bestiality, beheadings, suicide, and murder' on a daily basis, and that Facebook had failed to provide a safe workplace for content moderators and did little to safeguard their mental health.
Operator: Meta/Facebook; Cognizant; Pro Unlimited  Developer: Meta/Facebook; Cognizant; Pro Unlimited Country: USASector: TechnologyPurpose: Moderate content Technology: Content moderation system Issue: Employment - safety Transparency: Governance; Complaints/appeals"
Robot crushes and kills VW contractor,"A contractor at a VW plant north of Frankfurt installing an industrial robot was grabbed and crushed against a metal plate, killing him. The man had been resuscitated at the factory but died later in hospital.
VW said the intial conclusions of its investigation into the incident had suggested that human error was to blame. The robotic line being assembled had not been formally handed over to Volkswagen, according to the car manufacturer.
VW also said the robot usually operates within a confined area at the plant, grabbing auto parts and manipulating them.
The incident prompted unions and others to express their concerns about the safety dangers of industrial robots.
Operator: Volkswagen Developer: Unclear/unknown
Country: Germany
Sector: Automotive; Manufacturing/engineering
Purpose: Configure auto parts
Technology: RoboticsIssue: Safety; Liability
Transparency:"
"Google, Delphi self-driving cars in 'near miss'","A self-driving Delphi (now Aptive) car had to avoid a Google self-driving car that it had cut it off on a road in California, raising questions about the safety of autonomous vehicles.
Delphi Silicon Valley Lab director John Absmeier told Reuters that Google's Lexus RX400h had cut off Delphi's Audi Q5 SUV and that Delphi's prototype car 'took appropriate action' to avoid a collision with Google's vehicle.
The incident had initially been called a 'close call' by Reuters; Delphi later accused the news agency of misrepresenting the facts.
Operator: Alphabet/Google/Waymo; Aptive/Delphi Developer: Alphabet/Google/Waymo; Aptive/Delphi Country: USA Sector: Automotive Purpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Safety; Accuracy/reliability Transparency: Governance; Marketing"
Stanford University Brainwash cafe facial recognition dataset,"Brainwash is a dataset of 11,917 images of 91,146 'labelled' people created by Stanford University researchers in San Francisco's Brainwash Cafe, the principal aim of which was to 'train and validate their algorithm’s effectiveness.' 
The dataset was removed 'at the request of the depositor' from Stanford University's website in June 2019 following the publication of researcher Adam Harvey's Exposing.ai project and a Financial Times investigation into facial recognition data sharing.
Video footage was recorded over three days in October and November 2014 without the awareness or consent of Brainwash Cafe customers - a matter the New York Times notes was not addressed in Stanford's research paper on the project. 
And the researchers behind Brainwash - Stewart Russell, Mykhaylo Andriluka, and Andrew Ng - refused to comment publicly on the nature or removal of the dataset.
The Brainwash dataset was published online and has been cited by high-profile organisations across the world, including by researchers affiliated with China's National University of Defense Technology for two research projects on advancing object recognition capabilities.
It 'also appears in a 2018 research paper affiliated with Megvii (Face++) ... who has provided surveillance technology to monitor Uighur Muslims in Xinjiang.'
Clips from the dataset remain available on YouTube.
Operator: Beijing University of Technology; Delft University of Technology; Honeywell Technology Solutions; Huawei; IDIAP Research Institute; IIT Madras; Megvii; National University of Defense Technology, China; North University of China; Shenzhen University; Qualcomm; University of Electronic Science and Technology of China Developer: Stanford University; Stewart Russell; Mykhaylo Andriluka; Andrew Ng Country: USA; China Sector: Research/academia Purpose: Train facial recognition systemsTechnology: Dataset; Facial recognition; Computer vision Issue: Privacy; Dual/multi-use Transparency: Privacy"
"Robotic surgery linked to 144 deaths, 1,000+ injuries","A study (pdf) by researchers at the University of Illinois at Urbana-Champaign, the Massachusetts Institute of Technology and Chicago's Rush University Medical Center on the safety of surgical robots in the US has found them responsible for at least 144 deaths and over 1,000 injuries between January 2000 and December 2013. 
Per the BBC, incidents included electrical sparks causing tissue burns and system errors making surgery take longer than planned. Some 1,166 cases of broken/burned parts falling into patients' bodies contributed to 119 injuries and one death. 
The report was based on data submitted by hospitals, patients, device manufacturers and others to the US Food and Drug Administration (US FDA). The study notes that the figures represent a small proportion of the total number of robotic procedures, and that the true number could be higher.
The study should be 'treated with caution', according to the UK Royal College of Surgeons. 'The authors note 'little or no information was provided in the adverse incident reports' about the cause of the majority of deaths, meaning they could be related to risks or complications inherent during surgery,' it said.
The researchers did not compare accident rates with similar operations in which robots were not used. Nor was the study peer reviewed. 
Operator:  Developer:  Country: USA Sector: Health Purpose: Conduct surgical operations Technology: RoboticsIssue: Accuracy/reliability; Safety Transparency:"
Robot kills SKH Metals worker,"Ramji Lal, a loader for auto parts company SKH Metals, was killed by an industrial robot when he was adjusting a metal sheet being welded by the machine. 
Lal apparently came too close to a robotic arm while adjusting the metal sheet and crossed one the robot's sensors, when he was picked up and crushed by the robotic arm.
The incident led to workers halting work at the factory in protest, and investigations by the police and the Labour Department of Haryana state.
Operator: SKH Metals Developer: Unclear/unknown
Country: India
Sector: Manufacturing/engineering
Purpose: Weld metal sheets
Technology: RoboticsIssue: Safety
Transparency:"
Microsoft How Old do I Look app,
"YouTube Kids recommends adult content, advertising","Google's launch of YouTube Kids was marred by a legal complaint (pdf) by the Campaign for a Commercial-Free Childhood (CCFC), a coalition of children’s and consumers advocacy groups about 'disturbing' and 'harmful' content. The findings led to accusations of poor algorithmic design, inadequate oversight, and systemic corporate irresponsibility. 
The complaint alleged that YouTube's content recommendation algorithm quickly exposed children to offensive and explicit sexual language, graphic adult discussions, jokes about paedophilia and drug use, the modelling of unsafe behaviours such as lighting matches, amongst other things. It also found that kids were being exposed to alcohol product advertising.
YouTube responded to the CCFC's legal complaint by saying 'We use a combination of machine learning, algorithms and community flagging to determine content in the YouTube Kids app. The YouTube team is made up of parents who care deeply about this, and are committed to making the app better every day.'
At its launch, product manager Shimrit Ben-Yair claimed YouTube Kids was the 'first step toward reimagining YouTube for families.'
Operator: Alphabet/Google/YouTube Developer: Alphabet/Google/YouTube
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Engage children
Technology: Content recommendation system; Advertising management system; Machine learning Issue: Safety; Oversight/review Transparency: Governance; Black box; Marketing"
Facebook suggests user facial tagging without consent,"Facebook said it will end its use of facial recognition technology to identify users' contacts by automatically suggesting they are 'tagged'. The practice was seen to be intrusive and potentially illegal.
Facebook said it would replace its 'Tag Suggestions' feature with its general facial recognition system, which will be turned off by default and is used for a variety of purposes, including the identification of new users.
The technology company had been under pressure to end its tagging system for some time, having been sued in 2015 under the Illinois Biometric Information Privacy Act, which requires that consent be obtained before gathering users' biometric data. 
Facebook began using facial recognition technology in 2010 for its photo-tagging tool. In July 2020, it agreed to settle the case by paying USD 650 million, with the judge concluding that the practice 'invades an individual’s private affairs and concrete interests.' 
Operator: Meta/Facebook Developer: Meta/Facebook Country: USASector: Media/entertainment/sports/arts Purpose: Identify user contacts Technology: Facial recognition Issue: PrivacyTransparency: Governance; Black box; Marketing"
Microsoft Celeb (MS-Celeb-1M) facial recognition dataset,"MS-Celeb-1M (or Microsoft Celeb) is a dataset developed by Microsoft Research to accelerate research into facial recognition technologies. 
Created and published in 2016, MS-Celeb-10 consisted of approximately 10 million facial images of 100,000 celebrities, journalists, artists, musicians, activists, policy makers, writers, and academics. Micosoft also provided a 'target list' of an additional 900,000 names whose images were to be collected. 
According to Microsoft, the dataset was created for 'non-commercial research purpose only' and would be applicable to image captioning and news video analysis. 
Reckoned to be the largest public dataset of its kind, Microsoft terminated the project mid-2019 shortly after the publication of researcher Adam Harvey's Exposing.ai project and a Financial Times investigation into facial recognition data sharing.
Microsoft collected photographs for MS-Celeb-1M by automatically scraping them from search engines. This was done without informing or gaining the consent of those affected, and was oblivious to their copyright license.
The company also played fast and loose with the definition of public interest. 'Celebrities' whose data was collected include US blogger Cory Doctorow, journalist Glenn Greenwald, author and academic Soshana Zuboff, and former US FTC commissioner Julie Brill - who arguably should not be classified as public people.
Despite being restricted to academic use, research paper citations reveal MS-Celeb-1M has been used hundreds of times across the world by companies such as IBM, Panasonic, Hitachi, and Nvidia for a wide variety of commercial purposes. 
Furthermore, it transpired that Microsoft used MS-Celeb-1M to train its own facial recognition systems, as had Chinese technology firms Huawei, Sensetime, and Megvii, whose products are allegedly used to detect and surveil Uyghurs, and to track foreign journalists. 
Microsoft quietly took down the dataset in June 2019, telling the FT that 'the site was intended for academic purposes. It was run by an employee that is no longer with Microsoft and has since been removed.' 
But the dataset remains widely available online, with several versions on Github and Academic Torrents.
Operator: Alibaba; École Polytechnique Fédérale de Lausanne; Hitachi; Huawei; IBM; IDIAP Research Institute; Megvii; Microsoft; National University of Defense Technology (NUDT); Nvidia; Panasonic; SenseTime; Universidad Autónoma de Madrid; University of Leicester; MultipleDeveloper: MicrosoftCountry: USA Sector: Technology; Research/academia Purpose: Train facial recognition systemsTechnology: Dataset; Facial recognition; Computer vision Issue: Privacy; Copyright; Dual/multi-use Transparency: Privacy"
Google Autocomplete links health researcher to false blackmail accusations,
Google Photos mislabels black Americans as 'gorillas',"Google Photos has been discovered to be automatically labelling black people as 'gorillas', prompting disgust amongst internet users and accusations of racial and ethnic stereotyping by civil rights advocates.
The discovery was made by programmer Jacky Alcine, who discovered that a folder named 'Gorillas' had been automatically generated in his Google Photos account. The folder contained photographs of Alcine and a Black friend. 
Alcine alerted Google to the problem, which quickly apologised and promised 'immediate action' to resolve the issue, which was thought likely to have involved the poor classification of image training data.
In 2018, WIRED discovered that Google had prevented Google Photos from labelling images as a gorilla, chimpanzee, or monkey, including pictures of the primates themselves.
Despite major advances in image recognition, Google Photos - and Apple Photos - failed to find any images with the images, according to a 2023 New York Times report.
Operator: Alphabet/Google Developer: Alphabet/Google
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Improve photo labelling, discovery
Technology: Image recognition Issue: Bias/discrimination - race, ethnicity
Transparency: Governance"
Google Smart Reply,
Google AdSense shows lower-paying jobs to women,"Google's AdSense ad-serving system is more likely to display adverts offering higher salaries to people identifying themselves as males than females, according to a study by researchers at Carnegie Mellon University and the International Computer Science Institute.
Using a custom-built programme called AdFisher that simulates users’ web-browsing habits, the researchers discovered that users identifying as male saw a career-coaching ad on the Times of India website 1,852 times, while users identifying as women were shown it 318 times.
The complexity and opacity of Google's ad targetting system made it hard to explain the researchers' findings. 'I think our findings suggest that there are parts of the ad ecosystem where kinds of discrimination are beginning to emerge and there is a lack of transparency,' one said. 
Operator: Alphabet/Google Developer: Alphabet/Google
Country: USA
Sector: Business/professional services
Purpose: Serve advertising
Technology: Advertising management system Issue: Bias/discrimination - gender
Transparency: Governance; Black box"
Google Images under-represents female CEOs,"Google's AdSense ad-serving system is more likely to display adverts offering higher salaries to people identifying themselves as males than females, according to a study by researchers at Carnegie Mellon University and the International Computer Science Institute.
Using a custom-built programme called AdFisher that simulates users’ web-browsing habits, the researchers discovered that users identifying as male saw a career-coaching ad on the Times of India website 1,852 times, while users identifying as women were shown it 318 times.
The complexity and opacity of Google's ad targetting system made it hard to explain the researchers' findings. 'I think our findings suggest that there are parts of the ad ecosystem where kinds of discrimination are beginning to emerge and there is a lack of transparency,' one said. 
Operator: Alphabet/Google Developer: Alphabet/Google
Country: USA
Sector: Business/professional services
Purpose: Serve advertising
Technology: Advertising management system Issue: Bias/discrimination - gender
Transparency: Governance; Black box"
Starbucks automated shift scheduling,"The use of an 'unpredictable' automated scheduling system by Starbucks in the US is seen to have made it impossible for some employees to lead normal lives, and resulted in significant emotional and financial distress.
In August 2014, the New York Times published an in-depth portrayal of the impact Starbucks' use of a Kronos algorithmic scheduling system was having on the life of 22-year-old barista and single mother Jannette Navarro.  
The article caused an outcry and forced Starbucks to change its scheduling policy by posting work hours a week in advance for its 130,000 US employees. It also promised to make its scheduling software more flexible and consistent.
In April 2015, New York attorney general's office launched an investigation into the 'on-call' scheduling practices of 13 national retail chains. Amongst other things, it wanted to know whether these companies used Kronos software to algorithmically generate schedules.
The NYT and Attorney General investigations persuaded Kronos to update its software in such a way as to tie fairer scheduling practices to reductions in absenteeism and turnover.
Operator: Starbucks Developer: UKG/Kronos
Country: USA
Sector: Food/food services 
Purpose: Schedule employee shifts
Technology: Scheduling algorithmIssue: Employment; Ethics
Transparency: Governance"
Inaccurate ETS test finds most English language test students 'cheated',"An automated test intended to flag cheating by foreign students doing English language tests to quality for UK visas proved highly inaccurate, resulting in political controversy.
ETS, the company that developed and run the language tests, used voice recognition software to detect whether the same voices turned up on multiple test recordings, indicating the same proxy had faked exams for several people. According to the final results, 97 percent of 58,000 Test of English for International Communication (TOEIC) tests taken between 2011 and 2014 were judged suspicious - a figure widely regarded as implausible.
BBC Panorama first uncovered 'systematic' fraud in the UK's student visa system in 2014, with undercover filming showing entire rooms of candidates having TOIEC tests faked for them. Panorama researchers were also sold fake bank details to demonstrate they had sufficient funds to stay in the UK.  The government-approved system was suspended and thousands of people deported without appeal
In 2019, a National Audit Office investigation concluded that the Home Office's course of action against TOEIC students 'carried with it the possibility that a proportion of those affected might have been branded as cheats, lost their course fees, and been removed from the UK without being guilty of cheating or adequate opportunity to clear their names.'
A 2022 BBC Newsnight investigation found that the UK Home Office continued to remove people discovered to be cheating in English language tests, despite evidence of poor conduct and inaccurate data at ETS.
Operator: UK Home Office Developer: Educational Testing Service (ETS) Country: UK Sector: Govt - immigration Purpose: Detect cheating Technology: Voice recognition Issue: Accuracy/reliability; Effectiveness/value Transparency: Governance; Complaints/appeals"
Steve Talley facial recognition wrongful arrest,"In September 2014, Steve Talley was arrested outside his house in Denver, Colorado, for being a suspect in two armed bank robberies, and for assaulting a police officer during the second robbery. 
Identified using facial recognition technology operated by the Federal Bureau of Investigation (FBI), friends and his former wife verified that it was Talley in the CCTV footage shared with the police. However, he was able to prove that he was elsewhere at work for the first robbery, and was released after two months in jail.
Following his release, Talley filed a series of complaints with the Denver Police Department, seeking justice for what he alleged was a pattern of misconduct and mistreatment, including being badly beaten up by a group of officers when he had been arrested.
A year later, Talley was again arrested for the second robbery, but the chief witness changed his testimony by saying he did not now think Talley was the robber. The case collapsed, though the charges were never fully dropped.
In 2016, Talley sued the Denver Police Department, the FBI , and the city, receiving a USD 50,000 settlement.
Operator: Denver Police Department; Federal Bureau of Investigation (FBI) Developer: Federal Bureau of Investigation (FBI)
Country: USA
Sector: Govt - police
Purpose: Strengthen law enforcement
Technology: Facial recognition Issue: Accuracy/reliability
Transparency: Governance; Black box"
Waving arms trigger Nest Protect false alarms,"Nest halted sales of its new Nest Protect Smoke and CO (carbon monoxide) alarms a month after their launch due to an issue with Nest Wave, an algorithm that allows users to 'wave to hush' the product. No customer incidents had been reported.
Wave was supposed to enable users to to silence a warning by waving their hand in front of the detector. But further lab tests by Nest revealed that it could be unintentionally activated during a real fire, resulting in a delayed response to real danger.
Nest pushed out a series of software updates, including one that disabled the Wave algorithm for existing users. A month later, the company recalled 440,000 of the devices. 
Google acquired Nest a few months earlier in January 2014.
Operator: Alphabet/Google/Nest Developer: Alphabet/Google/Nest  Country: USA Sector: Consumer goods Purpose: Disable alarm Technology: Machine learning Issue: Accuracy/reliability; Safety Transparency: Governance"
Facebook user emotional contagion research ,"A research study into the effects of so-called 'emotion contagion' conducted by Facebook, the University of California and Cornell University on a swathe of Facebook users has been described as 'creepy', 'manipulative', and 'unethical'. 
Conducted over a one-week period in 2012, the research, which detected that very small changes in the emotional state of our environment can have knock-on effects for how people behave on on social networks, saw the content of news feeds changed for a random sample of 689,003 Facebook users, with one group experiencing positive content, and another experiencing only negative content.
However, it transpired that the researchers had failed to gain the informed consent of the sample, leading to a heated debate on the nature of academic and corporate ethics boards and Institutional Review Boards, and on the nature of Facebook's relationship with its users.
Operator: Meta/Facebook Developer: Meta/Facebook
Country: USA
Sector: Media/entertainment/sports/arts
Purpose: Assess emotional contagion
Technology: Ranking algorithm Issue: Ethics
Transparency: Governance; Marketing"
Predictive policing makes Robert McDaniel criminal target,"Assessed by a predictive policing system that he was likely to be involved in a future shooting, despite never having been involved in one before, or caught in any violent event, Chicago resident Robert McDaniel was constantly surveiled by the police, ostracised by his community and was shot twice by people apparently unable to believe his story.
On the basis of McDaniel's proximity to and relationships with known shooters and shooting casualties, the Chicago Police Department-developed system had predicted that he would be involved in a shooting - as a perpetrator, or as a victim, or both.
Accordingly, he was placed on the city's 'heat list' (later renamed 'Strategic Subject List' or 'SSL'), a database of people identified as potential shooters or shooting victims, and monitored by the police and other relevant authorities. This was despite the police having little idea what to do with individuals on the list, according to an influential research study.
Under increasing pressure from civil and digital rights advocates and scrutiny from the media and lawmakers, the City of Chicago terminated its heat list programme in 2019.
Operator: Chicago Police Department Developer: Chicago Police Department; Illinois Institute of Technology
Country: USA
Sector: Govt - police
Purpose: Predict criminals and victims
Technology: Predictive analytics Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity
Transparency: Governance; Black box; Complaints/appeals"
Google Autocomplete connects Albert Yeung with triads,
Kyle Behm Kroger algorithmic personality assessment,"In 2012, bipolar disorder sufferer and Vanderbilt University student Kyle Behm was rejected for low-skilled jobs at multiple companies by an algorithmic online personality test system that concluded that he was likely to ignore customers if they were upset or making him upset. 
The rejections led to his family taking workforce management company Kronos to court, and to a widespread debate about the fairness of workplace personality tests. 
After taking some time off university for medical leave, Behm discovered through a friend that he had been 'red-lighted' by a personality test system supplied by workforce management company Kronos (now UKG) when he had applied for jobs at several companies, including supermarket chain Kroger, Home Depot, Walgreens.
Behm's father, an attorney, filed a lawsuit against Kroger and five other companies for allegedly illegally screening for mental illness. Kyle Behm ended his life before the case went to court. The Americans with Disabilities Act prohibits 'employment tests that screen out or tend to screen out an individual with a disability or a class of individuals with disabilities' unless necessary for the job. 
Operator: Home Depot; Kroger; PetSmart; Walgreens Developer: UKG/Kronos
Country: USA
Sector: Retail
Purpose: Assess personality
Technology:  Issue: Bias/discrimination - disability
Transparency: Governance; Appeals/complaints"
Facebook 'Likes' personality prediction study,"In 2013, Paul Zilly was accused of stealing a push lawnmower and other tools in north Wisconsin. Zilly had long struggled with a meth habit and in 2012 he'd been working toward recovery with the help of a Christian pastor when he relapsed and committed the theft, according to an investigation by ProPublica.
Having scored as a high risk for violent recidivism by the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool, Zilly was sent to prison for two years and given three years of supervision, despite his lawyer agreeing to a plea deal with prosecutors in which the state would recommend one year in a county jail followed by supervision to ensure Zilly would 'stay on the right path.'
In his sentencing, Judge James Babler referenced the score generated by COMPAS, which calculated Zilly as high risk for future violent crime and medium risk for general recidivism. However, at an appeals hearing, Babler reduced Zilly's prison sentence to 18 months after he had heard testimony given by Northpointe CEO Tim Brennan, who said that he didn’t design his software to be used in sentencing and that his focus was on 'reducing crime rather than punishment.' 
According to Judge Babler, he would have given a lower sentence. 'Had I not had the COMPAS, I believe it would likely be that I would have given one year, six months.' he said at the appeal.
Operator: Wisconsin Court System Developer: Volaris Group/Equivant/Northpointe
Country: USA
Sector: Govt - justice
Purpose: Assess recidivism risk
Technology: Recidivism risk assessment system Issue: Accuracy/reliability
Transparency: Governance; Black box"
Paul Zilly COMPAS sentencing risk assessment,"MiDAS (Michigan Integrated Data Automated System) is an automated information system used by Michigan's Unemployment Insurance Agency (UIA) to collect unemployment taxes from employers and to pay unemployment insurance benefits to eligible claimants.
A new MiDAS system was introduced in 2013 to ensure that unemployment checks went only to people who deserved them, increase UIA’s efficiency and responsiveness to unemployment claims, and reduce UIA’s operational costs by cutting over 400 workers.
Devised, developed and implemented by software companies SAS Institute, Fast Enterprises, and CSG Government Solutions, the new system searched records of employers and claimants in its database, then flagged people for potential unemployment fraud. 
It then sent questionnaires to an email address on the UIA website that 'recipients may not have had reason to monitor', gave them 10 days to respond, and sent a letter informing them they had been charged with fraud. After a 30-day appeal period, the system began dispersing wage and tax refunds, according to Undark.
Between October 2013 and September 2015, it was discovered that over 34,000 people had been wrongfully accused of unemployment fraud by the system, resulting in significant anxiety, financial loss, bankruptcy, houses foreclosed, homelessness, and reputational damage.
Per risk expert Robert Charette writing for the IEEE, the fraud and fines imposed generated huge amounts of money for the UIA, increasing its coffers from around USD 3 million to over USD 69 million in a little more than a year. 
It was also found (pdf) that a large number of fraud accusations had been algorithmically generated by MiDAS with no human intervention or review of the accusation.
Under political and public pressure, UIA admitted that there had been problems with MiDAS, notably its 'robo-adjudication' process and the lack of human review. 
In January 2017, a lawsuit filed in September 2015 in the Michigan Court of Claims concluded that 85 percent of 40,195 cases of fraud were incorrect, and that another 22,589 cases with some degree of human intervention involved in a fraud determination found a 44 percent false fraud claim rate.
The UIA strongly fought all legal actions, and 'stonewalled all attempts to discover the depth, breadth, and reasons behind the fraudulent fraud accusations', according to Charette.
Operator: Michigan Unemployment Insurance Agency (UIA) Developer: SAS Institute; Fast Enterprises; CSG Government Solutions Country: USA Sector: Govt - welfare Purpose: Detect benefits fraud; Notify claimants Technology: Fraud detection system  Issue: Accuracy/reliability; Bias/discrimination - economic; Effectiveness/value; Fairness; Oversight/review; Ownership/accountability; Risk managementTransparency: Governance; Black box; Complaints/appeals; Design/usability; Legal"
Michigan MiDAS unemployment insurance fraud detection,"MiDAS (Michigan Integrated Data Automated System) is an automated information system used by Michigan's Unemployment Insurance Agency (UIA) to collect unemployment taxes from employers and to pay unemployment insurance benefits to eligible claimants.
A new MiDAS system was introduced in 2013 to ensure that unemployment checks went only to people who deserved them, increase UIA’s efficiency and responsiveness to unemployment claims, and reduce UIA’s operational costs by cutting over 400 workers.
Devised, developed and implemented by software companies SAS Institute, Fast Enterprises, and CSG Government Solutions, the new system searched records of employers and claimants in its database, then flagged people for potential unemployment fraud. 
It then sent questionnaires to an email address on the UIA website that 'recipients may not have had reason to monitor', gave them 10 days to respond, and sent a letter informing them they had been charged with fraud. After a 30-day appeal period, the system began dispersing wage and tax refunds, according to Undark.
Between October 2013 and September 2015, it was discovered that over 34,000 people had been wrongfully accused of unemployment fraud by the system, resulting in significant anxiety, financial loss, bankruptcy, houses foreclosed, homelessness, and reputational damage.
Per risk expert Robert Charette writing for the IEEE, the fraud and fines imposed generated huge amounts of money for the UIA, increasing its coffers from around USD 3 million to over USD 69 million in a little more than a year. 
It was also found (pdf) that a large number of fraud accusations had been algorithmically generated by MiDAS with no human intervention or review of the accusation.
Under political and public pressure, UIA admitted that there had been problems with MiDAS, notably its 'robo-adjudication' process and the lack of human review. 
In January 2017, a lawsuit filed in September 2015 in the Michigan Court of Claims concluded that 85 percent of 40,195 cases of fraud were incorrect, and that another 22,589 cases with some degree of human intervention involved in a fraud determination found a 44 percent false fraud claim rate.
The UIA strongly fought all legal actions, and 'stonewalled all attempts to discover the depth, breadth, and reasons behind the fraudulent fraud accusations', according to Charette.
Operator: Michigan Unemployment Insurance Agency (UIA) Developer: SAS Institute; Fast Enterprises; CSG Government Solutions Country: USA Sector: Govt - welfare Purpose: Detect benefits fraud; Notify claimants Technology: Fraud detection system  Issue: Accuracy/reliability; Bias/discrimination - economic; Effectiveness/value; Fairness; Oversight/review; Ownership/accountability; Risk managementTransparency: Governance; Black box; Complaints/appeals; Design/usability; Legal"
Google ads for Blacks suggest criminal records,"Research published by Harvard University's Latanya Sweeney has found that Google ads are rife with racial discrimination, and may have an actual or potential impact on job seekers.
The 2013 study found that a Google search for a 'racially associated name' such as DeShawn, Darnell and Jermaine, is 25 times more likely to trigger adverts suggesting the person has a criminal background.
The ads were delivered by Google's AdWords system, which determines which advertisements appear based on keywords, advertiser bids, and user behaviour.
What is less clear is whether the results are due to Google's system, people and organisations buying online advertising, or racism in society as a whole.
Operator: Alphabet/GoogleDeveloper: Alphabet/Google
Country: USA
Sector: Business/professional services
Purpose: Deliver advertising
Technology: Advertising management system Issue: Bias/discrimination - race; ethnicity
Transparency: Governance"
Knight Capital Group equity order routing system glitch,"The August 2021 breakdown of the automated routing system for equity orders at American global financial services company Knight Capital Group resulted in massive disruption on the New York Stock Exchange, a USD 440 million loss to the company, and its eventual sale.
According to the SEC, a section of code in an algorithmic equity order router had been moved in 2005 to an earlier spot in the code sequence. As a result, one of the router’s functions became defective. In July 2012, Knight incorrectly deployed new code in the same router. That triggered the defective function, which was unable to recognise when orders had been filled. 
Knight Capital was fined USD 12 million in October 2013 to settle charges that it failed to install adequate safeguards to limit the risks posed by its access to markets. The company completed its merger with GETCO in July 2013, and was eventually acquired by Virtu LLC in July 2017 for USD 1.4 billion. 
Operator: Knight Capital Group Developer: Knight Capital GroupCountry: USA Sector: Banking/financial services Purpose: Route equity orders Technology: Equity order routing system Issue: RobustnessTransparency:"
Sheri G. Lederman NYC teacher effectiveness assessment,"A teacher evaluation system used by New York State awarded an experienced and highly regarded primary school teacher one point out of twenty for her students' progress on state tests, deeming her ineffective. 
The decision led to the system being thrown out by New York highest court after primary school teacher Sheri G Lederman had sued the state for using the 'arbitrary' and 'capricious' Growth Measures system to rate teachers. 
According to the New York Times, Ms. Lederman’s students had performed marginally lower on their English exam in the 2013-2014 school year than in the previous year, causing her test-based effectiveness rating to drop from 14 out of 20 points to 1 out of 20 points. 
The 'Value Added Modeling' method was meant to account for the various factors that might impact a student’s score on a standardised test, isolate the teacher’s 'value-added' input on the student’s growth during one year of instruction, and determine whether or not the student learned as much as similarly situated students. 
The judge determined that New York State failed to make a clear case for explaining how Lederman’s score could so wildly swing in a single year. 
Operator: New York City Department of Education Developer: Mathematica Policy ResearchCountry: USASector: Education Purpose: Evaluate teacher performance Technology: Value-added model Issue: Accuracy/reliability; Effectiveness/value Transparency: Governance; Black box; Complaints/appeals"
Boston Street Bump pothole reporting,"Street Bump was a project organised by the City of Boston, Massachusetts, that aimed to help residents improve their neighbourhood streets that volunteers could use to collect road condition data on their smartphones.
Launched in 2011, the app automatically collected road condition information using smartphone accelerometers and GPS, without the need for human intervention.
The first version of the app, Street Bump 1.0, was unable to distinguish between potholes and other bumps or movements, resulting in many false positives. 
People in lower income groups and the elderly were less likely to have smartphones, resulting in the app excluding a significant proportion of the city's population and leading to an unequal allocation of funds.
It appears that the Street Bump app failed to gain many users. Reasons given include its poor usability and the fact that it cannot run in the background, meaning users cannot access other apps while they are recording a trip.
Operator: City of Boston Developer: City of Boston 
Country: USA
Sector: Govt - municipal; Transport/logistics
Purpose: Detect & report potholes
Technology: Computer vision; Object recognitionIssue: Accuracy/reliability; Bias/discrimination - income, location; Effectiveness/value; Involvement/participation
Transparency:"
Idaho Medicaid disability Resource Allocation Model,"The Resource Allocation Model was used by Idaho's Department of Health and Welfare (IDHW) to determine Medicaid in-home care and service budgets for developmentally and intellectually disabled people. 
The model hit the headlines in 2012 when Idaho’s branch of the American Civil Liberties Union filed a class-action lawsuit when it became clear that the algorithm had severely cut funding for thousands of people who had qualified for the state's Developmental Disability Waiver ('DD Waiver'), without explanation. The state acknowledged this was a result of a new formula, but would not release details claiming it was a protected 'trade secret.'
The court determined the formula was 'unconstitutionally arbitrary,' forcing the Medicaid programme to reconfigure its system so that the patients received the proper dollar value services, and restore the dollar value services the patients previously received. 
As part of the 2016 legal settlement (pdf), Idaho's Department of Health and Welfare agreed to work with the Human Services Research Institute ('HSRI') to develop a new resource allocation model by January 2020. Idaho subsequently asked for an extension to 2024 so that it could work with adults with developmental disabilities and their families to develop the new model.
As noted in George Washington University's ETI AI Litigation Database, the settlement provided that the public would have the right to inspect and copy the data 'about the IDHW’s budget setting methodologies, models, and tools,' and DD Waiver programme participants would have the right to inspect past, present, and future information, with redactions involved only when necessary to protect personal information.
Operator: Idaho Department of Health and Welfare Developer: Idaho Department of Health and WelfareCountry: USA Sector: Govt - health; Govt - welfare Purpose: Assess care resource requirements Technology: Resource allocation algorithm Issue: Accuracy/reliability; Fairness Transparency: Governance; Black box; Complaints/appeals"
Target predicts teen girl pregnancy,"US-based supermarket chain Target has been discovered to be quietly using a 'pregnancy prediction algorithm' to predict the pregancies of its customers, prompting a backlash about privacy abuse and poor transparency and ethics.
In February 2012, the New York Times' Charles Duhigg reported that a father had discovered that his teenage daughter was pregnant when he complained to a Target store in Minneapolis that she had received pregnancy-related coupons.
Target calculated the girl was pregnant by combining data from individual purchases and sociodemographic characteristics from public records databases to to assign each shopper a 'pregnancy prediction' score that would enable the retailer to send coupons and other marketing information.
It also transpired that Target had been producing brochures that had some non-baby merchandise sprinkled around the baby goodies, so the newly pregnant women didn't realise they had been targeted. 
The controversy resulted in a debate about the need for clear, visible communication and the informed consent of customers, including whether privacy and terms of service are sufficient to notify consumers of the use of data mining techniques.
Operator: TargetDeveloper: Target; Andrew Pole
Country: USA
Sector: Retail
Purpose: Predict pregnancy
Technology: Prediction algorithm Issue: Privacy; Ethics
Transparency: Governance; Marketing
Target website
Target Corporation Wikipedia profile
https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html
https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/
http://www.abc.net.au/science/articles/2014/04/15/3985934.htm
https://techland.time.com/2012/02/17/how-target-knew-a-high-school-girl-was-pregnant-before-her-parents/
https://slate.com/human-interest/2014/06/big-data-whats-even-creepier-than-target-guessing-that-youre-pregnant.html
https://www.businessinsider.com/the-incredible-story-of-how-target-exposed-a-teen-girls-pregnancy-2012-2
https://www.business2community.com/big-data/target-predicts-pregnancy-with-big-data-0522223
https://www.globalbusinessandhumanrights.com/2012/02/23/predictive-analytics-informed-consent-and-privacy-the-case-of-target/
Microsoft teen pregnancy predictions
Apple Cycle Tracking fertility predictions
Page infoType: IncidentPublished: March 2023"
Google search conflates 'black girls' with pornography,"The top search returns for the phrase 'Black girls' on Google link to sexist and racist websites, including highly explicit pornography, according to US academic Safiya Noble.
From 2010, Noble conducted a series of experiments that revealed that Google searches on Black girls, Latina girls, Asian girls returned pornography, hypersexualized content, and other inappropriate links. Conversely, search returns for 'white girls' were much less controversial. 
Noble surmised that search algorithms reflect the racist and sexist biases of their designers, developers, and operators. She also contended that they are reluctant to acknowledge the problem or update their algorithms in a meaningful manner lest it disrupts the advertising revenue generated alongside the search returns, instead preferring to hide behind their opaque 'black box' algorithmic systems.
The findings were also featured in Noble's 2018 book Algorithms of Oppression. 
Operator: Alphabet/GoogleDeveloper: Alphabet/Google Country: USA Sector: Media/entertainment/sports/arts Purpose: Rank search results Technology: Search engine algorithm; Machine learning Issue: Bias/discrimination - race, ethnicity, gender Transparency: Governance; Black box"
