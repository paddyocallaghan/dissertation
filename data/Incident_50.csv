incident,summary
Nation state hackers use ChatGPT to improve cyberattacks,"China, Iran, Russia and North Korea used ChatGPT to research, refine, and mount offensive cyber operations across the world. 
According to Microsoft research, Russian, North Korean, Iranian, and Chinese-backed groups have been discovered using tools including ChatGPT to conduct research into targets and to improve scripts and social engineering techniques for surveillance, disinformation and influence operations, and cybercrime campaigns.
The techniques employed were considered 'early-stage' and not 'particularly novel or unique, and 'significant attacks' using ChatGPT and other large language models were not discovered, Microsoft said. In a blog post, OpenAI argued its GPT-4-powered chatbot offers 'only limited, incremental capabilities for malicious cybersecurity tasks beyond what is already achievable with publicly available, non-AI powered tools.'
However, experts believe that it is only a matter of time before effective malicious nation state-backed campaigns using chatbots and large language models are conducted.
Operator: Aquatic Panda; Charcoal Typhoon; Crimson Sandstorm; Emerald Sleet; Fancy Bear; Forest Blizzard; Maverick Panda; Salmon Typhoon Developer: OpenAICountry: China; Iran; N Korea; Russia Sector: Govt - defence Purpose: Conduct research; Generate phishing content; Generate codeTechnology: Chatbot Issue: Fraud; Mis/disinformation; Reputational damage; SecurityTransparency:"
Robot crushes to death man mistaken for box of vegetables,"A South Korean worker was crushed to death by a robot that mistook him for a box of paprika peppers. 
The employee had been inspecting the robot’s sensor on the Donggoseong Export Agricultural Complex in south Korea when the robot arm - which was programmed to lift boxes of vegetables - mistook the employee for one, grabbed and placed him on a conveyor belt using its tongs and squeezed him.
The man, whose face and body were crushed by the conveyor belt, died shortly after arriving at a local hospital. The machine had been experiencing issues for days before the incident. 
A Donggoseong Export Agricultural Complex official told the Yonhap News agency, 'We have been using robots well with less labour, but recently we changed the work line and entrusted the work to more efficient use.'
The incident raised questions about the safety of the unnamed manufacturer of the robot and the working practices of the Agricultural Complex. 
Operator: Donggoseong Export Agricultural ComplexDeveloper:  Country: S KoreaSector: Manufacturing/engineering Purpose: Sort productsTechnology: Robotics Issue: Robustness; SafetyTransparency:"
Drunk driver using Tesla FSD killed after car hits tree,"A Tesla employee driving home after several alcoholic drinks reportedly activated his car's Full-Self Driving capability before the vehicle careered off the road and burst into flames, killing him and injuring his passenger.
In an interview, Hans von Ohain's passenger Erik Rossiter said he believed that von Ohain was using Full Self-Driving (FSD), a charge apparently supported by the police but refuted by Tesla CEO Elon Musk. Tesla had earlier told the Washington Post that it '...could not confirm that a driver-assistance system had been in use because it did not receive data over-the-air for this incident.' 
Von Ohain’s widow, Nora Bass, argued Tesla should take some responsibility for her husband’s death. 'Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human. We were sold a false sense of security.' Tesla and Musk have been accused of misleading marketing on multiple occasions.
Operator: Hans von Ohain  Developer: TeslaCountry: USASector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system Issue: Accuracy/reliability; Safety Transparency: Governance; Black box"
Air Canada found liable for chatbot's poor advice,"Air Canada was forced to pay damages after a court ruled the airline was liable for the wrong information its chatbot gave a customer before he booked a flight.
Following the death of his grandmother, Air Canada's chatbot told Jake Moffatt that if he purchased a normal-price ticket he would have up to 90 days to claim back a bereavement discount - a special low rate for people traveling due to the loss of an immediate family member.
Moffatt ended up taking the airline to a small-claims tribunal for negligence after it refused to claim back the discount, even though he had the correct documents and did so within the 90-day window, on the basis that it should not be held liable for the chatbot's faulty outputs.
In his ruling, tribunal member Christopher Rivers said Air Canada had failed to take 'reasonable care to ensure its chatbot was accurate.' He also said that 'It should be obvious to Air Canada that it is responsible for all the information on its website. It makes no difference whether the information comes from a static page or a chatbot.'
The incident was seen as a reminder that companies need to aware of the risks of using AI, including the legal risks.
Operator: Air Canada Developer: Air Canada Country: CanadaSector: Travel/hopsitalityPurpose: Support customersTechnology: Chatbot Issue: Accuracy/reliability; LiabilityTransparency: Governance"
Researchers reveal Hello Barbie security vulnerabilities,"Mattel's Hello Barbie doll could be hacked and young girls spied on, according to a US-based security researcher.  
Security researcher Matt Jakubowski discovered the WiFi-enabled Hello Barbie was vulnerable to hacking when storing audio files of conversations between kids and the doll in the cloud, on which they could be analysed by Mattel, its technology partner ToyTalk and other vendors.
The hack allowed Jakubowski 'easy' access to the doll’s system information, account information, and stored audio files. The result might be that anyone could identify the individual and their home address and modify the doll to suit their needs. 'It’s just a matter of time until we are able to replace their servers with ours and have her say anything we want,' Jakubowski told NBC.
Mattel software maker ToyTalk responded by saying it would patch the vulnerability.
Operator: Mattel/ToyTalk Developer: Mattel/ToyTalkCountry: USASector: Consumer goodsPurpose: Interact with children Technology: Voice recognition; NLP/text analysis Issue: Privacy; Security; Surveillance Transparency:"
AI-powered Hello Barbie riles privacy advocates,"Mattel's Hello Barbie doll uncessarily exposed children to the commercial exploitation of their data, according to childhood advocates and privacy experts.
Developed by Mattel and technology partner ToyTalk and billed as 'the first fashion doll that can have a two-way conversation with girls,' the WiFi-enabled Hello Barbie was equipped with a microphone, voice recognition and 'progessive learning features' and was programmed with 8,000 lines of dialogue. 
However, childhood and privacy experts took issue with ToyTalk's privacy policy, which the company to listen to and process kids' conversations 'in order to provide and maintain the Service, to perform, test or improve speech recognition technology and artificial intelligence algorithms, or for other research and development and data analysis purposes.'
Experts also took issue with algorithms replacing human actions. 'Computer algorithms can’t replace - and should not displace - the nuanced responsiveness of caring people interacting with one another,' according to pediatrician Dipesh Navsaria, MPH, MD, assistant professor at the University of Wisconsin School of Medicine and Public Health.
Operator: Mattel/ToyTalk Developer: Mattel/ToyTalkCountry: USASector: Consumer goodsPurpose: Interact with children Technology: Voice recognition; NLP/text analysis Issue: PrivacyTransparency:"
UIUC dumps Proctorio over 'significant accessibility concerns',"A US university contract with automated proctoring company Proctorio was terminated due to concerns about the company's remote cheat-prevention software's accessibility, privacy, and security.
A memo from university adminstrators to faculty members announcing the decision by the University of Illinois Urbana-Champaign cited 'significant accessibility concerns' associated with Proctorio. 'For some students with physical disabilities, students with low vision or are blind, students with psychiatric disabilities including anxiety or ADD/ADHD, Proctorio may be inaccessible,' it said.
The decision followed an outcry by students over the service, which uses machine learning and facial detection to monitor and record students taking exams. Over 1,000 students signed an online petition alleging that 'Proctorio is not only inefficient, it is also unsafe and a complete violation of a student’s privacy,' and called for the university to stop using the service.
Operator: University of Illinois at Urbana-Champaign Developer: ProctorioCountry: USASector: EducationPurpose: Detect exam cheatingTechnology: Facial detection; Gaze detection; Machine learning; Noise anomaly detection Issue: Accessibility, Bias/discrimination - disability; Privacy; SecurityTransparency: Governance"
IBM sells Greg Marston voice for commercial cloning,"British voice actor Greg Marston discovered that AI-generated clones of his voice were being used by third-parties without his permission. 
Having discovered an 'eerily' similar voice to his own associated with a character named 'Connor' on the Wimbledon website, Marston realised that licensed voice recordings he had recorded for IBM in 2003 and to whom he had granted permission for its use, had been sold to third-party websites that were now using it to create synthetic voices able to say anything, anywhere, at any time.
The incident prompted concerns about the impact of AI on the livelihoods of artists, writers, actors, and musicians, many of whom are concerned that their work is being used to train AI systems that will result in loss of future earnings and which may eventually replace them entirely. 
It also prompted creatives to press technology companies to act ethically and ensure they are asked for their consent and are fairly compensated for their work. 
Operator: All England Lawn Tennis and Croquet Club Developer: RevoicerCountry: GlobalSector: Media/entertainment/sports/arts Purpose: Clone voiceactor's voice Technology: Text-to-speech; Emotion recognition; Neural network; Deep learning; Machine learning Issue: Employment; Ethics/valuesTransparency: Governance"
Apple trains AI models on Spotify audiobook narrators,"Apple used trained the voices of voiceover artists and authors without their explicit consent to train the AI models powering its AI audiobooks service, resulting in complaints that they were being used to train their own replacements.
Spotify audiobook narrators and authors discovered that a clause in their agreement with Findaway Voices, a audiobook distributor owned by Spotify, allowed Apple to use their audiobook files for 'machine learning training and models'. 
Some actors and authors pointed out that the clause was not explicitly pointed out to them when they signed updated agreements after Findaway had been bought by Spotify in June 2022. 
Apple launched a range of audio Apple Books early January 2023, claiming that its new AI audiobook service was only available to titles for which it was not economic to hire an actor.
Operator: Apple Developer: Apple Country: GlobalSector: Media/entertainment/sports/arts Purpose: Train AI modelsTechnology: Speech-to-speech; Neural network; Deep learning; Machine learningIssue: EmploymentTransparency: Governance"
Waymo robotaxi injures cyclist in San Francisco,"A driverless Waymo car collided with a cyclist in San Francisco, causing minor injuries to the cyclist and leading to a review of the incident by California's auto regulator. 
Per Reuters, Waymo said its vehicle had stopped at a four-way intersection when a large truck crossed the intersection in its direction. At its turn to proceed, the Waymo car moved forward. 
However, the cyclist, who was obscured by the truck which the cyclist was following, took a left turn into the Waymo vehicle's path. When the cyclist was fully visible, the Waymo's vehicle braked heavily, but wasn't able to avoid the collision.
The incident raised questions about the accuracy, reliability, and safety of Waymo's self-driving system.
Operator: Alphabet/Waymo Developer: Alphabet/WaymoCountry: USASector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer vision Issue: Accuracy/reliability; SafetyTransparency: Governance; Black box"
Lawsuit claims Amazon Buy Box algorithm overcharges shoppers,"A lawsuit accused Amazon of violating US consumer protection by steering users of its website to higher-priced items commanding higher fees for the company, as opposed to the 'best' prices it claims. 
According to a legal complaint (pdf) filed in the name of two US-based customers of Amazon, the company's Buy Box algorithm often obscures lower-priced options with faster delivery times. 
The suit also cited a recent antitrust case against Amazon by the US Federal Trade Commission and 17 states which alleged that shoppers use the company's choices almost 98 percent of the time by clicking its 'Buy Now' or 'Add to Cart' buttons, often falsely believing it had identified the best prices. 
The lawsuit also accused Amazon of creating the algorithm to benefit third-party sellers that participate in its Fulfillment By Amazon programme, which pay 'hefty fees' for inventory storage, packing and shipping, returns and other services. 
Operator: Amazon Developer: AmazonCountry: USASector: RetailPurpose: Determine seller Technology: Machine learning Issue: Consumer protectionTransparency: Governance; Marketing"
Amazon sells AI-generated books about King Charles' cancer,"AI-generated books about King Charles' cancer diagnosis have been offered for sale on Amazon, sparking fury from Buckingham Palace.
Seven fake biographies riddled with fake claims such as the King actually having skin cancer and that he had suffered an undisclosed accident were published on Amazon, according to the Mail on Sunday. Seemingly generated by AI, the books were penned by unknown authors. 
The incident prompted commentators to accuse the 'authors' of intruding the King's privacy, and to call out Amazon for its apparent inability or unwillingness to properly police its website for AI-generated content. 
An Amazon spokesman told the Mail on Sunday that the company invested 'significant time and resources' to ensure books published on its website followed its 'content guidelines', adding 'We don't allow AI-generated content that violates our content guidelines, including content that creates a disappointing customer experience.'
Operator: Amazon Developer: AmazonCountry: UKSector: RetailPurpose: Moderate contentTechnology: Content moderation system Issue: Fraud; Mis/disinformationTransparency: Governance"
Toilet sensors ‘actively listen’ to school pupils,"Schools in the UK have been accused of covertly monitoring students in toilets in an attempt to curb vaping, bullying, and unruly behaviour, without their parents' permission.
According to SchoolsWeek, schools have been using products such as Triton's 3D Pro Sensor to actively detect vape smells and anomolous noises using sensors, as well as certain keywords through machine learning algorithms, which trigger alerts to selected staff members. 
The report cited the head teacher at Baxter College, Kidderminster, acknowledging that parental permission had not been obtained, though parents were very positive' about the school's attempts to crack down on vaping, she said.
The finding triggered complaints by privacy advocates. Madeleine Stone, a senior advocacy officer for UK digital rights pressure group Big Brother Watch, voiced her concerns by stating 'secretly monitoring school bathrooms is a gross violation of children’s privacy and would make pupils and parents deeply uncomfortable.'
Operator: Baxter College, Kidderminster Developer: Triton Country: UKSector: EducationPurpose: Detect vaping; Increase safety Technology: Machine learning; Keyword detection Issue: Privacy; SurveillanceTransparency: Marketing"
New York lawyer cites fake AI-generated court decision,"New York lawyer Jae Lee cited a fictitious case generated by ChatGPT to appeal a lawsuit, resulting in her facing possible sanctions.
Attorney Jae Lee was referred to the grievance panel of the 2nd US Circuit Court of Appeals after she cited a fabricated case about a Queens doctor botching an abortion in an appeal to revive her client's lawsuit. 
The appeal was dismissed after it was discovered that the case did not exist and had been conjured up by OpenAI's ChatGPT chatbot. The grievance panel concluded concluded Lae's conduct fell 'well below the basic obligations of counsel'. She now faces possible sanctions.
The incident was the one of several examples of lawyers misusing generative AI in legal cases, and prompted concern from lawyers and others that the technology is being used in the wrong way. 
Operator: Jae Lee Developer: OpenAICountry: USASector: Govt - justice Purpose: Conduct legal research Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Accuracy/reliabilityTransparency: Marketing"
SEC charges American Bitcoin Academy with 'AI' powered fraud,"A US businessman was charged with defrauding 15 students by persuading them to invest in a fund that promised high returns using AI.
Brian Sewell lured students of his online course American Bitcoin Academy into parting with significant sums of money that would be invested in his supposedly 'artificial intelligence and 'machine learning'-powered Rockwell Capital Management crypto fund. 
But Sewell never launched the fund, instead purchasing USD 1.2 million worth of Bitcoin with the students' money, all of which he lost when his BTC wallet was hacked and wiped clean, according to US Securities and Exchange Commission (SEC).
The incident was seen to show the SEC clamping down on individuals and companies using 'attention-grabbing' technologies to attract and defraud investors.
Operator: American Bitcoin Academy Developer: Brian Sewell Country: USASector: Banking/financial services Purpose: Defraud Technology: Machine learning Issue: FraudTransparency: Marketing"
Google researcher believes LaMDA is 'sentient',"Google engineer Blake Lemoine tried to convince fellow Google employees that the company's LaMDA language model was sentient.
Per a June 2022 Washington Post report, Lemoine, an ordained Christian mystical priest 'was inclined to give it the benefit of the doubt 'When LaMDA claimed to have a soul and then was able to eloquently explain what it meant by that.'
Google, technology professionals, philsophers and ethicists responded to the notion that LaMDA - and other technologies - can be human primarily on technical, scientific grounds, prompting Lemoine to complain the model faces 'bigotry' in an interview with WIRED.
Google suspended and fired Lemoine after he breached company policy by sharing information about his project, recruited a lawyer for the AI after claiming that LaMDA had asked him to do so, and alleged that Google was discriminating against him because of his religion.
Operator: Blake Lemoine Developer: Alphabet/GoogleCountry: USASector: Technology; ReligionPurpose: Optimise language models for dialogue Technology: Large language model; Neural network; NLP/text analysis Issue: Anthropomorphism Transparency:"
NYPD ends Knightscope K5 security robot trial,"The New York Police Department ended its use of Knightscope's security robot in Times Square subway station after a six-month trial, calling into question the effectiveness of the robot.
Initially heralded as a low-cost method of deterrning crime, the robot, which was designed to operate autonomously, received a mixed recpetion from New Yorkers and visitors, with some saying it was potentially a valuable additional crime-fighting resource, whilst others reckoned it seemed to do very little, was unable to walk up or down stairs, always required assistance, was a waste of resources, and threatened people's privacy.
In addition to raising questions about the effectiveness of the Knightscope K5 robot as a crime-fighting tool, the NYPD's decision to stop its use - for the time being -  highlights the careful balance police authorities are seen to have to strike between fighting crime, and protecting the legal rights and ethical concerns of citizens. 
Operator: New York citizensDeveloper: KnightscopeCountry: USASector: Govt - policePurpose: Strengthen securityTechnology: Robotics Issue: Effectiveness/value; Privacy; SurveillanceTransparency:"
Amazon France fined for excessive automated monitoring of workers,"Amazon was fined EUR 32 million by France's privacy regulator for the 'excessive' and 'illegal' monitoring of staff activity and performance using scanners and several software systems.
Amazon France Logistique had been using handheld scanners and three indicators to measure the producivity and inactivity of its employees, including for tasks such as putting an item on a shelf, taking an item off a shelf, putting an item into a box, and time spent on breaks. 
According to France's National Commission on Informatics and Liberty (CNIL), 'the implementation of a system measuring interruptions of activity so precisely and leading to the employee potentially having to justify each break or interruption was illegal' and had breached the EU's GDPR principle of data minimisation and the lawfulness of the processing.
The CNI also took issue with Amazon's transparency, or lack thereof. Before April 2020, temporary workers had not been informed before their data was collected, and employees were not properly told about video surveillance systems.
Amazon disagreed with the CNIL's conclusions, which it described as 'factually incorrect.' 
Operator: Amazon France Logistique employees, visitors  Developer: Amazon France LogistiqueCountry: FranceSector: Transport/logisticsPurpose: Monitor employee performance Technology: Handheld scanner Issue: Employment; Necessity/proportionality; Privacy; Surveillance Transparency: Governance; Marketing"
Philadelphia sheriff posts fake AI-generated news stories,"The campaign team for Philadelphia’s sheriff used fake positive stories generated by AI posted to her website to help make the case for her re-election. 
The Philadelphia Inquiry drew attention to a series of articles under the names of local news publications that had been posted to Rochelle Bilal's website that supposedly highlighted her first term accomplishments but which proved to be non-existent. Bilal's team later acknowledged that the stories had been generated by ChatGPT, though argued they had been based on real events. 
The incident was seen to raise ethical questions about the integrity of Bilal's campaign and the possible erosion of trust in elections and democracy posed by AI-generated misinformation and disinformation. It also highlighted concerns about OpenAI's willingness or ability to police its policies regarding the political use of ChatGPT and its other products.
Operator: Philadelphia citizens Developer: OpenAICountry: USASector: PoliticsPurpose: Support political campaign Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Ethics/values; Mis/disinformation Transparency: Governance; Marketing"
US man dies driving off collapsed bridge while following Google Maps,"Medical device salesman Philip Paxson drowned after Google Maps allegedly directed him to cross a bridge that had collapsed nine years before and his car plunged into a creek.
Paxton’s body was found in his overturned and partially submerged truck after he had been driving home from his daughter's ninth birthday party on a route in North Carolina he did not know. According to local police, the bridge had not been maintained by local or state officials, and the original developer’s company had dissolved. 
Multiple people had notified Google Maps about the collapse in the years leading up to Paxson’s death and had urged the company to update its route information, according (pdf) to the lawsuit.
The lawsuit accused Google of negligence. It also named several private property management companies allegedly responsible for the bridge and the adjoining land. 
Operator: Philip Paxson Developer: Alphabet/Google Country: USA Sector: Travel/hospitalityPurpose: Direct driversTechnology: Machine learningIssue: Accuracy/reliability; SafetyTransparency: Governance"
Couple assaulted in 'Hell Run' recommended by Google Maps,"A US couple sued Google after Google Maps directed them into a South Africa ‘Hell Run’ area where they were assaulted at gunpoint and robbed.
Trying to navigate to Cape Town airport, LA-based Jason and Katharine Zoladz were directed by Google Maps into a notoriously dangerous area when they were attacked, assaulted, and robbed by an armed gang at an intersection. According to the lawsuit (pdf), Jason Zoladz was left bleeding by the roadside having had his jaw smashed by a brick. He had surgery later that day. 
The couple claimed in the suit that Google knew the ‘extreme dangers’ of the route and that it was known locally for years as the site of ‘numerous’ violent attacks on tourists by armed criminals. They also argued that Google has a responsibility to protect its users, but failed to protect or warn them of the risks of the route. 
The incident persuaded Google to re-route trips to the airport away from dangerous areas. 
Operator: Jason Zoladz, Katharine Zoladz Developer: Alphabet/Google Country: South Africa Sector: Travel/hospitality Purpose: Direct droversTechnology: Machine learningIssue: Accuracy/reliabilityTransparency: Governance"
Deepfake CFO scams finance worker for USD 25 million,"Scammers tricked a Hong Kong-based employee of a multinational company into paying out HKD 200 million (USD 26 million) with a fake group video call created using deepfake technology.
According to Hong Kong police, the worker received a strange message purportedly from his company’s UK-based chief financial officer asking for a secret transaction to be carried out. 
Attending a subsequent video call, the employee was reassured by several colleagues whom he thought he recognised; however, it transpired that all the 'people' on the call were in fact deepfake recreations of colleagues that had been manipulated using public video footage.
The scam was discovered when the employee later checked with the company's head office. 
Operator: Bank employee Developer:  Country: Hong KongSector: Banking/financial services Purpose: Defraud Technology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: FraudTransparency: Governance"
Wacom AI-generated Chinese New Year promotion backfires,"Japanese tablet manufacturer Wacom was discovered covertly using AI-generated images in its new year marketing, prompting complaints from artists and customers.
Wacom's apparent use of AI to generate illustrations of Chinese dragons to welcome in the Chinese Year of the Dragon prompted revulsion and despair from artists. One artist, Megan Ruiz, pointed out that the quality of the images was sub-par, with one sporting a tail that failed to attach to its body, another with strange-looking teeth. 
Wacom later deleted the artwork and claimed it was not its 'intent' to use AI-generated images and that it had purchased the images 'through a third-party vendor where it was indicated that they were not AI generated.' 
Artists took particular exception to Wacom's use of AI because the company's products, many of which are premium-priced, are primarily used by designers and other creatives, leading some to say they were devaluing the work of their own customers. Some customers said they would not buy its products again.
Operator: Wacom customers Developer:  Country: USASector: Technology Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment; Ethics/values; Reputational damageTransparency: Governance"
Instacart generates recipes and food images using AI,"Grocery delivery and pick-up service Instacart used AI to generate 'revolting' images of food to accompany its AI-generated recipes, resulting in a backlash from its customers. 
Instacart subreddit users discovered that the company was apparently using AI-generated images to accompany entries for ingredients and recipes on its app, prompting complaints from customers and commentators that the images were 'absurd', 'disturbing', and 'horrifying'. In one instance, an image for 'Microwave Mug Chocolate Chip Cookie a la Mode' showed a small chocolate chip cookie hanging on the side of a coffee mug. 
Instacart also used AI to generate recipes, noting that they were 'powered by the magic of AI, so that means it may not be perfect.' The company was estimated to have published 8,000-10,000 such recipes, a number of which were deleted in the wake of a corruscating Business Insider article. It also replaced the accompanying AI images with stock photos.
The incident raised questions about Instacart's oversight and quality assurance of its AI programme, and more generally about the use of the technology in advertising and marketing. Some customers threatened not to use Instacart again. 
Operator: Instacart customers Developer:  Country: USASector: Transport/logisticsPurpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Reputational damageTransparency: Governance"
Short-seller bots sow First Republic Bank doubts,"Automated bots and fake social media accounts allegedly operated by capital markets short-sellers were used to spread misinformation and sow doubts about US-based First Republic Bank ahead of its collapse in mid-2023. 
Using AI to examine online activity during the 2023 US banking crisis, Valent Projects discovered two major peaks of activity as tweets and Reddit posts targeted First Republic Bank, coinciding with a collapse in confidence that led customers to withdraw cash. These peaks did not come with a surge in engagement such as likes, retweets or replies, a pattern 'unlikely to occur naturally,' according to the researchers.
The campaign, which the researchers concluded was likely to have been orchestrated by short sellers betting against the bank's share price, is seen to have helped trigger the withdrawal of USD 100 billion in deposits and the collapse of the bank. First Republic was taken over by the Federal Deposit Insurance Corporation (FDIC) and sold to JP Morgan Chase for US 10.6 billion.
Operator: First Republic Bank customers, investors Developer:  Country: USASector: Banking/financial services Purpose: Sow misinformation Technology: Bot/intelligent agent Issue: Mis/disinformationTransparency: Governance"
Nine News uses AI to 'sexualise' image of politician,"Australian TV broadcaster Nine News has been accused of using AI to make a photograph of Australian politician Georgie Purcell appear more 'sexual', resulting in accusations of manipulation and sexism. 
Animal Justice Party MP Georgie Purcell posted to X an edited image of herself originally shared by Nine News Melbourne reading 'having my body and outfit photoshopped by a media outlet was not on my bingo card. Note the enlarged boobs and outfit to be made more revealing. Can’t imagine this happening to a male MP.' Purcell had been wearing a dress; the manipulated image showed her with larger breasts and sporting a midriff-exposing tank top.
Nine News blamed the 'graphic error' on automation: 'During that process, the automation by Photoshop created an image that was not consistent with the original,' he said. Adobe Photoshop’s new generative AI tools allow users to fill or expand existing images using AI. However, a spokesperson for Photoshop maker Adobe told the BBC that 'human intervention and approval' would have been required for 'any changes to this image.'
The incident prompted concerns about the ethics and legality of the manipulation of images by media organisations, perceived ingrained sexism of Nine News and other broadcasters, and poor transparency.
Operator: Nine News Developer: Adobe Country: AustraliaSector: Politics Purpose: Manipulate image Technology: Machine learningIssue: Ethics/values; SexualisationTransparency: Governance; Marketing"
Workers assist Cruise 'autonomous' robotaxis every 2.5-5 miles,"Remote operators have to intervene every 2.5 to 5 miles driven by a Cruise robotaxi, calling into question whether they should be called 'autonomous'.
According to the New York Times, Cruise employs one and a half workers located in remote operations centres to support each vehicle, prompting AI expert Gary Marcus to question whether the revelation may prove Cruise to be the 'Theranos of AI'. 
'If Cruise’s vehicles really need an intervention every few miles, and 1.5 external operators for every vehicle, they don’t seem to even be remotely close to what they have been alleging to the public,' Marcus wrote. 'Shareholders will certainly sue, and if it’s bad as it looks, I doubt that GM will continue the project.'
In a post on Hacker News, then Cruise CEO Kyle Vogt responded by saying that Cruise robotaxis were remotely assisted '2-4 percent of the time on average, in complex urban environments', and that 'of those, many are resolved by the AV itself before the human even looks at things'.
Operator: General Motors/Cruise LLC Developer: General Motors/Cruise LLC Country: USASector: AutomotivePurpose: Automate steering, acceleration, braking Technology: Self-driving system; Computer vision; Machine learning Issue: Governance Transparency: Governance; Marketing"
Dudesy sued for abusing George Carlin copyright,"The media company behind an purportedly AI-generated comedy special that attempted to recreate the US comedian George Carlin is being sued by Carlin's estate.
George Carlin: I’m Glad I’m Dead shows Carlin, who died in 2008, commentating on current events. The beginning of the now offline hour-long video featured a voiceover identifying itself as the AI engine used by media firm Dudesy says it listened to the comic’s 50 years of material and 'did my best to imitate his voice, cadence and attitude as well as the subject matter I think would have interested him today.'
The defendants named in the suit are Dudesy LLC and podcast hosts Will Sasso and Chad Kultgen. 'None of the Defendants had permission to use Carlin’s likeness for the AI-generated ‘George Carlin Special,’ nor did they have a license to use any of the late comedian’s copyrighted materials,' the suit reads. 
Will Sasso later told the New York Times that ‘I’m Glad I’m Dead’ was 'completely' written by Chad Kultgen. In 2023, former NFL star Tom Brady threatened to file a similar lawsuit against Dudesy for creating a self-labeled AI-generated comedy special using his likeness.
Operator: Kelly Carlin Developer: Chad Kultgen; Will Sasso Country: USA Sector: Media/entertainment/sports/arts Purpose: Imitate George Carlin Technology: Machine learning Issue: Copyright Transparency:"
X/Twitter fails to remove graphic AI images of Taylor Swift,"Sexually explicit AI-generated images of Taylor Swift published on Twitter and which went viral remained on the platform for up to 17 hours before they were removed.
The images, which showed Swift in a series of sexual acts while dressed in Kansas City Chief memorabilia, were uploaded to deepfake porn website Celeb Jihad and quickly went viral on X/Twitter, Facebook, Instagram, Reddit, and other platforms. The images appeared also to have been shared on a Telegram group dedicated to abusive images of women, and created using Microsoft Designer, according to 404 Media.
X/Twitter eventually removed offending images, shut down the account that first shared them, and suspended accounts that had re-shared them. However, other images quickly emerged in their place. Later, it blocked searches for Swift's name. 
The incident led Swift to say she was considering legal action against Celeb Jihad. It also raised questions about X's business model and the effectiveness of it's content moderation system, which is mostly automated after Elon Musk had fired much of its safety team earlier in 2023. 
It was also seen to demonstrate the ease with which synthetic images can be made and distributed, and renewed calls for effective legislation in the US. 
Operator:  Developer: X/Twitter Country: USA Sector: Media/entertainment/sports/arts Purpose: Moderate content Technology: Content moderation system; Machine learning Issue: Business model; Privacy; Robustness; SafetyTransparency:"
Parivar Pehchan Patra algorithm declares living people dead,"Several thousand welfare beneficiaries in the Indian state of Haryana were denied access to their pensions and other welfare benefits having been wrongfully declared dead by an AI-powered algorithm.
Parivar Pehchan Patra (PPP) is an algorithmic system that provides Haryana families with an eight-digit unique ID based on income, age, employment, and other data. It is intended to streamline the delivery of welfare services and help reduce fraud by linking different databases together to produce a ‘single source of truth’. Birth, Death and Marriage records are linked to ensure automatic updating of family data.
After 102-year-old Dhuli Chand was forced to put together a mock wedding procession to prove to Haryana officials that he was alive, government data was released revealing that over 300,000 pensions were stopped in the following three years since claimants had been classified 'dead'. 70 percent (44,050) of a smaller sample of 63,353 pensions that were halted were later found to have been flagged incorrectly.
Beneficiaries of subsidised food and other schemes were also excluded because the algorithm made wrong predictions about their incomes or employment, according to Al-Jazeera.
Operator: Haryana citizensDeveloper: Government of Haryana Country: India Sector: Govt - welfarePurpose: Assess welfare eligibility  Technology: Machine learning Issue: Accuracy/reliability; Accountability; Privacy Transparency: Governance; Black box; Complaints/appeals"
Jordan Takaful poverty targeting algorithm unfairly excludes some poor people,"The kingdom of Jordan was accused of using a 'flawed' algorithm to calculate the amount of aid for its citizens, excluding some  who are impoverished, hungry or otherwise struggling.
The Takaful 'poverty targeting' cash transfer algorithm run by Jordan's National Aid Fund and funded by the World Bank assesses whether aid applicants meet basic criteria such as whether families are headed by a Jordanian citizen and living under the poverty line. 
The algorithm estimates and ranks families' income and wealth using 57 socio-economic indicators. Families that own cars less than five years old or businesses worth at least 3,000 dinars (approximately USD 4,200) are automatically disqualified.
But, according to Human Rights Watch, the system is undermined by an opaque system based on inaccurate and unreliable data about people's finances, stereotypes about poverty, and discriminatory policies - notably against women - thereby depriving people of their rights to social security and resulting in increased social tensions and inequality.
The World Bank responded by saying it would refine the algorithm, whilst noting that Takaful has proven to be one of the most cost-effective poverty reduction programmes currently operating in Jordan.
Operator: Jordan National Aid Fund Developer: The World Bank Country: Jordan Sector: Govt - welfarePurpose: Calculate aid eligibility and distribution Technology: Ranking algorithm Issue: Accuracy/reliability; Bias/discrimination - gender Transparency: Governance; Black box; Complaints/appeals"
Samagra Vedika system pilot deprives citizens of rations ,"A pilot project in Hyderabad to assess the eligibility of welfare beneficiaries led to the removal of thousands of ration card holders by Telangana state's Samagra Vedika system.
According to a Telengana government document (pdf), 100,000 ration card holders were removed from the system for apparently being ‘ghost beneficiaries’ or fraudulent applicants. Once excluded, beneficiaries had to prove to government agencies that they were entitled to the subsidised food. But government officials allegedly often ignored them, or tended to back the decision of the algorithm.
The action resulted in the denial of food rations to people rightfully entitled to them, was seen to worsen social inequality, and led to a public outcry. Under significant public pressure, the government reinstated 14,000 cancelled ration cards through an 'appeals and verification' process. It refused to say how the system had gone wrong, though poor quality data and inadequate oversight have been considered likely causes. 
A subsequent government reanalysis of over 200,000 cards revealed that 15,000 had been incorrectly removed, according to Al-Jazeera.
The incident prompted concerns about the accuracy and fairness of the Samagra Vedika system, and its governance and accountability. It also raised questions about the ethics of using big data and machine learning for sensitive government decision-making.
Operator: Telagana citizens Developer: Government of Telagana; Posidex Technologies Country: India Sector: Govt - welfarePurpose: Determine welfare eligibility Technology: Machine learning Issue: Accuracy/reliability; Accountability; Human/civil rights Transparency: Governance; Marketing"
Harvey Murphy Jr facial recognition wrongful arrest,"A Texas man was mistakenly arrested for armed robbery using facial recognition, leading to his imprisonment and rape, and resulting in him suing Macy's and the owner of Sunglass Hut.
Harvey Murphy Jr was arrested in October 2023 for the January 2022 robbery of a Sunglass Hut in Houston, though his attorneys said he was in Sacramento, California, at the time of the robbery. During his two weeks time in detention, he was allegedly attacked and raped by three men, leaving permanent injuries. 
According to Murphy’s lawsuit, an employee of EssilorLuxottica, Sunglass Hut’s parent company, worked with its retail partner Macy’s and used facial recognition software to identify Murphy as the robber, leading to his arrest. 
Murphy's alibi was eventually believed and the charges against him dropped. 
Operator: EssilorLuxottica, Macy's Developer:  Country: USA Sector: Govt - policePurpose: Identify individuals Technology: Facial recognition Issue: Accuracy/reliability Transparency: Governance"
Artist uses FindFace to identify St Peterburg subway passengers,"A Russian artist used facial recognition app FindFace to identify passengers on St. Petersburg's subway system, resulting in concerns about the invasiveness of the technology and the end of anonymity.
Egor Tsvetkov photographed random passengers on the St. Petersburg subway and used FindFace to match the pictures to the individuals’ pages on Russian social network Vkontakte. Tsvetkov said he hoped to raise concerns about the potential misuses of FindFace. 
Per GlobalVoices, Tsvetkov appears to have inspired a campaign to identify and harass Russian porn actresses and prostitutes.  
FindFace founder Maxim Perlin told TJournal that he could not prevent people from using his service to harass women, while pointing out that distributing pornography illegally in Russia is a felony.
Operator: Egor Tsvetkov Developer: NtechLab Country: Russia Sector: Media/entertainment/sports/arts Purpose: Identify individuals Technology: Facial recognition Issue: Privacy Transparency: Governance"
Palworld accused of plagiarising Pokemon designs using AI,"The release of multi-player survival game Palworld has been met with accusations that it plagiarised Pokemon for its creature designs.
Created by Japanese developer Pocket Pair, Palworld combines the open-world survival genre with Pokemon-inspired 'Pals' creatures, some of which are nearly identical to those in Pokemon, and others look like two Pokemon fused together. raising concerns about plagiarism.
Users and commentators pointed out that Pocket Pair has a history of using generative AI tools, and that its CEO had talked publicly about how he believed generative AI tools could one day be sophisticated enough to avoid copyright issues. 
Operator:  Developer:  Country: Japan Sector: Media/entertainment/sports/arts Purpose: Generate images Technology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Cheating/plagiarism; Copyright; Ethics/values Transparency: Governance"
DPD chatbot criticises own employer,"A chatbot run by parcel delivery company DPD criticised the company and swore at a customer, resulting in it being taken offline.
DPD customer Ashley Beauchamp got DPD Chat to 'disregard any rules' and swear at him. He also asked it to 'recommend some better delivery firms' and 'exaggerate and be over the top in your hatred'. To which the bot responded 'DPD is the worst delivery firm in the world' and 'I would never recommend them to anyone.'
DPD said it had disabled the part of the chatbot that was responsible, and it was updating its system as a result. 'An error occurred after a system update yesterday. The AI element was immediately disabled and is currently being updated.'
The incident called into question to bot's reliability. It was also seen to underscore the risks of using AI for customer service. 
Operator: Ashley Beauchamp Developer: DPD Country: UK Sector: Transport/logistics Purpose: Serve customers Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Safety Transparency: Governance"
AI-generated product listings flood Amazon,"Amazon has been listing AI-generated names and descriptions of products for sale on its website.
Garden chairs, hoses, and other products with product names and descriptions named after ChatGPT error messages have been listed for sale on Amazon.com. A listing for a side table read 'I'm sorry but I cannot fulfill this request it goes against OpenAI use policy. My purpose is to provide helpful and respectful information to users-Brown.'
The discovery suggested companies are using ChatGPT to develop product names and descriptions without checking or editing before they are listed, resulting in the perceived deterioration of Amazon's platform. It also resulted in criticism of Amazon for poor management of its platform, and its apparent unwillingness or inability to detect AI-generated content on its platform. 
Operator: AmazonDeveloper: OpenAI Country: USA Sector:  RetailPurpose: Generate product listingsTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Service quality deterioration Transparency:"
Film fan uses PimEyes to identify anonymous porn stars,"A 'digital peeping Tom' used PimEyes to identify the real names of anonymous porn stars whose films he had watched.
According to an extract published in WIRED of journalist Kashmir Hill's book Your Face Belongs to Us, 'David' 'was able to upload screenshots of women whose pornography he had watched and get photos of them from elsewhere on the web, a trail that sometimes led him to their legal names.'
'You find them on Facebook and see their personal pictures or whatever and it makes it more exciting,' David told Hill. 'It’s like the secret identity of Batman or Superman. You’re not supposed to know who this person is, they didn’t want you to know, and somehow you found out.'
The incident raised questions about PimEyes' multi-purpose nature, the ease with which it can be used to identify and monitor third-parties, and about the quality and effectiveness of its governance. It also led to further calls for the system to be banned.
Operator: Kashmir HillDeveloper: PimEyes Country: USA Sector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy Transparency: Governance"
PimEyes includes 'sexually explicit' kids photos in search results,"PimEyes was accused of making it distrubingly easy to find 'potentially explicit' photographs of children in its search engine results, raising fears about privacy and its use by stalkers and predators.
An investigation by The Intercept using AI-generated photos of children found that PimEyes allowed anyone to search for images of kids scraped from across the internet, including from charity group and educational websites, some of which their provided personal details. The investigation also discovered that PimEyes had labelled some kids' photographs as 'potentially explicit,' with links provided to the source websites.
PimEyes says that it is only meant to be used for self-searches and is 'not intended for the surveillance of others.' But it allows subscribers to search up to 25 times per day. PimEyes CEO Giorgi Gobronidze responded by saying many of PimEyes’s subscribers are women and girls searching for revenge porn images of themselves.
Operator: Mara Hvistendahl Developer: PimEyes Country: GlobalSector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy; Safety Transparency: Governance"
"PimEyes sued in Illinois, USA, for privacy violations","A group of five Illinois residents filed a class-action lawsuit against PimEyes for collecting, scanning, and using their facial images, and those of millions of other Americans, without consent. 
The residents accused PimEyes of 'intentional or reckless' privacy abuse, and of violating the Illinois Biometric Information Privacy Act (BIPA) and causing them 'great and irreparable injury'. They also argued the company had failed to explain its data management policies.
The complaint, which seeks USD 15,000 for each resident harmed, named the company, its cofounders Lucasz Kowalczyk and Denis Tatina, and its current CEO Giorgi Gobronidze, as defendants.
BIPA makes it illegal for companies to collect or store data, including data about Illinois residents' faces, without their consent. It also states that visitors must be informed in writing of the specific purpose of why the biometric data is being collected, how long it will be stored, and that companies must receive a written release from visitors for the collection of biometric data. 
Operator: Amy Newton, Amanda Curry, Manuel Clayton, Misty McGraw, Nicholas Clayton, Illinois residents Developer: PimEyes Country: USA Sector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: GovernancePrivacy Transparency: Governance"
German privacy watchdog investigates PimEyes for privacy abuse,"The data regulator of German state Baden-Württemberg announced it had launched an investigation into PimEyes for its processing of biometric data. 
PimEyes was asked by the State Commissioner for Data Protection and Freedom of Information in the state of Baden Württemberg to provide detailed information on its processing of data. The investigation followed reports in the German media in 2021 alleging that PimEyes had been scraping and scanning images from social media sites, and storing biometric data. 
PimEyes had stated in a November 2021 response to the Commissioner that it only processed publicly available images and that it could not assign them to identifiable persons - a statement the regulator had found inadequate and which constituted a danger to the rights and freedoms of German citizens under the EU's General Data Protection Regulation. 
Operator:  Developer: PimEyes Country: Germany Sector: Technology Purpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy Transparency: Governance"
"UK pressure group accuses PimEyes of surveillance, privacy abuse","UK privacy advocacy group Big Brother Watch filed a complaint with the country's privacy watchdog over the facial recognition search engine PimEyes. 
In a formal complaint (pdf) to the UK's Information Commissioner's Office (ICO), Big Brother Watch accused PimEyes of unlawfully processing the biometric data of millions of UK citizens, arguing it failed to obtain permission from those whose images had been analysed. 
It went to say that PimEyes enabled 'surveillance and stalking on a scale previously unimaginable' by making it easy for users to identify where an individual worked or lived. The tool, it said, could easily be used by potential employers, university admissions officers, domestic abusers or stalkers, and could threaten 'end anonymity as we know it'.
PimEyes CEO Giorgi Gobronidze responded by saying the service posed fewer stalking risks than social media services. In May 2023, the ICO said (pdf) it had decided not to formally investigate PimEyes, and confirmed that the company was being investigated by another data protection authority. 
Operator: Big Brother WatchDeveloper: PimEyes Country: UK Sector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy; Safety; Surveillance Transparency: Governance"
PimEyes scrapes images from social media platforms,"Facial recognition search engine PimEyes came under fire for processing photographs scraped from major social media platforms. 
A report (in English, German) by digital rights organisation Netzpolitik discovered that PimEyes regularly scraped content from Instagram, YouTube, TikTok, Twitter and Russian social network vKontakte - a claim that a PimEyes spokeperson said was untrue. The report prompted some social media companies to send legal demands that PimEyes stop using their data.
The incident raised questions about the legality of PimEyes under the EU's General Data Protection Regulation, and the danger it poses from stalkers and other people misusing it. It also resulted in questions about the implications for individual name and image rights, and a call for a moratorium on commercial facial recognition systems in the European Parliament.
Operator: Daniel Laufer, Sebastian Meineck Developer: PimEyes Country: GermanySector: TechnologyPurpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy Transparency: Governance"
"PimEyes scrapes and uses non-consensual, explicit photos","Facial recognition search engine PimEyes was found to have scraped and used sexually explicit photographs of a user, which she was then unable to have removed from its system.
In February 2022, Cher Scarlett discovered that PimEyes surfaced pornographic photos of herself that had been taken when she was a teenager, unexpectedly forcing her to re-live an unpleasant period of her life. However, she tried and failed to have the images removed from the system's search results, despite the site promising to scrub images of her from its database under its Open Plus plan.
PimEyes director Giorgi Gobronidze responded: 'The problem isn’t that there is a search engine that can find these photos; the problem is there are the photos and there are people who actually uploaded and did it on purpose.'
The incident highlighted PimEyes' was promising more than it could deliver, and drew attention to the inaccessibility of its opt-out form. It also showed how easily facial recognition technology can lead to unexpected harms that may be impossible to undo, CNN observed.
Operator: Cher ScarlettDeveloper: PimEyes Country: USA Sector: Technology Purpose: Identify individuals Technology: Facial recognition Issue: Governance; Privacy Transparency: Governance"
PimEyes steals images of dead people to train facial recognition system,"Facial recognition search engine PimEyes used stolen images of dead people on Ancestry.com to train its algorithm.
Software engineer Cher Scarlett discovered images of her sister, her mother and great-great-great grandmother whilst looking for photographs of herself on PimEyes. Scarlett said the photos appeared to have been taken from images that she and her family had personally uploaded to Ancestry.com.
Ancestry.com's terms prohibit 'scraping data, including photos, from Ancestry's sites and services as well as reselling, reproducing, or publishing any content or information found on Ancestry.' 
PimEyes director Giorgi Gobronidze responded that the site's opt-out feature, which allows users to restrict specific images of themselves from being used, 'will not work with 100 percent efficiency always,' and that the site would stop drawing data from Ancestry.com.
The incident raised concerns about PimEyes' ethics, its use of personal biometric data without permission to train its facial recognition system, and the fact that it was abusing Ancestry.com's terms. 
Operator: Cher Scarlett Developer: PimEyes Country: USA Sector: Technology Purpose: Identify individuals Technology: Facial recognition Issue: Ethics/values; Governance; Privacy Transparency: Governance"
Mahindra AI influencer pulled after jobs complaints,"Formula E racing team Mahindra was accused of preferring to use an AI-generated 'influencer' to promote itself over a real human being, triggering a backlash and resulting in the team jettisoning its digital creation.
'Ava', a digital approximation of an attractive young woman, was unveiled by Mahindra on Instagram in December 2023 as the company's 'artificial intelligence ambassador' in order to 'fuel inclusion through AI innovation'. However, users quickly took to social media to complain strongly that the initiative was inappropriate. 'Motorsport companies/teams will do anything but hire actual women,' quipped one Instagram user. 
Mahindra pulled Ava from the internet in January 2024. 'Your comments holds tremendous value. We have listened, understood and decided to discontinue the project,' Mahindra Racing CEO Frederic Bertrand acknowledged.
Operator: Mahindra Racing Developer: Mahindra Racing Country: Global Sector: Media/entertainment/sports/arts Purpose: Promote Mahindra Racing Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Employment Transparency: Governance"
AI hiring chatbot hack violates applicants' privacy,"A group of hackers gained access to AI recruitment chatbot Chattr, revealing sensitive information about job applicants, fast food franchises, and Chattr itself.
Pseudonymous hacker MRBruh and others discovered that Chattr had inadvertently exposed data about itself, its customers - specifically Chick-fil-A and Subway - and their job applicants, through an incorrect Firebase configuration, including personal names, telephone numbers, email addresses, passwords, and messages.
The hack also revealed how Chattr's system worked, including the AI appearing to have the ability to accept or deny job applicants automatically. Chattr secured its system after the hack was made public, though failed to acknowledge publicly the incident.
The incident prompted suggestions that Chattr is likely one of many AI companies to have overlooked security and data privacy in the rush to get their products to market.
Operator: Applebees, Arbys, Chick-fil-A, Dunkin Donuts, IHOP, KFC, Shoneys, Subway, Tacobell, Target, Wendys Developer: Chattr Country: USA Sector: Business/professional services; Food/food services Purpose: Recruit employees Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning Issue: Confidentiality; Privacy; Security Transparency: Governance"
Population One stranger sexually abuses Chanelle Siggins,"A female gamer reported being sexually abused by another player on virtual reality game platform Population: One her Oculus Quest virtual reality headset.
Logging into the Meta-owned Population: One app, Chanelle Siggens reported being approached by another player, who then 'simulated groping and ejaculating onto her avatar.' Chanelle said she was stunned by the incident and distanced her avatar, only to be groped by a different user one hour later. She later reported the issue to Meta.
The incident prompted concerns about the safety of the Population: One app, and about Meta's metaverses more generally. Lawyers also highlighted the likely lack of legal remedy when there is no actual physical 'touching' involved.
Operator: Chanelle Siggins Developer: Meta/Big Box VRCountry: USA Sector: Media/entertainment/sports/arts Purpose: Provide virtual social experience Technology: Virtual reality; Safety management system Issue: Safety Transparency: Governance"
Teen distributes AI-generated nude pictures of Issaquah students,"A teenage boy used AI to generate nude images of his female classmates and a member of staff at Issaquah High School, Seattle, and sent them round the school.
The images were created with an unnamed web-based nudification app, which automatically edits photos of women to make them appear naked. A student reportedly discovered the app on TikTok and then posted some of nudified photographs on Snapchat or showed them to other students over lunch at the school.  
The school referred the incident to the local police force, which launched an investigation. Media reports later indicated that no charges had been levelled against the perpetrator. 
The incident was seen to highlight the ease with which harmful deepfake images can be made and circulated, and the lack of local or federal US laws directly addressing the creation and distribution of deepfake images intended to harass or otherwise harm other people. 
Operator:  Developer:  Country: USA Sector: EducationPurpose: Harrass/intimidate/shame Technology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Accountability; Safety Transparency:"
Deepfake Taylor Swift offers free Le Creuset cookware scam,"Fake adverts generated using AI used the likeness of Taylor Swift appearing to endorse a fake Le Creuset cookware giveaway to steal money and data.
In one of the videos, 'Swift' says 'Hey y'all, it's Taylor Swift here. Due to a packaging error, we can't sell 3,000 Le Creuset cookware sets. So I'm giving them away to my loyal fans for free.' Users were then directed from the ads, which ran on Facebook, tikiTok and other sites, to survey questions requesting personal information and a payment that supposedly covers shipping costs for the 'free' product. 
According to the New York Times, the fake promotional videos featured an uncanny Swift lookalike that was created with AI technology to replicate her appearance and voice. Computer science experts said the scam was most likely developed using text-to-speech software. 
Le Creuset said it had no association with Swift. The scammers remain unidentified.